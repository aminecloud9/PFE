{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NSLKDD_Originaldata_Model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "16_X2LMa1V7ENkqAtbATRAiCkL9sf9fXy",
      "authorship_tag": "ABX9TyOXBPXEG9NQrHl7DO03cCoT",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aminecloud9/PFE/blob/main/NSLKDD_Originaldata_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zubbW2t0qkI4"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "np.random.seed(1337)  # for reproducibility\n",
        "from keras.preprocessing import sequence\n",
        "from keras.utils import np_utils\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import SGD\n",
        "from keras.layers import Dense, Dropout, Activation, Embedding\n",
        "from keras.layers import LSTM, SimpleRNN, GRU\n",
        "from keras.datasets import imdb\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from sklearn.metrics import (precision_score, recall_score,\n",
        "                             f1_score, accuracy_score,mean_squared_error,mean_absolute_error)\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import Normalizer\n",
        "import h5py\n",
        "from keras import callbacks\n",
        "from keras import callbacks\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Px30HHOq6ihg"
      },
      "source": [
        "Download dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGP9ArVErwIu"
      },
      "source": [
        "#data=pd.read_csv('/content/drive/MyDrive/CICIDS2017_multi_class_StandardScaler_NormalsationResults.csv')\n",
        "#train_data = pd.read_csv('/content/drive/MyDrive/KDDTrain+.csv', header = None,nrows=1) # read just first line for columns\n",
        "#columns = train_data.columns.tolist() # get the columns\n",
        "#cols_to_use = columns[:len(columns)-1] \n",
        "train_data=pd.read_csv('/content/drive/MyDrive/KDDTrain+.csv',header=None)\n",
        "train_data1=pd.read_csv('/content/drive/MyDrive/KDDTrain+.csv',header=None)\n",
        "train_data = train_data.iloc[:, :-1]# drop the last one\n",
        "test_data=pd.read_csv('/content/drive/MyDrive/KDDTest+.csv',header=None)\n",
        "test_data = test_data.iloc[:, :-1]# drop the last one"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BEplQGjzx15"
      },
      "source": [
        "Data nemerisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5_5KUiyzxSp"
      },
      "source": [
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "enc = OrdinalEncoder()\n",
        "train_data[[1]] = enc.fit_transform(train_data[[1]])\n",
        "test_data[[1]] = enc.fit_transform(test_data[[1]])\n",
        "\n",
        "train_data[[2]] = enc.fit_transform(train_data[[2]])\n",
        "test_data[[2]] = enc.fit_transform(test_data[[2]])\n",
        "\n",
        "train_data[[3]] = enc.fit_transform(train_data[[3]])\n",
        "test_data[[3]] = enc.fit_transform(test_data[[3]])"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmrrpPmpz5zu"
      },
      "source": [
        "Data scaling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaDvNxT97K_X"
      },
      "source": [
        "Train and test data formatting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vbi5xXhgsu1K"
      },
      "source": [
        "train_X = train_data.values[:,:-1]\n",
        "train_y = pd.factorize(train_data.values[:,-1])\n",
        "test_X = test_data.values[:,:-1]\n",
        "test_y = pd.factorize(test_data.values[:,-1])"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7DFNv1M6H5G"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "train_X=scaler.fit_transform(train_X)\n",
        "test_X = scaler.fit_transform(test_X)"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UI-nGqLG42QG",
        "outputId": "d2e5f87b-db9c-4fc7-db67-7a9b402df9a4"
      },
      "source": [
        "train_X.shape"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(125973, 41)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "7WVaZZg6JMog",
        "outputId": "211361cd-d338-4197-d095-2d238e5d9510"
      },
      "source": [
        "train_data1"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>tcp</td>\n",
              "      <td>ftp_data</td>\n",
              "      <td>SF</td>\n",
              "      <td>491</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>150</td>\n",
              "      <td>25</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.00</td>\n",
              "      <td>normal</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>udp</td>\n",
              "      <td>other</td>\n",
              "      <td>SF</td>\n",
              "      <td>146</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.00</td>\n",
              "      <td>255</td>\n",
              "      <td>1</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>normal</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>tcp</td>\n",
              "      <td>private</td>\n",
              "      <td>S0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>123</td>\n",
              "      <td>6</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.00</td>\n",
              "      <td>255</td>\n",
              "      <td>26</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>dos</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>tcp</td>\n",
              "      <td>http</td>\n",
              "      <td>SF</td>\n",
              "      <td>232</td>\n",
              "      <td>8153</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>30</td>\n",
              "      <td>255</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.01</td>\n",
              "      <td>normal</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>tcp</td>\n",
              "      <td>http</td>\n",
              "      <td>SF</td>\n",
              "      <td>199</td>\n",
              "      <td>420</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>32</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.09</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>normal</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125968</th>\n",
              "      <td>0</td>\n",
              "      <td>tcp</td>\n",
              "      <td>private</td>\n",
              "      <td>S0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>184</td>\n",
              "      <td>25</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.00</td>\n",
              "      <td>255</td>\n",
              "      <td>25</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>dos</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125969</th>\n",
              "      <td>8</td>\n",
              "      <td>udp</td>\n",
              "      <td>private</td>\n",
              "      <td>SF</td>\n",
              "      <td>105</td>\n",
              "      <td>145</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>255</td>\n",
              "      <td>244</td>\n",
              "      <td>0.96</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>normal</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125970</th>\n",
              "      <td>0</td>\n",
              "      <td>tcp</td>\n",
              "      <td>smtp</td>\n",
              "      <td>SF</td>\n",
              "      <td>2231</td>\n",
              "      <td>384</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>255</td>\n",
              "      <td>30</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.72</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.00</td>\n",
              "      <td>normal</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125971</th>\n",
              "      <td>0</td>\n",
              "      <td>tcp</td>\n",
              "      <td>klogin</td>\n",
              "      <td>S0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>144</td>\n",
              "      <td>8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.00</td>\n",
              "      <td>255</td>\n",
              "      <td>8</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>dos</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125972</th>\n",
              "      <td>0</td>\n",
              "      <td>tcp</td>\n",
              "      <td>ftp_data</td>\n",
              "      <td>SF</td>\n",
              "      <td>151</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>255</td>\n",
              "      <td>77</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>normal</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>125973 rows × 43 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0    1         2   3     4     5   ...    37    38    39    40      41  42\n",
              "0        0  tcp  ftp_data  SF   491     0  ...  0.00  0.00  0.05  0.00  normal  20\n",
              "1        0  udp     other  SF   146     0  ...  0.00  0.00  0.00  0.00  normal  15\n",
              "2        0  tcp   private  S0     0     0  ...  1.00  1.00  0.00  0.00     dos  19\n",
              "3        0  tcp      http  SF   232  8153  ...  0.03  0.01  0.00  0.01  normal  21\n",
              "4        0  tcp      http  SF   199   420  ...  0.00  0.00  0.00  0.00  normal  21\n",
              "...     ..  ...       ...  ..   ...   ...  ...   ...   ...   ...   ...     ...  ..\n",
              "125968   0  tcp   private  S0     0     0  ...  1.00  1.00  0.00  0.00     dos  20\n",
              "125969   8  udp   private  SF   105   145  ...  0.00  0.00  0.00  0.00  normal  21\n",
              "125970   0  tcp      smtp  SF  2231   384  ...  0.72  0.00  0.01  0.00  normal  18\n",
              "125971   0  tcp    klogin  S0     0     0  ...  1.00  1.00  0.00  0.00     dos  20\n",
              "125972   0  tcp  ftp_data  SF   151     0  ...  0.00  0.00  0.00  0.00  normal  21\n",
              "\n",
              "[125973 rows x 43 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "4kxf7eSfF9Vx",
        "outputId": "1bf83861-6750-4943-fdba-823617f9486c"
      },
      "source": [
        "train_data"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>491</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>150</td>\n",
              "      <td>25</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.00</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>43.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>146</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.00</td>\n",
              "      <td>255</td>\n",
              "      <td>1</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>48.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>123</td>\n",
              "      <td>6</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.00</td>\n",
              "      <td>255</td>\n",
              "      <td>26</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>dos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>232</td>\n",
              "      <td>8153</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>30</td>\n",
              "      <td>255</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.01</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>199</td>\n",
              "      <td>420</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>32</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.09</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125968</th>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>48.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>184</td>\n",
              "      <td>25</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.00</td>\n",
              "      <td>255</td>\n",
              "      <td>25</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>dos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125969</th>\n",
              "      <td>8</td>\n",
              "      <td>2.0</td>\n",
              "      <td>48.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>105</td>\n",
              "      <td>145</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>255</td>\n",
              "      <td>244</td>\n",
              "      <td>0.96</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125970</th>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>54.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>2231</td>\n",
              "      <td>384</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>255</td>\n",
              "      <td>30</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.72</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.00</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125971</th>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>29.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>144</td>\n",
              "      <td>8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.00</td>\n",
              "      <td>255</td>\n",
              "      <td>8</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>dos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125972</th>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>151</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>255</td>\n",
              "      <td>77</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>125973 rows × 42 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0    1     2    3     4     5   ...    36    37    38    39    40      41\n",
              "0        0  1.0  20.0  9.0   491     0  ...  0.00  0.00  0.00  0.05  0.00  normal\n",
              "1        0  2.0  43.0  9.0   146     0  ...  0.00  0.00  0.00  0.00  0.00  normal\n",
              "2        0  1.0  48.0  5.0     0     0  ...  0.00  1.00  1.00  0.00  0.00     dos\n",
              "3        0  1.0  24.0  9.0   232  8153  ...  0.04  0.03  0.01  0.00  0.01  normal\n",
              "4        0  1.0  24.0  9.0   199   420  ...  0.00  0.00  0.00  0.00  0.00  normal\n",
              "...     ..  ...   ...  ...   ...   ...  ...   ...   ...   ...   ...   ...     ...\n",
              "125968   0  1.0  48.0  5.0     0     0  ...  0.00  1.00  1.00  0.00  0.00     dos\n",
              "125969   8  2.0  48.0  9.0   105   145  ...  0.00  0.00  0.00  0.00  0.00  normal\n",
              "125970   0  1.0  54.0  9.0  2231   384  ...  0.00  0.72  0.00  0.01  0.00  normal\n",
              "125971   0  1.0  29.0  5.0     0     0  ...  0.00  1.00  1.00  0.00  0.00     dos\n",
              "125972   0  1.0  20.0  9.0   151     0  ...  0.00  0.00  0.00  0.00  0.00  normal\n",
              "\n",
              "[125973 rows x 42 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-4aCMAUrn3D"
      },
      "source": [
        "#Import Module\n",
        "#from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# train_X, test_X, train_y, test_y = train_test_split(X, y, \n",
        "#                                                     train_size=0.7,\n",
        "#                                                     test_size=0.3,\n",
        "#                                                     random_state=122)\n",
        "# print(\"Labels for training and testing data\")\n",
        "# print(train_y)\n",
        "# print(test_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7FOxrdJ2tye"
      },
      "source": [
        "# reshape input to be [samples, time steps, features]\n",
        "import keras\n",
        "\n",
        "#X_train = np.reshape(data.values[:,:-1].astype('float32'), (data.values[:,:-1].shape[0], 1, data.values[:,:-1].shape[1]))\n",
        "X_train = np.reshape(train_X.astype('float32'), (train_X.shape[0], 1, train_X.shape[1]))\n",
        "X_test = np.reshape(test_X.astype('float32'), (test_X.shape[0], 1, test_X.shape[1]))\n",
        "#y_train = keras.utils.to_categorical(pd.factorize(data.values[:,77])[0],15)\n",
        "y_train = keras.utils.to_categorical(train_y[0],5)\n",
        "y_train = y_train.astype('int')\n",
        "y_train = np.reshape(y_train,(y_train.shape[0],1,y_train.shape[1]))\n",
        "y_test = keras.utils.to_categorical(test_y[0],5)\n",
        "y_test = y_test.astype('int')\n",
        "y_test = np.reshape(y_test,(y_test.shape[0],1,y_test.shape[1]))"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sX2VddNf7iyt"
      },
      "source": [
        "Define the network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhuIuz4p00I8"
      },
      "source": [
        "# 1. define the network\n",
        "import time\n",
        "data_dim = X_train.shape[1]\n",
        "timesteps = 1\n",
        "num_classes = y_train.shape[1]\n",
        "batch_size = 512\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(4, input_dim=X_train.shape[1]))  # try using a GRU instead, for fun\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(5))\n",
        "model.add(Activation('softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bmi9FeVpM7z8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        },
        "outputId": "56b6b64c-7698-4573-d789-b88218d60522"
      },
      "source": [
        "import time\n",
        "data_dim = X_train.shape[1]\n",
        "timesteps = 1\n",
        "num_classes = y_train.shape[1]\n",
        "batch_size = 1\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, return_sequences=True, stateful=False, batch_input_shape=(batch_size, timesteps, data_dim)))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(LSTM(50, return_sequences=True, stateful=False))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(LSTM(50, stateful=True))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(15, activation='softmax'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "start_time = time.time()\n",
        "model.fit(X_train, y_train,batch_size=batch_size, epochs=2, shuffle=False)\n",
        "finish_time = time.time() - start_time\n",
        "model.save(\"/content/drive/My Drive/final_model_NSL-KDD_brute_Multiclass.h5\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-2101a1fc50e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mfinish_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/My Drive/final_model_NSL-KDD_brute_Multiclass.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    869\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    724\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    725\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 726\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2967\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2968\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2969\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2970\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3204\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3205\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3206\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3207\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3208\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n        return step_function(self, iterator)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:795 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:788 run_step  **\n        outputs = model.train_step(data)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:754 train_step\n        y_pred = self(x, training=True)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py:998 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/input_spec.py:274 assert_input_compatibility\n        ', found shape=' + display_shape(x.shape))\n\n    ValueError: Input 0 is incompatible with layer sequential_1: expected shape=(None, None, 1), found shape=(1, 1, 41)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2X1EbkTFiow",
        "outputId": "54483cc3-dd1a-41d9-9a3a-0a9d846f35e5"
      },
      "source": [
        "import time\n",
        "#train_object_num=len(train_data)\n",
        "#print(test[0])\n",
        "#model training\n",
        "batch_size = 1024\n",
        "model=Sequential()\n",
        "model.add(LSTM(X_train.shape[1],input_dim=X_train.shape[2],return_sequences=True,kernel_initializer='uniform',activation='relu'))\n",
        "model.add(LSTM(32,kernel_initializer='uniform',return_sequences=True,activation='relu'))\n",
        "#model.add(LSTM(128,kernel_initializer='uniform',activation='relu'))\n",
        "model.add(Dense(256,activation='relu'))\n",
        "model.add(Dense(128,activation='relu'))\n",
        "model.add(Dense(64,activation='relu'))\n",
        "model.add(Dense(16,activation='relu'))\n",
        "model.add(Dense(y_train.shape[2],activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy',optimizer='NADAM',metrics=['accuracy'])\n",
        "model.summary()\n",
        "start_time = time.time()\n",
        "history = model.fit(X_train,y_train,validation_split=.3,epochs=100,batch_size=batch_size,verbose=1)\n",
        "finish_time = time.time() - start_time\n",
        "#model.save(\"/content/drive/My Drive/model_NSL-KDD_42col_Multiclass_valsplit.3_bs1024_100ep.h5\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_6 (LSTM)                (None, None, 1)           172       \n",
            "_________________________________________________________________\n",
            "lstm_7 (LSTM)                (None, None, 32)          4352      \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, None, 256)         8448      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, None, 128)         32896     \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, None, 64)          8256      \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, None, 16)          1040      \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, None, 5)           85        \n",
            "=================================================================\n",
            "Total params: 55,249\n",
            "Trainable params: 55,249\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "87/87 [==============================] - 6s 30ms/step - loss: 0.5450 - accuracy: 0.3875 - val_loss: 0.2389 - val_accuracy: 0.5351\n",
            "Epoch 2/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.1887 - accuracy: 0.7804 - val_loss: 0.1451 - val_accuracy: 0.8590\n",
            "Epoch 3/100\n",
            "87/87 [==============================] - 2s 22ms/step - loss: 0.1383 - accuracy: 0.8676 - val_loss: 0.1119 - val_accuracy: 0.9018\n",
            "Epoch 4/100\n",
            "87/87 [==============================] - 2s 23ms/step - loss: 0.1005 - accuracy: 0.9154 - val_loss: 0.0790 - val_accuracy: 0.9386\n",
            "Epoch 5/100\n",
            "87/87 [==============================] - 2s 22ms/step - loss: 0.0736 - accuracy: 0.9436 - val_loss: 0.0646 - val_accuracy: 0.9510\n",
            "Epoch 6/100\n",
            "87/87 [==============================] - 2s 19ms/step - loss: 0.0625 - accuracy: 0.9537 - val_loss: 0.0595 - val_accuracy: 0.9562\n",
            "Epoch 7/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0575 - accuracy: 0.9589 - val_loss: 0.0723 - val_accuracy: 0.9558\n",
            "Epoch 8/100\n",
            "87/87 [==============================] - 2s 22ms/step - loss: 0.0584 - accuracy: 0.9585 - val_loss: 0.0543 - val_accuracy: 0.9622\n",
            "Epoch 9/100\n",
            "87/87 [==============================] - 2s 19ms/step - loss: 0.0528 - accuracy: 0.9640 - val_loss: 0.0552 - val_accuracy: 0.9633\n",
            "Epoch 10/100\n",
            "87/87 [==============================] - 2s 19ms/step - loss: 0.0522 - accuracy: 0.9643 - val_loss: 0.0626 - val_accuracy: 0.9576\n",
            "Epoch 11/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0520 - accuracy: 0.9646 - val_loss: 0.0512 - val_accuracy: 0.9663\n",
            "Epoch 12/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0494 - accuracy: 0.9668 - val_loss: 0.0504 - val_accuracy: 0.9660\n",
            "Epoch 13/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0485 - accuracy: 0.9670 - val_loss: 0.0492 - val_accuracy: 0.9667\n",
            "Epoch 14/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0474 - accuracy: 0.9682 - val_loss: 0.0496 - val_accuracy: 0.9670\n",
            "Epoch 15/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0466 - accuracy: 0.9691 - val_loss: 0.0515 - val_accuracy: 0.9671\n",
            "Epoch 16/100\n",
            "87/87 [==============================] - 2s 20ms/step - loss: 0.0474 - accuracy: 0.9685 - val_loss: 0.0474 - val_accuracy: 0.9701\n",
            "Epoch 17/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0463 - accuracy: 0.9692 - val_loss: 0.0467 - val_accuracy: 0.9706\n",
            "Epoch 18/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0457 - accuracy: 0.9701 - val_loss: 0.0464 - val_accuracy: 0.9698\n",
            "Epoch 19/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0460 - accuracy: 0.9697 - val_loss: 0.0457 - val_accuracy: 0.9705\n",
            "Epoch 20/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0436 - accuracy: 0.9715 - val_loss: 0.0495 - val_accuracy: 0.9689\n",
            "Epoch 21/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0445 - accuracy: 0.9706 - val_loss: 0.0453 - val_accuracy: 0.9712\n",
            "Epoch 22/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0450 - accuracy: 0.9707 - val_loss: 0.0450 - val_accuracy: 0.9704\n",
            "Epoch 23/100\n",
            "87/87 [==============================] - 2s 22ms/step - loss: 0.0444 - accuracy: 0.9707 - val_loss: 0.0447 - val_accuracy: 0.9705\n",
            "Epoch 24/100\n",
            "87/87 [==============================] - 2s 22ms/step - loss: 0.0445 - accuracy: 0.9704 - val_loss: 0.0454 - val_accuracy: 0.9704\n",
            "Epoch 25/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0445 - accuracy: 0.9705 - val_loss: 0.0441 - val_accuracy: 0.9707\n",
            "Epoch 26/100\n",
            "87/87 [==============================] - 2s 19ms/step - loss: 0.0440 - accuracy: 0.9709 - val_loss: 0.0440 - val_accuracy: 0.9708\n",
            "Epoch 27/100\n",
            "87/87 [==============================] - 2s 20ms/step - loss: 0.0436 - accuracy: 0.9710 - val_loss: 0.0453 - val_accuracy: 0.9714\n",
            "Epoch 28/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0441 - accuracy: 0.9709 - val_loss: 0.0447 - val_accuracy: 0.9705\n",
            "Epoch 29/100\n",
            "87/87 [==============================] - 2s 19ms/step - loss: 0.0428 - accuracy: 0.9714 - val_loss: 0.0448 - val_accuracy: 0.9701\n",
            "Epoch 30/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0424 - accuracy: 0.9715 - val_loss: 0.0440 - val_accuracy: 0.9718\n",
            "Epoch 31/100\n",
            "87/87 [==============================] - 2s 22ms/step - loss: 0.0559 - accuracy: 0.9640 - val_loss: 0.0477 - val_accuracy: 0.9689\n",
            "Epoch 32/100\n",
            "87/87 [==============================] - 2s 22ms/step - loss: 0.0440 - accuracy: 0.9707 - val_loss: 0.0437 - val_accuracy: 0.9712\n",
            "Epoch 33/100\n",
            "87/87 [==============================] - 2s 22ms/step - loss: 0.0429 - accuracy: 0.9718 - val_loss: 0.0428 - val_accuracy: 0.9718\n",
            "Epoch 34/100\n",
            "87/87 [==============================] - 2s 22ms/step - loss: 0.0421 - accuracy: 0.9720 - val_loss: 0.0426 - val_accuracy: 0.9720\n",
            "Epoch 35/100\n",
            "87/87 [==============================] - 2s 22ms/step - loss: 0.0413 - accuracy: 0.9727 - val_loss: 0.0443 - val_accuracy: 0.9714\n",
            "Epoch 36/100\n",
            "87/87 [==============================] - 2s 22ms/step - loss: 0.0396 - accuracy: 0.9742 - val_loss: 0.0432 - val_accuracy: 0.9718\n",
            "Epoch 37/100\n",
            "87/87 [==============================] - 2s 22ms/step - loss: 0.0410 - accuracy: 0.9730 - val_loss: 0.0442 - val_accuracy: 0.9714\n",
            "Epoch 38/100\n",
            "87/87 [==============================] - 2s 22ms/step - loss: 0.0417 - accuracy: 0.9719 - val_loss: 0.0424 - val_accuracy: 0.9714\n",
            "Epoch 39/100\n",
            "87/87 [==============================] - 2s 20ms/step - loss: 0.0410 - accuracy: 0.9729 - val_loss: 0.0421 - val_accuracy: 0.9720\n",
            "Epoch 40/100\n",
            "87/87 [==============================] - 2s 20ms/step - loss: 0.0408 - accuracy: 0.9726 - val_loss: 0.0425 - val_accuracy: 0.9720\n",
            "Epoch 41/100\n",
            "87/87 [==============================] - 2s 20ms/step - loss: 0.0406 - accuracy: 0.9730 - val_loss: 0.0447 - val_accuracy: 0.9708\n",
            "Epoch 42/100\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 0.0416 - accuracy: 0.9724 - val_loss: 0.0422 - val_accuracy: 0.9725\n",
            "Epoch 43/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0405 - accuracy: 0.9732 - val_loss: 0.0455 - val_accuracy: 0.9711\n",
            "Epoch 44/100\n",
            "87/87 [==============================] - 2s 22ms/step - loss: 0.0417 - accuracy: 0.9724 - val_loss: 0.0429 - val_accuracy: 0.9718\n",
            "Epoch 45/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0410 - accuracy: 0.9720 - val_loss: 0.0427 - val_accuracy: 0.9726\n",
            "Epoch 46/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0403 - accuracy: 0.9734 - val_loss: 0.0419 - val_accuracy: 0.9725\n",
            "Epoch 47/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0411 - accuracy: 0.9726 - val_loss: 0.0459 - val_accuracy: 0.9707\n",
            "Epoch 48/100\n",
            "87/87 [==============================] - 2s 22ms/step - loss: 0.0397 - accuracy: 0.9735 - val_loss: 0.0432 - val_accuracy: 0.9722\n",
            "Epoch 49/100\n",
            "87/87 [==============================] - 2s 20ms/step - loss: 0.0413 - accuracy: 0.9726 - val_loss: 0.0423 - val_accuracy: 0.9724\n",
            "Epoch 50/100\n",
            "87/87 [==============================] - 2s 22ms/step - loss: 0.0402 - accuracy: 0.9733 - val_loss: 0.0418 - val_accuracy: 0.9727\n",
            "Epoch 51/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0410 - accuracy: 0.9725 - val_loss: 0.0418 - val_accuracy: 0.9722\n",
            "Epoch 52/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0406 - accuracy: 0.9731 - val_loss: 0.0431 - val_accuracy: 0.9722\n",
            "Epoch 53/100\n",
            "87/87 [==============================] - 2s 22ms/step - loss: 0.0399 - accuracy: 0.9732 - val_loss: 0.0413 - val_accuracy: 0.9727\n",
            "Epoch 54/100\n",
            "87/87 [==============================] - 2s 22ms/step - loss: 0.0395 - accuracy: 0.9735 - val_loss: 0.0449 - val_accuracy: 0.9711\n",
            "Epoch 55/100\n",
            "87/87 [==============================] - 2s 23ms/step - loss: 0.0401 - accuracy: 0.9735 - val_loss: 0.0845 - val_accuracy: 0.9173\n",
            "Epoch 56/100\n",
            "87/87 [==============================] - 2s 22ms/step - loss: 0.0441 - accuracy: 0.9695 - val_loss: 0.0419 - val_accuracy: 0.9728\n",
            "Epoch 57/100\n",
            "87/87 [==============================] - 2s 22ms/step - loss: 0.0411 - accuracy: 0.9725 - val_loss: 0.0418 - val_accuracy: 0.9718\n",
            "Epoch 58/100\n",
            "87/87 [==============================] - 2s 22ms/step - loss: 0.0384 - accuracy: 0.9747 - val_loss: 0.0419 - val_accuracy: 0.9725\n",
            "Epoch 59/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0392 - accuracy: 0.9735 - val_loss: 0.0413 - val_accuracy: 0.9730\n",
            "Epoch 60/100\n",
            "87/87 [==============================] - 2s 20ms/step - loss: 0.0395 - accuracy: 0.9735 - val_loss: 0.0619 - val_accuracy: 0.9430\n",
            "Epoch 61/100\n",
            "87/87 [==============================] - 2s 19ms/step - loss: 0.0416 - accuracy: 0.9713 - val_loss: 0.0425 - val_accuracy: 0.9724\n",
            "Epoch 62/100\n",
            "87/87 [==============================] - 2s 22ms/step - loss: 0.0394 - accuracy: 0.9733 - val_loss: 0.0410 - val_accuracy: 0.9729\n",
            "Epoch 63/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0391 - accuracy: 0.9735 - val_loss: 0.0410 - val_accuracy: 0.9730\n",
            "Epoch 64/100\n",
            "87/87 [==============================] - 2s 22ms/step - loss: 0.0401 - accuracy: 0.9732 - val_loss: 0.0433 - val_accuracy: 0.9713\n",
            "Epoch 65/100\n",
            "87/87 [==============================] - 2s 22ms/step - loss: 0.0399 - accuracy: 0.9731 - val_loss: 0.0415 - val_accuracy: 0.9723\n",
            "Epoch 66/100\n",
            "87/87 [==============================] - 2s 22ms/step - loss: 0.0402 - accuracy: 0.9726 - val_loss: 0.0407 - val_accuracy: 0.9729\n",
            "Epoch 67/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0393 - accuracy: 0.9740 - val_loss: 0.0408 - val_accuracy: 0.9730\n",
            "Epoch 68/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0396 - accuracy: 0.9735 - val_loss: 0.0553 - val_accuracy: 0.9516\n",
            "Epoch 69/100\n",
            "87/87 [==============================] - 2s 23ms/step - loss: 0.0411 - accuracy: 0.9712 - val_loss: 0.0428 - val_accuracy: 0.9730\n",
            "Epoch 70/100\n",
            "87/87 [==============================] - 2s 22ms/step - loss: 0.0386 - accuracy: 0.9743 - val_loss: 0.0447 - val_accuracy: 0.9717\n",
            "Epoch 71/100\n",
            "87/87 [==============================] - 2s 22ms/step - loss: 0.0401 - accuracy: 0.9731 - val_loss: 0.0415 - val_accuracy: 0.9731\n",
            "Epoch 72/100\n",
            "87/87 [==============================] - 2s 20ms/step - loss: 0.0395 - accuracy: 0.9737 - val_loss: 0.0591 - val_accuracy: 0.9468\n",
            "Epoch 73/100\n",
            "87/87 [==============================] - 2s 22ms/step - loss: 0.0407 - accuracy: 0.9718 - val_loss: 0.0420 - val_accuracy: 0.9729\n",
            "Epoch 74/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0392 - accuracy: 0.9737 - val_loss: 0.0415 - val_accuracy: 0.9731\n",
            "Epoch 75/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0397 - accuracy: 0.9738 - val_loss: 0.0408 - val_accuracy: 0.9729\n",
            "Epoch 76/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0394 - accuracy: 0.9740 - val_loss: 0.0411 - val_accuracy: 0.9735\n",
            "Epoch 77/100\n",
            "87/87 [==============================] - 2s 22ms/step - loss: 0.0395 - accuracy: 0.9740 - val_loss: 0.0408 - val_accuracy: 0.9731\n",
            "Epoch 78/100\n",
            "87/87 [==============================] - 2s 22ms/step - loss: 0.0388 - accuracy: 0.9746 - val_loss: 0.0405 - val_accuracy: 0.9732\n",
            "Epoch 79/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0395 - accuracy: 0.9736 - val_loss: 0.0417 - val_accuracy: 0.9720\n",
            "Epoch 80/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0395 - accuracy: 0.9743 - val_loss: 0.0408 - val_accuracy: 0.9726\n",
            "Epoch 81/100\n",
            "87/87 [==============================] - 2s 22ms/step - loss: 0.0394 - accuracy: 0.9736 - val_loss: 0.0468 - val_accuracy: 0.9676\n",
            "Epoch 82/100\n",
            "87/87 [==============================] - 2s 23ms/step - loss: 0.0407 - accuracy: 0.9727 - val_loss: 0.0405 - val_accuracy: 0.9734\n",
            "Epoch 83/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0388 - accuracy: 0.9743 - val_loss: 0.0447 - val_accuracy: 0.9716\n",
            "Epoch 84/100\n",
            "87/87 [==============================] - 2s 23ms/step - loss: 0.0394 - accuracy: 0.9744 - val_loss: 0.0419 - val_accuracy: 0.9734\n",
            "Epoch 85/100\n",
            "87/87 [==============================] - 2s 23ms/step - loss: 0.0395 - accuracy: 0.9741 - val_loss: 0.0419 - val_accuracy: 0.9725\n",
            "Epoch 86/100\n",
            "87/87 [==============================] - 2s 22ms/step - loss: 0.0394 - accuracy: 0.9742 - val_loss: 0.0407 - val_accuracy: 0.9738\n",
            "Epoch 87/100\n",
            "87/87 [==============================] - 2s 22ms/step - loss: 0.0375 - accuracy: 0.9753 - val_loss: 0.0420 - val_accuracy: 0.9732\n",
            "Epoch 88/100\n",
            "87/87 [==============================] - 2s 20ms/step - loss: 0.0385 - accuracy: 0.9736 - val_loss: 0.0416 - val_accuracy: 0.9731\n",
            "Epoch 89/100\n",
            "87/87 [==============================] - 2s 23ms/step - loss: 0.0401 - accuracy: 0.9734 - val_loss: 0.0434 - val_accuracy: 0.9729\n",
            "Epoch 90/100\n",
            "87/87 [==============================] - 2s 22ms/step - loss: 0.0394 - accuracy: 0.9745 - val_loss: 0.0440 - val_accuracy: 0.9721\n",
            "Epoch 91/100\n",
            "87/87 [==============================] - 2s 22ms/step - loss: 0.0387 - accuracy: 0.9749 - val_loss: 0.0419 - val_accuracy: 0.9736\n",
            "Epoch 92/100\n",
            "87/87 [==============================] - 2s 22ms/step - loss: 0.0388 - accuracy: 0.9745 - val_loss: 0.0416 - val_accuracy: 0.9730\n",
            "Epoch 93/100\n",
            "87/87 [==============================] - 2s 23ms/step - loss: 0.0390 - accuracy: 0.9740 - val_loss: 0.0414 - val_accuracy: 0.9731\n",
            "Epoch 94/100\n",
            "87/87 [==============================] - 2s 22ms/step - loss: 0.0393 - accuracy: 0.9737 - val_loss: 0.0429 - val_accuracy: 0.9717\n",
            "Epoch 95/100\n",
            "87/87 [==============================] - 2s 22ms/step - loss: 0.0385 - accuracy: 0.9746 - val_loss: 0.0412 - val_accuracy: 0.9736\n",
            "Epoch 96/100\n",
            "87/87 [==============================] - 2s 22ms/step - loss: 0.0390 - accuracy: 0.9744 - val_loss: 0.0401 - val_accuracy: 0.9738\n",
            "Epoch 97/100\n",
            "87/87 [==============================] - 2s 22ms/step - loss: 0.0389 - accuracy: 0.9739 - val_loss: 0.0420 - val_accuracy: 0.9736\n",
            "Epoch 98/100\n",
            "87/87 [==============================] - 2s 22ms/step - loss: 0.0384 - accuracy: 0.9747 - val_loss: 0.0430 - val_accuracy: 0.9730\n",
            "Epoch 99/100\n",
            "87/87 [==============================] - 2s 22ms/step - loss: 0.0394 - accuracy: 0.9737 - val_loss: 0.0405 - val_accuracy: 0.9735\n",
            "Epoch 100/100\n",
            "87/87 [==============================] - 2s 22ms/step - loss: 0.0394 - accuracy: 0.9736 - val_loss: 0.0406 - val_accuracy: 0.9738\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgFh6eY-RogJ",
        "outputId": "25eea94a-912f-4c04-be35-a5763adb26f9"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "loss, accuracy = model.evaluate(X_test,y_test)\n",
        "print(\"\\nLoss: %.2f, Accuracy: %.2f%%\" % (loss, accuracy*100))\n",
        "y_pred = model.predict_classes(X_test)\n",
        "target_names = ['Normal', 'Dos', 'Probe','R2L', 'U2R']\n",
        "print(classification_report(y_true = np.transpose(test_y[0]), y_pred = y_pred))\n",
        "#print(accuracy_score(y_true = np.transpose(test_y[0]), y_pred = y_pred))\n",
        "print(\"execution time :  \",finish_time)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "705/705 [==============================] - 1s 1ms/step - loss: 2.9198 - accuracy: 0.0749\n",
            "\n",
            "Loss: 2.92, Accuracy: 7.49%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.09      0.16      0.12      7458\n",
            "           1       0.05      0.03      0.03      9710\n",
            "           2       0.72      0.07      0.14      2421\n",
            "           3       0.01      0.01      0.01      2754\n",
            "           4       0.00      0.00      0.00       200\n",
            "\n",
            "    accuracy                           0.07     22543\n",
            "   macro avg       0.17      0.06      0.06     22543\n",
            "weighted avg       0.13      0.07      0.07     22543\n",
            "\n",
            "execution time :   191.17897701263428\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBLLxMkV25MY",
        "outputId": "1cd20db3-6f7d-45b6-e3d5-521895213215"
      },
      "source": [
        "model.metrics_names"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['loss', 'accuracy']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqrg8ftGdlZd",
        "outputId": "aace09a7-f2e0-47bb-adba-465e6711a6c6"
      },
      "source": [
        "test_y.sh"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 1, ..., 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPW7sxx-yiwT",
        "outputId": "72de27e0-0eaa-46a9-ccfa-5e71a66065a6"
      },
      "source": [
        "s= np.transpose(test_y[0])\n",
        "s.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(22543,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c98fMqo4yf5b",
        "outputId": "82a77c16-52b0-4d62-b7c4-3bc1116832bb"
      },
      "source": [
        "y_pred.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(22543, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vn7ng4MLMOb"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix,  roc_auc_score, roc_curve\n",
        "import sklearn.metrics as metrics\n",
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "score = model.evaluate(X_test, y_test, batch_size)\n",
        "y_pred = model.predict(X_test, batch_size= batch_size)\n",
        "y_pred = y_pred.astype('int')\n",
        "#y_pred = [np.round(x) for x in y_pred]\n",
        "#y_pred = np.array(y_pred)\n",
        "y2 = np.array(test_y[0])\n",
        "\n",
        "#print(y2)\n",
        "print(classification_report(y_test,y_pred,\"multiclass\"))\n",
        "cm = confusion_matrix(y2.argmax(axis=1), y_pred.argmax(axis=1))\n",
        "print(cm)\n",
        "\n",
        "\n",
        "probs = model.predict_proba(X_test,batch_size=batch_size)\n",
        "#y_preds = np.transpose([pred[:, 1] for pred in probs])\n",
        "#y_preds=y_preds.reshape(1,-1)\n",
        "fpr, tpr, threshold = metrics.roc_auc_score(y_score==np.transpose(y_test),y_true= np.transpose(probs),multi_class='ovo')\n",
        "roc_auc = metrics.auc(fpr, tpr)\n",
        "\n",
        "# method I: plt\n",
        "import matplotlib.pyplot as plt\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
        "plt.legend(loc = 'lower right')\n",
        "plt.plot([0, 1], [0, 1],'r--')\n",
        "plt.xlim([0, 1])\n",
        "plt.ylim([0, 1])\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bq6QuCZu2BgD"
      },
      "source": [
        "# try using different optimizers and different optimizer configs\n",
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "#checkpointer = callbacks.ModelCheckpoint(filepath=\"kddresults/lstm1layer/checkpoint-{epoch:02d}.hdf5\", verbose=1, save_best_only=True, monitor='val_acc',mode='max')\n",
        "#csv_logger = CSVLogger('training_set.csv',separator=',', append=False)\n",
        "start_time = time.time()\n",
        "model.fit(X_train, y_train, batch_size=batch_size, epochs=2, validation_split=.2)#,callbacks=[checkpointer,csv_logger]\n",
        "finish_time = time.time() - start_time\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieGpV2zYXZwV"
      },
      "source": [
        "#saving model\n",
        "model.save(\"/content/drive/MyDrive/NSL-KDD_42norm_results_5Dense_po_Dropout_200epoch_1024batch_model.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xLvXBh0SyWh",
        "outputId": "759c83ac-9279-4d13-e4d4-f9c857e325bc"
      },
      "source": [
        "from keras.models import load_model\n",
        "#model = load_model('/content/drive/MyDrive/NSL-KDD_results_SimpleRNN_model.hdf5')\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"\\nLoss: %.2f, Accuracy: %.2f%%\" % (loss, accuracy*100))\n",
        "\n",
        "#print(\"--- %s seconds ---\",finish_time)\n",
        "y_pred = model.predict_classes(X_train)\n",
        "#np.savetxt('/content/drive/MyDrive/NSL-KDD_results_SimpleRNN_predicted.txt', np.transpose([y_test,y_pred]), fmt='%s')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "705/705 [==============================] - 4s 6ms/step - loss: 19.9034 - accuracy: 0.0648\n",
            "\n",
            "Loss: 19.90, Accuracy: 6.48%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRdRVMFrQSrR",
        "outputId": "5c60ba9a-0ef4-413f-ac6b-2be3fae6c6b4"
      },
      "source": [
        "print(y_train)\n",
        "y_pred"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[1 0 0 0 0]]\n",
            "\n",
            " [[1 0 0 0 0]]\n",
            "\n",
            " [[0 1 0 0 0]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[1 0 0 0 0]]\n",
            "\n",
            " [[0 1 0 0 0]]\n",
            "\n",
            " [[1 0 0 0 0]]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0],\n",
              "       [0],\n",
              "       [1],\n",
              "       ...,\n",
              "       [0],\n",
              "       [1],\n",
              "       [0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqAFEhU4_CMH",
        "outputId": "dc7ca7e5-50c4-4595-ba53-6ba7104f0767"
      },
      "source": [
        "42 col pca 80% : b_s=256 epochs= 300 Lstm(32,32)->Dense(256,128,64,ytrain.shape(2))   ->   accuracy = 62.8%\n",
        "\n",
        "\n",
        "24 col normalisé : b_s=1024 epochs= 100 Lstm(32,32)->Dense(256,128,64,16,ytrain.shape(2))   ->   accuracy = 83.73%\n",
        "24 col pca 80% : b_s=1024 epochs= 100 Lstm(32,32)->Dense(256,128,64,16,ytrain.shape(2))   ->   accuracy = 40.28%\n",
        "42 col normalisé : b_s=1024 epochs= 100 Lstm(32,32)->Dense(256,128,64,16,ytrain.shape(2))   ->   accuracy = 48.52%\n",
        "42 col pca 80% : b_s=1024 epochs= 100 Lstm(32,32)->Dense(256,128,64,16,ytrain.shape(2))   ->   accuracy = 45.22%\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<tensorflow.python.keras.engine.sequential.Sequential object at 0x7f324f860ba8>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZpW4B3GT5_ac",
        "outputId": "0735ebab-cf15-4ca5-fa46-5178521d0915"
      },
      "source": [
        "batch_size= 1024\n",
        "lrate = 0.1\n",
        "model = Sequential()\n",
        "model.add(Dense(X_train.shape[2], input_dim=X_train.shape[2], activation='relu', kernel_initializer='he_uniform'))\n",
        "model.add(Dense(1024, input_dim=X_train.shape[2], activation='relu', kernel_initializer='he_uniform'))\n",
        "#model.add(Dropout(0.1))\n",
        "model.add(Dense(768, input_dim=X_train.shape[2], activation='relu', kernel_initializer='he_uniform'))\n",
        "#model.add(Dropout(0.1))\n",
        "model.add(Dense(512, input_dim=X_train.shape[2], activation='relu', kernel_initializer='he_uniform'))\n",
        "#model.add(Dropout(0.1))\n",
        "model.add(Dense(256, input_dim=X_train.shape[2], activation='relu', kernel_initializer='he_uniform'))\n",
        "#model.add(Dropout(0.1))\n",
        "model.add(Dense(128, input_dim=X_train.shape[2], activation='relu', kernel_initializer='he_uniform'))\n",
        "#model.add(Dropout(0.1))\n",
        "model.add(Dense(y_train.shape[2], activation='softmax'))\n",
        "\t# compile model\n",
        "opt = SGD(lr=lrate)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "# fit model\n",
        "history = model.fit(X_train, y_train, validation_split=.3,batch_size=batch_size, epochs=200, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 41) for input KerasTensor(type_spec=TensorSpec(shape=(None, 41), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 1, 41).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 41) for input KerasTensor(type_spec=TensorSpec(shape=(None, 41), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 1, 41).\n",
            "86/87 [============================>.] - ETA: 0s - loss: 0.8852 - accuracy: 0.8587WARNING:tensorflow:Model was constructed with shape (None, 41) for input KerasTensor(type_spec=TensorSpec(shape=(None, 41), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 1, 41).\n",
            "87/87 [==============================] - 18s 194ms/step - loss: 0.8727 - accuracy: 0.8605 - val_loss: 0.0832 - val_accuracy: 0.9771\n",
            "Epoch 2/200\n",
            "87/87 [==============================] - 16s 188ms/step - loss: 0.0694 - accuracy: 0.9800 - val_loss: 0.0556 - val_accuracy: 0.9824\n",
            "Epoch 3/200\n",
            "87/87 [==============================] - 16s 188ms/step - loss: 0.0599 - accuracy: 0.9823 - val_loss: 0.0660 - val_accuracy: 0.9801\n",
            "Epoch 4/200\n",
            "87/87 [==============================] - 16s 186ms/step - loss: 0.0516 - accuracy: 0.9846 - val_loss: 0.0357 - val_accuracy: 0.9906\n",
            "Epoch 5/200\n",
            "87/87 [==============================] - 16s 186ms/step - loss: 0.0311 - accuracy: 0.9907 - val_loss: 0.0344 - val_accuracy: 0.9902\n",
            "Epoch 6/200\n",
            "87/87 [==============================] - 16s 188ms/step - loss: 0.0271 - accuracy: 0.9920 - val_loss: 0.0328 - val_accuracy: 0.9907\n",
            "Epoch 7/200\n",
            "48/87 [===============>..............] - ETA: 6s - loss: 0.0274 - accuracy: 0.9917"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "PW_Mq1ZZS1Nc",
        "outputId": "bef6bc13-2144-494a-b00c-cce41f8058e0"
      },
      "source": [
        "from matplotlib import pyplot\n",
        "# plot accuracy\n",
        "pyplot.plot(history.history['accuracy'], label='train')\n",
        "pyplot.plot(history.history['val_accuracy'], label='test')\n",
        "pyplot.title('lrate='+str(lrate), pad=-50)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'lrate=0.1')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "vV49FqMAb9E6",
        "outputId": "dfd0ee03-f813-476a-ac38-ae4fee4e591c"
      },
      "source": [
        "\n",
        "# plot leoss\n",
        "pyplot.plot(history.history['loss'], label='train')\n",
        "pyplot.plot(history.history['val_loss'], label='test')\n",
        "pyplot.title('lrate='+str(lrate), pad=-50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'lrate=0.1')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}