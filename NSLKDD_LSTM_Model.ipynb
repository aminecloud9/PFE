{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NSLKDD_LSTM_Model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "16_X2LMa1V7ENkqAtbATRAiCkL9sf9fXy",
      "authorship_tag": "ABX9TyNmZvOr4mnBNeW3r4inRNPZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aminecloud9/PFE/blob/main/NSLKDD_LSTM_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zubbW2t0qkI4"
      },
      "source": [
        "%matplotlib inline\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "from __future__ import print_function\r\n",
        "import numpy as np\r\n",
        "np.random.seed(1337)  # for reproducibility\r\n",
        "from keras.preprocessing import sequence\r\n",
        "from keras.utils import np_utils\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.optimizers import SGD\r\n",
        "from keras.layers import Dense, Dropout, Activation, Embedding\r\n",
        "from keras.layers import LSTM, SimpleRNN, GRU\r\n",
        "from keras.datasets import imdb\r\n",
        "from keras.utils.np_utils import to_categorical\r\n",
        "from sklearn.metrics import (precision_score, recall_score,\r\n",
        "                             f1_score, accuracy_score,mean_squared_error,mean_absolute_error)\r\n",
        "from sklearn import metrics\r\n",
        "from sklearn.preprocessing import Normalizer\r\n",
        "import h5py\r\n",
        "from keras import callbacks\r\n",
        "from keras import callbacks\r\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Px30HHOq6ihg"
      },
      "source": [
        "Download dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGP9ArVErwIu"
      },
      "source": [
        "#data=pd.read_csv('/content/drive/MyDrive/CICIDS2017_multi_class_StandardScaler_NormalsationResults.csv')\r\n",
        "\r\n",
        "train_data=pd.read_csv('/content/drive/MyDrive/Train_42col_normalisation.csv')\r\n",
        "test_data=pd.read_csv('/content/drive/MyDrive/Test_42col_normalisation.csv')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaDvNxT97K_X"
      },
      "source": [
        "Train and test data formatting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vbi5xXhgsu1K"
      },
      "source": [
        "X=train_data.values[:,1:-1]\r\n",
        "y=train_data.values[:,-1]\r\n",
        "train_X = X\r\n",
        "train_y = pd.factorize(y)\r\n",
        "test_X = test_data.values[:,1:-1]\r\n",
        "test_y = pd.factorize(test_data.values[:,-1])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfaE4iDWZJB9"
      },
      "source": [
        "test_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-4aCMAUrn3D"
      },
      "source": [
        "#Import Module\r\n",
        "#from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "\r\n",
        "# train_X, test_X, train_y, test_y = train_test_split(X, y, \r\n",
        "#                                                     train_size=0.7,\r\n",
        "#                                                     test_size=0.3,\r\n",
        "#                                                     random_state=122)\r\n",
        "# print(\"Labels for training and testing data\")\r\n",
        "# print(train_y)\r\n",
        "# print(test_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qa6201kxssMy",
        "outputId": "c6a3895f-85ed-455d-e54d-4e6900863d53"
      },
      "source": [
        "X_train.shape[1]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgIXyYp5sgq-",
        "outputId": "ad560c47-aafd-40fe-97d7-d3d5febf50ff"
      },
      "source": [
        "X_test.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(22543, 1, 41)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7FOxrdJ2tye"
      },
      "source": [
        "# reshape input to be [samples, time steps, features]\r\n",
        "import keras\r\n",
        "\r\n",
        "#X_train = np.reshape(data.values[:,:-1].astype('float32'), (data.values[:,:-1].shape[0], 1, data.values[:,:-1].shape[1]))\r\n",
        "X_train = np.reshape(train_X.astype('float32'), (train_X.shape[0], 1, train_X.shape[1]))\r\n",
        "X_test = np.reshape(test_X.astype('float32'), (test_X.shape[0], 1, test_X.shape[1]))\r\n",
        "#y_train = keras.utils.to_categorical(pd.factorize(data.values[:,77])[0],15)\r\n",
        "y_train = keras.utils.to_categorical(train_y[0],5)\r\n",
        "y_train = y_train.astype('int')\r\n",
        "y_train = np.reshape(y_train,(y_train.shape[0],1,y_train.shape[1]))\r\n",
        "y_test = keras.utils.to_categorical(test_y[0],5)\r\n",
        "y_test = y_test.astype('int')\r\n",
        "y_test = np.reshape(y_test,(y_test.shape[0],1,y_test.shape[1]))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sX2VddNf7iyt"
      },
      "source": [
        "Define the network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhuIuz4p00I8"
      },
      "source": [
        "# 1. define the network\r\n",
        "import time\r\n",
        "data_dim = X_train.shape[1]\r\n",
        "timesteps = 1\r\n",
        "num_classes = y_train.shape[1]\r\n",
        "batch_size = 512\r\n",
        "\r\n",
        "\r\n",
        "model = Sequential()\r\n",
        "model.add(SimpleRNN(4, input_dim=X_train.shape[1]))  # try using a GRU instead, for fun\r\n",
        "model.add(Dropout(0.2))\r\n",
        "model.add(Dense(5))\r\n",
        "model.add(Activation('softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bmi9FeVpM7z8"
      },
      "source": [
        "import time\r\n",
        "data_dim = X_train.shape[1]\r\n",
        "timesteps = 1\r\n",
        "num_classes = y_train.shape[1]\r\n",
        "batch_size = 1\r\n",
        "\r\n",
        "model = Sequential()\r\n",
        "model.add(LSTM(50, return_sequences=True, stateful=False, batch_input_shape=(batch_size, timesteps, data_dim)))\r\n",
        "model.add(Dropout(0.1))\r\n",
        "model.add(LSTM(50, return_sequences=True, stateful=False))\r\n",
        "model.add(Dropout(0.1))\r\n",
        "model.add(LSTM(50, stateful=True))\r\n",
        "model.add(Dropout(0.1))\r\n",
        "model.add(Dense(15, activation='softmax'))\r\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "\r\n",
        "\r\n",
        "start = time.time()\r\n",
        "\r\n",
        "start_time = time.time()\r\n",
        "model.fit(X_train, y_train,batch_size=batch_size, epochs=2, shuffle=False)\r\n",
        "finish_time = time.time() - start_time\r\n",
        "model.save(\"/content/drive/My Drive/final_model_NSL-KDD_brute_Multiclass.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2X1EbkTFiow",
        "outputId": "a2303c51-b7bf-4890-8a25-063bea879b2e"
      },
      "source": [
        "import time\r\n",
        "#train_object_num=len(train_data)\r\n",
        "#print(test[0])\r\n",
        "#model training\r\n",
        "batch_size = 1024\r\n",
        "model=Sequential()\r\n",
        "model.add(LSTM(X_train.shape[1],input_dim=X_train.shape[2],return_sequences=True,kernel_initializer='uniform',activation='relu'))\r\n",
        "model.add(LSTM(32,kernel_initializer='uniform',return_sequences=True,activation='relu'))\r\n",
        "#model.add(LSTM(128,kernel_initializer='uniform',activation='relu'))\r\n",
        "model.add(Dense(256,activation='relu',r))\r\n",
        "model.add(Dense(128,activation='relu'))\r\n",
        "model.add(Dense(64,activation='relu'))\r\n",
        "model.add(Dense(16,activation='relu'))\r\n",
        "model.add(Dense(y_train.shape[2],activation='sigmoid'))\r\n",
        "model.compile(loss='binary_crossentropy',optimizer='NADAM',metrics=['accuracy'])\r\n",
        "model.summary()\r\n",
        "start_time = time.time()\r\n",
        "history = model.fit(X_train,y_train,validation_split=.3,epochs=100,batch_size=batch_size,verbose=1)\r\n",
        "finish_time = time.time() - start_time\r\n",
        "model.save(\"/content/drive/My Drive/model_NSL-KDD_42col_Multiclass_valsplit.3_bs1024_100ep.h5\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_12 (LSTM)               (None, None, 1)           72        \n",
            "_________________________________________________________________\n",
            "lstm_13 (LSTM)               (None, None, 32)          4352      \n",
            "_________________________________________________________________\n",
            "dense_30 (Dense)             (None, None, 256)         8448      \n",
            "_________________________________________________________________\n",
            "dense_31 (Dense)             (None, None, 128)         32896     \n",
            "_________________________________________________________________\n",
            "dense_32 (Dense)             (None, None, 64)          8256      \n",
            "_________________________________________________________________\n",
            "dense_33 (Dense)             (None, None, 16)          1040      \n",
            "_________________________________________________________________\n",
            "dense_34 (Dense)             (None, None, 5)           85        \n",
            "=================================================================\n",
            "Total params: 55,149\n",
            "Trainable params: 55,149\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "87/87 [==============================] - 6s 32ms/step - loss: 0.5757 - accuracy: 0.3883 - val_loss: 0.3503 - val_accuracy: 0.5351\n",
            "Epoch 2/100\n",
            "87/87 [==============================] - 2s 19ms/step - loss: 0.3216 - accuracy: 0.6109 - val_loss: 0.1609 - val_accuracy: 0.8643\n",
            "Epoch 3/100\n",
            "87/87 [==============================] - 2s 23ms/step - loss: 0.1597 - accuracy: 0.8662 - val_loss: 0.1546 - val_accuracy: 0.8655\n",
            "Epoch 4/100\n",
            "87/87 [==============================] - 2s 20ms/step - loss: 0.1527 - accuracy: 0.8666 - val_loss: 0.1331 - val_accuracy: 0.8632\n",
            "Epoch 5/100\n",
            "87/87 [==============================] - 2s 19ms/step - loss: 0.1209 - accuracy: 0.8653 - val_loss: 0.1028 - val_accuracy: 0.8612\n",
            "Epoch 6/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0956 - accuracy: 0.8794 - val_loss: 0.0994 - val_accuracy: 0.8822\n",
            "Epoch 7/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0902 - accuracy: 0.8976 - val_loss: 0.0880 - val_accuracy: 0.9125\n",
            "Epoch 8/100\n",
            "87/87 [==============================] - 2s 22ms/step - loss: 0.0842 - accuracy: 0.9092 - val_loss: 0.0824 - val_accuracy: 0.9141\n",
            "Epoch 9/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0824 - accuracy: 0.9132 - val_loss: 0.0819 - val_accuracy: 0.9140\n",
            "Epoch 10/100\n",
            "87/87 [==============================] - 2s 19ms/step - loss: 0.0813 - accuracy: 0.9129 - val_loss: 0.0802 - val_accuracy: 0.9150\n",
            "Epoch 11/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0789 - accuracy: 0.9151 - val_loss: 0.0860 - val_accuracy: 0.8664\n",
            "Epoch 12/100\n",
            "87/87 [==============================] - 2s 19ms/step - loss: 0.0803 - accuracy: 0.9103 - val_loss: 0.0797 - val_accuracy: 0.9162\n",
            "Epoch 13/100\n",
            "87/87 [==============================] - 2s 20ms/step - loss: 0.0782 - accuracy: 0.9163 - val_loss: 0.0811 - val_accuracy: 0.9117\n",
            "Epoch 14/100\n",
            "87/87 [==============================] - 2s 23ms/step - loss: 0.0777 - accuracy: 0.9151 - val_loss: 0.0785 - val_accuracy: 0.9162\n",
            "Epoch 15/100\n",
            "87/87 [==============================] - 2s 23ms/step - loss: 0.0771 - accuracy: 0.9158 - val_loss: 0.0851 - val_accuracy: 0.9148\n",
            "Epoch 16/100\n",
            "87/87 [==============================] - 2s 20ms/step - loss: 0.0778 - accuracy: 0.9153 - val_loss: 0.0777 - val_accuracy: 0.9154\n",
            "Epoch 17/100\n",
            "87/87 [==============================] - 2s 20ms/step - loss: 0.0766 - accuracy: 0.9161 - val_loss: 0.0768 - val_accuracy: 0.9160\n",
            "Epoch 18/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0773 - accuracy: 0.9157 - val_loss: 0.0760 - val_accuracy: 0.9167\n",
            "Epoch 19/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0768 - accuracy: 0.9146 - val_loss: 0.0756 - val_accuracy: 0.9171\n",
            "Epoch 20/100\n",
            "87/87 [==============================] - 2s 23ms/step - loss: 0.0762 - accuracy: 0.9157 - val_loss: 0.0782 - val_accuracy: 0.9150\n",
            "Epoch 21/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0768 - accuracy: 0.9148 - val_loss: 0.0755 - val_accuracy: 0.9163\n",
            "Epoch 22/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0755 - accuracy: 0.9157 - val_loss: 0.0750 - val_accuracy: 0.9181\n",
            "Epoch 23/100\n",
            "87/87 [==============================] - 2s 22ms/step - loss: 0.0755 - accuracy: 0.9168 - val_loss: 0.0756 - val_accuracy: 0.9155\n",
            "Epoch 24/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0756 - accuracy: 0.9158 - val_loss: 0.0749 - val_accuracy: 0.9163\n",
            "Epoch 25/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0750 - accuracy: 0.9164 - val_loss: 0.0744 - val_accuracy: 0.9166\n",
            "Epoch 26/100\n",
            "87/87 [==============================] - 2s 23ms/step - loss: 0.0749 - accuracy: 0.9165 - val_loss: 0.0752 - val_accuracy: 0.9189\n",
            "Epoch 27/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0735 - accuracy: 0.9188 - val_loss: 0.0751 - val_accuracy: 0.9167\n",
            "Epoch 28/100\n",
            "87/87 [==============================] - 2s 23ms/step - loss: 0.0750 - accuracy: 0.9163 - val_loss: 0.0770 - val_accuracy: 0.9170\n",
            "Epoch 29/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0741 - accuracy: 0.9178 - val_loss: 0.0754 - val_accuracy: 0.9174\n",
            "Epoch 30/100\n",
            "87/87 [==============================] - 2s 20ms/step - loss: 0.0746 - accuracy: 0.9167 - val_loss: 0.0750 - val_accuracy: 0.9172\n",
            "Epoch 31/100\n",
            "87/87 [==============================] - 2s 22ms/step - loss: 0.0738 - accuracy: 0.9164 - val_loss: 0.0749 - val_accuracy: 0.9198\n",
            "Epoch 32/100\n",
            "87/87 [==============================] - 2s 20ms/step - loss: 0.0749 - accuracy: 0.9179 - val_loss: 0.0738 - val_accuracy: 0.9173\n",
            "Epoch 33/100\n",
            "87/87 [==============================] - 2s 20ms/step - loss: 0.0733 - accuracy: 0.9189 - val_loss: 0.0736 - val_accuracy: 0.9174\n",
            "Epoch 34/100\n",
            "87/87 [==============================] - 2s 23ms/step - loss: 0.0758 - accuracy: 0.9154 - val_loss: 0.0771 - val_accuracy: 0.9182\n",
            "Epoch 35/100\n",
            "87/87 [==============================] - 2s 23ms/step - loss: 0.0731 - accuracy: 0.9201 - val_loss: 0.0733 - val_accuracy: 0.9182\n",
            "Epoch 36/100\n",
            "87/87 [==============================] - 2s 22ms/step - loss: 0.0732 - accuracy: 0.9176 - val_loss: 0.0736 - val_accuracy: 0.9169\n",
            "Epoch 37/100\n",
            "87/87 [==============================] - 2s 22ms/step - loss: 0.0734 - accuracy: 0.9180 - val_loss: 0.0751 - val_accuracy: 0.9181\n",
            "Epoch 38/100\n",
            "87/87 [==============================] - 2s 23ms/step - loss: 0.0738 - accuracy: 0.9187 - val_loss: 0.0768 - val_accuracy: 0.9151\n",
            "Epoch 39/100\n",
            "87/87 [==============================] - 2s 23ms/step - loss: 0.0731 - accuracy: 0.9181 - val_loss: 0.0751 - val_accuracy: 0.9178\n",
            "Epoch 40/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0726 - accuracy: 0.9187 - val_loss: 0.0735 - val_accuracy: 0.9182\n",
            "Epoch 41/100\n",
            "87/87 [==============================] - 2s 19ms/step - loss: 0.0725 - accuracy: 0.9194 - val_loss: 0.0735 - val_accuracy: 0.9185\n",
            "Epoch 42/100\n",
            "87/87 [==============================] - 2s 20ms/step - loss: 0.0736 - accuracy: 0.9172 - val_loss: 0.0760 - val_accuracy: 0.9184\n",
            "Epoch 43/100\n",
            "87/87 [==============================] - 2s 19ms/step - loss: 0.0726 - accuracy: 0.9190 - val_loss: 0.0732 - val_accuracy: 0.9187\n",
            "Epoch 44/100\n",
            "87/87 [==============================] - 2s 22ms/step - loss: 0.0722 - accuracy: 0.9197 - val_loss: 0.0736 - val_accuracy: 0.9182\n",
            "Epoch 45/100\n",
            "87/87 [==============================] - 2s 20ms/step - loss: 0.0726 - accuracy: 0.9195 - val_loss: 0.0761 - val_accuracy: 0.9189\n",
            "Epoch 46/100\n",
            "87/87 [==============================] - 2s 20ms/step - loss: 0.0730 - accuracy: 0.9181 - val_loss: 0.0763 - val_accuracy: 0.9197\n",
            "Epoch 47/100\n",
            "87/87 [==============================] - 2s 20ms/step - loss: 0.0725 - accuracy: 0.9203 - val_loss: 0.0761 - val_accuracy: 0.9164\n",
            "Epoch 48/100\n",
            "87/87 [==============================] - 2s 22ms/step - loss: 0.0732 - accuracy: 0.9185 - val_loss: 0.0727 - val_accuracy: 0.9188\n",
            "Epoch 49/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0733 - accuracy: 0.9187 - val_loss: 0.0733 - val_accuracy: 0.9182\n",
            "Epoch 50/100\n",
            "87/87 [==============================] - 2s 20ms/step - loss: 0.0727 - accuracy: 0.9197 - val_loss: 0.0750 - val_accuracy: 0.9189\n",
            "Epoch 51/100\n",
            "87/87 [==============================] - 2s 20ms/step - loss: 0.0723 - accuracy: 0.9194 - val_loss: 0.0734 - val_accuracy: 0.9187\n",
            "Epoch 52/100\n",
            "87/87 [==============================] - 2s 20ms/step - loss: 0.0732 - accuracy: 0.9187 - val_loss: 0.0735 - val_accuracy: 0.9195\n",
            "Epoch 53/100\n",
            "87/87 [==============================] - 2s 20ms/step - loss: 0.0733 - accuracy: 0.9179 - val_loss: 0.0757 - val_accuracy: 0.9160\n",
            "Epoch 54/100\n",
            "87/87 [==============================] - 2s 19ms/step - loss: 0.0728 - accuracy: 0.9183 - val_loss: 0.0731 - val_accuracy: 0.9194\n",
            "Epoch 55/100\n",
            "87/87 [==============================] - 2s 20ms/step - loss: 0.0721 - accuracy: 0.9192 - val_loss: 0.0725 - val_accuracy: 0.9191\n",
            "Epoch 56/100\n",
            "87/87 [==============================] - 2s 20ms/step - loss: 0.0720 - accuracy: 0.9193 - val_loss: 0.0723 - val_accuracy: 0.9193\n",
            "Epoch 57/100\n",
            "87/87 [==============================] - 2s 24ms/step - loss: 0.0738 - accuracy: 0.9183 - val_loss: 0.0731 - val_accuracy: 0.9191\n",
            "Epoch 58/100\n",
            "87/87 [==============================] - 2s 23ms/step - loss: 0.0727 - accuracy: 0.9199 - val_loss: 0.0727 - val_accuracy: 0.9198\n",
            "Epoch 59/100\n",
            "87/87 [==============================] - 2s 19ms/step - loss: 0.0732 - accuracy: 0.9183 - val_loss: 0.0727 - val_accuracy: 0.9191\n",
            "Epoch 60/100\n",
            "87/87 [==============================] - 2s 19ms/step - loss: 0.0726 - accuracy: 0.9188 - val_loss: 0.0723 - val_accuracy: 0.9191\n",
            "Epoch 61/100\n",
            "87/87 [==============================] - 2s 20ms/step - loss: 0.0723 - accuracy: 0.9193 - val_loss: 0.0727 - val_accuracy: 0.9189\n",
            "Epoch 62/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0730 - accuracy: 0.9179 - val_loss: 0.0723 - val_accuracy: 0.9198\n",
            "Epoch 63/100\n",
            "87/87 [==============================] - 2s 23ms/step - loss: 0.0725 - accuracy: 0.9191 - val_loss: 0.0723 - val_accuracy: 0.9188\n",
            "Epoch 64/100\n",
            "87/87 [==============================] - 2s 23ms/step - loss: 0.0719 - accuracy: 0.9190 - val_loss: 0.0719 - val_accuracy: 0.9202\n",
            "Epoch 65/100\n",
            "87/87 [==============================] - 2s 22ms/step - loss: 0.0720 - accuracy: 0.9195 - val_loss: 0.0722 - val_accuracy: 0.9203\n",
            "Epoch 66/100\n",
            "87/87 [==============================] - 2s 20ms/step - loss: 0.0725 - accuracy: 0.9197 - val_loss: 0.0720 - val_accuracy: 0.9203\n",
            "Epoch 67/100\n",
            "87/87 [==============================] - 2s 20ms/step - loss: 0.0710 - accuracy: 0.9210 - val_loss: 0.0718 - val_accuracy: 0.9202\n",
            "Epoch 68/100\n",
            "87/87 [==============================] - 2s 23ms/step - loss: 0.0724 - accuracy: 0.9190 - val_loss: 0.0723 - val_accuracy: 0.9197\n",
            "Epoch 69/100\n",
            "87/87 [==============================] - 2s 23ms/step - loss: 0.0721 - accuracy: 0.9189 - val_loss: 0.0718 - val_accuracy: 0.9205\n",
            "Epoch 70/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0716 - accuracy: 0.9207 - val_loss: 0.0720 - val_accuracy: 0.9206\n",
            "Epoch 71/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0715 - accuracy: 0.9204 - val_loss: 0.0724 - val_accuracy: 0.9204\n",
            "Epoch 72/100\n",
            "87/87 [==============================] - 2s 22ms/step - loss: 0.0717 - accuracy: 0.9195 - val_loss: 0.0723 - val_accuracy: 0.9205\n",
            "Epoch 73/100\n",
            "87/87 [==============================] - 2s 22ms/step - loss: 0.0723 - accuracy: 0.9194 - val_loss: 0.0725 - val_accuracy: 0.9204\n",
            "Epoch 74/100\n",
            "87/87 [==============================] - 2s 23ms/step - loss: 0.0720 - accuracy: 0.9207 - val_loss: 0.0768 - val_accuracy: 0.8694\n",
            "Epoch 75/100\n",
            "87/87 [==============================] - 2s 24ms/step - loss: 0.0733 - accuracy: 0.9147 - val_loss: 0.0719 - val_accuracy: 0.9207\n",
            "Epoch 76/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0713 - accuracy: 0.9201 - val_loss: 0.0717 - val_accuracy: 0.9202\n",
            "Epoch 77/100\n",
            "87/87 [==============================] - 2s 19ms/step - loss: 0.0710 - accuracy: 0.9214 - val_loss: 0.0719 - val_accuracy: 0.9203\n",
            "Epoch 78/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0712 - accuracy: 0.9200 - val_loss: 0.0728 - val_accuracy: 0.9204\n",
            "Epoch 79/100\n",
            "87/87 [==============================] - 2s 22ms/step - loss: 0.0714 - accuracy: 0.9205 - val_loss: 0.0730 - val_accuracy: 0.9207\n",
            "Epoch 80/100\n",
            "87/87 [==============================] - 2s 20ms/step - loss: 0.0720 - accuracy: 0.9197 - val_loss: 0.0716 - val_accuracy: 0.9205\n",
            "Epoch 81/100\n",
            "87/87 [==============================] - 2s 19ms/step - loss: 0.0720 - accuracy: 0.9193 - val_loss: 0.0729 - val_accuracy: 0.9199\n",
            "Epoch 82/100\n",
            "87/87 [==============================] - 2s 20ms/step - loss: 0.0712 - accuracy: 0.9211 - val_loss: 0.0720 - val_accuracy: 0.9200\n",
            "Epoch 83/100\n",
            "87/87 [==============================] - 2s 20ms/step - loss: 0.0716 - accuracy: 0.9204 - val_loss: 0.0717 - val_accuracy: 0.9204\n",
            "Epoch 84/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0711 - accuracy: 0.9206 - val_loss: 0.0718 - val_accuracy: 0.9208\n",
            "Epoch 85/100\n",
            "87/87 [==============================] - 2s 20ms/step - loss: 0.0705 - accuracy: 0.9209 - val_loss: 0.0788 - val_accuracy: 0.9210\n",
            "Epoch 86/100\n",
            "87/87 [==============================] - 2s 20ms/step - loss: 0.0724 - accuracy: 0.9209 - val_loss: 0.0723 - val_accuracy: 0.9208\n",
            "Epoch 87/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0712 - accuracy: 0.9196 - val_loss: 0.0718 - val_accuracy: 0.9210\n",
            "Epoch 88/100\n",
            "87/87 [==============================] - 2s 22ms/step - loss: 0.0713 - accuracy: 0.9209 - val_loss: 0.0716 - val_accuracy: 0.9204\n",
            "Epoch 89/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0719 - accuracy: 0.9197 - val_loss: 0.0712 - val_accuracy: 0.9213\n",
            "Epoch 90/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0727 - accuracy: 0.9181 - val_loss: 0.0714 - val_accuracy: 0.9209\n",
            "Epoch 91/100\n",
            "87/87 [==============================] - 2s 22ms/step - loss: 0.0702 - accuracy: 0.9213 - val_loss: 0.0714 - val_accuracy: 0.9211\n",
            "Epoch 92/100\n",
            "87/87 [==============================] - 2s 22ms/step - loss: 0.0707 - accuracy: 0.9202 - val_loss: 0.0738 - val_accuracy: 0.9174\n",
            "Epoch 93/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0708 - accuracy: 0.9198 - val_loss: 0.0722 - val_accuracy: 0.9213\n",
            "Epoch 94/100\n",
            "87/87 [==============================] - 2s 20ms/step - loss: 0.0711 - accuracy: 0.9203 - val_loss: 0.0711 - val_accuracy: 0.9208\n",
            "Epoch 95/100\n",
            "87/87 [==============================] - 2s 20ms/step - loss: 0.0716 - accuracy: 0.9195 - val_loss: 0.0718 - val_accuracy: 0.9213\n",
            "Epoch 96/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0711 - accuracy: 0.9214 - val_loss: 0.0714 - val_accuracy: 0.9209\n",
            "Epoch 97/100\n",
            "87/87 [==============================] - 2s 19ms/step - loss: 0.0724 - accuracy: 0.9200 - val_loss: 0.0713 - val_accuracy: 0.9207\n",
            "Epoch 98/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0709 - accuracy: 0.9221 - val_loss: 0.0712 - val_accuracy: 0.9213\n",
            "Epoch 99/100\n",
            "87/87 [==============================] - 2s 22ms/step - loss: 0.0703 - accuracy: 0.9218 - val_loss: 0.0715 - val_accuracy: 0.9209\n",
            "Epoch 100/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0713 - accuracy: 0.9203 - val_loss: 0.0713 - val_accuracy: 0.9214\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgFh6eY-RogJ",
        "outputId": "6b00474b-d40d-4194-dc70-e71eb8a0075a"
      },
      "source": [
        "from sklearn.metrics import classification_report\r\n",
        "from sklearn.metrics import precision_recall_fscore_support\r\n",
        "loss, accuracy = model.evaluate(X_test,y_test)\r\n",
        "print(\"\\nLoss: %.2f, Accuracy: %.2f%%\" % (loss, accuracy*100))\r\n",
        "y_pred = model.predict_classes(X_test)\r\n",
        "target_names = ['Normal', 'Dos', 'Probe','R2L', 'U2R']\r\n",
        "print(classification_report(y_true = np.transpose(test_y[0]), y_pred = y_pred))\r\n",
        "#print(accuracy_score(y_true = np.transpose(test_y[0]), y_pred = y_pred))\r\n",
        "print(\"execution time :  \",finish_time)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "705/705 [==============================] - 1s 1ms/step - loss: 0.8132 - accuracy: 0.4852\n",
            "\n",
            "Loss: 0.81, Accuracy: 48.52%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.46      0.21      0.29      7458\n",
            "           1       0.55      0.87      0.68      9710\n",
            "           2       0.00      0.00      0.00      2421\n",
            "           3       0.24      0.34      0.29      2754\n",
            "           4       0.00      0.00      0.00       200\n",
            "\n",
            "    accuracy                           0.49     22543\n",
            "   macro avg       0.25      0.28      0.25     22543\n",
            "weighted avg       0.42      0.49      0.42     22543\n",
            "\n",
            "execution time :   187.6243758201599\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBLLxMkV25MY",
        "outputId": "1cd20db3-6f7d-45b6-e3d5-521895213215"
      },
      "source": [
        "model.metrics_names"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['loss', 'accuracy']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqrg8ftGdlZd",
        "outputId": "aace09a7-f2e0-47bb-adba-465e6711a6c6"
      },
      "source": [
        "test_y.sh"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 1, ..., 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPW7sxx-yiwT",
        "outputId": "72de27e0-0eaa-46a9-ccfa-5e71a66065a6"
      },
      "source": [
        "s= np.transpose(test_y[0])\r\n",
        "s.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(22543,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c98fMqo4yf5b",
        "outputId": "82a77c16-52b0-4d62-b7c4-3bc1116832bb"
      },
      "source": [
        "y_pred.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(22543, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vn7ng4MLMOb"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix,  roc_auc_score, roc_curve\r\n",
        "import sklearn.metrics as metrics\r\n",
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\r\n",
        "score = model.evaluate(X_test, y_test, batch_size)\r\n",
        "y_pred = model.predict(X_test, batch_size= batch_size)\r\n",
        "y_pred = y_pred.astype('int')\r\n",
        "#y_pred = [np.round(x) for x in y_pred]\r\n",
        "#y_pred = np.array(y_pred)\r\n",
        "y2 = np.array(test_y[0])\r\n",
        "\r\n",
        "#print(y2)\r\n",
        "print(classification_report(y_test,y_pred,\"multiclass\"))\r\n",
        "cm = confusion_matrix(y2.argmax(axis=1), y_pred.argmax(axis=1))\r\n",
        "print(cm)\r\n",
        "\r\n",
        "\r\n",
        "probs = model.predict_proba(X_test,batch_size=batch_size)\r\n",
        "#y_preds = np.transpose([pred[:, 1] for pred in probs])\r\n",
        "#y_preds=y_preds.reshape(1,-1)\r\n",
        "fpr, tpr, threshold = metrics.roc_auc_score(y_score==np.transpose(y_test),y_true= np.transpose(probs),multi_class='ovo')\r\n",
        "roc_auc = metrics.auc(fpr, tpr)\r\n",
        "\r\n",
        "# method I: plt\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "plt.title('Receiver Operating Characteristic')\r\n",
        "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\r\n",
        "plt.legend(loc = 'lower right')\r\n",
        "plt.plot([0, 1], [0, 1],'r--')\r\n",
        "plt.xlim([0, 1])\r\n",
        "plt.ylim([0, 1])\r\n",
        "plt.ylabel('True Positive Rate')\r\n",
        "plt.xlabel('False Positive Rate')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5ojJxnFa2aF",
        "outputId": "28466071-306a-4e2b-cba4-bbd039f3f64f"
      },
      "source": [
        "y2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 1, ..., 0, 1, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bq6QuCZu2BgD"
      },
      "source": [
        "# try using different optimizers and different optimizer configs\r\n",
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\r\n",
        "#checkpointer = callbacks.ModelCheckpoint(filepath=\"kddresults/lstm1layer/checkpoint-{epoch:02d}.hdf5\", verbose=1, save_best_only=True, monitor='val_acc',mode='max')\r\n",
        "#csv_logger = CSVLogger('training_set.csv',separator=',', append=False)\r\n",
        "start_time = time.time()\r\n",
        "model.fit(X_train, y_train, batch_size=batch_size, epochs=2, validation_split=.2)#,callbacks=[checkpointer,csv_logger]\r\n",
        "finish_time = time.time() - start_time\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieGpV2zYXZwV"
      },
      "source": [
        "#saving model\r\n",
        "model.save(\"/content/drive/MyDrive/NSL-KDD_results_3LSTM_1Dense_20epoch_1batch_model.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xLvXBh0SyWh",
        "outputId": "5f8c4d80-d3a3-4c10-c1d7-aaf4d59dddb8"
      },
      "source": [
        "from keras.models import load_model\r\n",
        "#model = load_model('/content/drive/MyDrive/NSL-KDD_results_SimpleRNN_model.hdf5')\r\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\r\n",
        "print(\"\\nLoss: %.2f, Accuracy: %.2f%%\" % (loss, accuracy*100))\r\n",
        "\r\n",
        "#print(\"--- %s seconds ---\",finish_time)\r\n",
        "y_pred = model.predict_classes(X_train)\r\n",
        "#np.savetxt('/content/drive/MyDrive/NSL-KDD_results_SimpleRNN_predicted.txt', np.transpose([y_test,y_pred]), fmt='%s')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "705/705 [==============================] - 4s 5ms/step - loss: 4.9965 - accuracy: 0.0768\n",
            "\n",
            "Loss: 5.00, Accuracy: 7.68%\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 41) for input KerasTensor(type_spec=TensorSpec(shape=(None, 41), dtype=tf.float32, name='dense_35_input'), name='dense_35_input', description=\"created by layer 'dense_35_input'\"), but it was called on an input with incompatible shape (None, 1, 41).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRdRVMFrQSrR",
        "outputId": "d2a8581b-03d8-4ceb-f4ef-b734c10842eb"
      },
      "source": [
        "print(y_train)\r\n",
        "y_pred"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[1 0 0 0 0]]\n",
            "\n",
            " [[1 0 0 0 0]]\n",
            "\n",
            " [[0 1 0 0 0]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[1 0 0 0 0]]\n",
            "\n",
            " [[0 1 0 0 0]]\n",
            "\n",
            " [[1 0 0 0 0]]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0],\n",
              "       [0],\n",
              "       [1],\n",
              "       ...,\n",
              "       [0],\n",
              "       [1],\n",
              "       [0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqAFEhU4_CMH",
        "outputId": "dc7ca7e5-50c4-4595-ba53-6ba7104f0767"
      },
      "source": [
        "42 col pca 80% : b_s=256 epochs= 300 Lstm(32,32)->Dense(256,128,64,ytrain.shape(2))   ->   accuracy = 62.8%\r\n",
        "\r\n",
        "\r\n",
        "24 col normalisé : b_s=1024 epochs= 100 Lstm(32,32)->Dense(256,128,64,16,ytrain.shape(2))   ->   accuracy = 83.73%\r\n",
        "24 col pca 80% : b_s=1024 epochs= 100 Lstm(32,32)->Dense(256,128,64,16,ytrain.shape(2))   ->   accuracy = 40.28%\r\n",
        "42 col normalisé : b_s=1024 epochs= 100 Lstm(32,32)->Dense(256,128,64,16,ytrain.shape(2))   ->   accuracy = 48.52%\r\n",
        "42 col pca 80% : b_s=1024 epochs= 100 Lstm(32,32)->Dense(256,128,64,16,ytrain.shape(2))   ->   accuracy = 45.22%\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<tensorflow.python.keras.engine.sequential.Sequential object at 0x7f324f860ba8>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZpW4B3GT5_ac",
        "outputId": "8ea20939-1794-47fd-a1b3-554d7bf940e1"
      },
      "source": [
        "batch_size= 1024\r\n",
        "lrate = 0.1\r\n",
        "model = Sequential()\r\n",
        "model.add(Dense(X_train.shape[2], input_dim=X_train.shape[2], activation='relu', kernel_initializer='he_uniform'))\r\n",
        "model.add(Dense(1024, input_dim=X_train.shape[2], activation='relu', kernel_initializer='he_uniform'))\r\n",
        "model.add(Dropout(0.1))\r\n",
        "model.add(Dense(768, input_dim=X_train.shape[2], activation='relu', kernel_initializer='he_uniform'))\r\n",
        "model.add(Dropout(0.1))\r\n",
        "model.add(Dense(512, input_dim=X_train.shape[2], activation='relu', kernel_initializer='he_uniform'))\r\n",
        "model.add(Dropout(0.1))\r\n",
        "model.add(Dense(256, input_dim=X_train.shape[2], activation='relu', kernel_initializer='he_uniform'))\r\n",
        "model.add(Dropout(0.1))\r\n",
        "model.add(Dense(128, input_dim=X_train.shape[2], activation='relu', kernel_initializer='he_uniform'))\r\n",
        "model.add(Dropout(0.1))\r\n",
        "model.add(Dense(y_train.shape[2], activation='softmax'))\r\n",
        "\t# compile model\r\n",
        "opt = SGD(lr=lrate)\r\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\r\n",
        "# fit model\r\n",
        "history = model.fit(X_train, y_train, validation_split=.3,batch_size=batch_size, epochs=200, verbose=1)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 41) for input KerasTensor(type_spec=TensorSpec(shape=(None, 41), dtype=tf.float32, name='dense_28_input'), name='dense_28_input', description=\"created by layer 'dense_28_input'\"), but it was called on an input with incompatible shape (None, 1, 41).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 41) for input KerasTensor(type_spec=TensorSpec(shape=(None, 41), dtype=tf.float32, name='dense_28_input'), name='dense_28_input', description=\"created by layer 'dense_28_input'\"), but it was called on an input with incompatible shape (None, 1, 41).\n",
            "86/87 [============================>.] - ETA: 0s - loss: 0.7081 - accuracy: 0.8310WARNING:tensorflow:Model was constructed with shape (None, 41) for input KerasTensor(type_spec=TensorSpec(shape=(None, 41), dtype=tf.float32, name='dense_28_input'), name='dense_28_input', description=\"created by layer 'dense_28_input'\"), but it was called on an input with incompatible shape (None, 1, 41).\n",
            "87/87 [==============================] - 18s 196ms/step - loss: 0.6985 - accuracy: 0.8332 - val_loss: 0.0509 - val_accuracy: 0.9852\n",
            "Epoch 2/200\n",
            "87/87 [==============================] - 17s 197ms/step - loss: 0.0984 - accuracy: 0.9751 - val_loss: 0.0489 - val_accuracy: 0.9868\n",
            "Epoch 3/200\n",
            "87/87 [==============================] - 17s 194ms/step - loss: 0.0495 - accuracy: 0.9862 - val_loss: 0.0303 - val_accuracy: 0.9922\n",
            "Epoch 4/200\n",
            "87/87 [==============================] - 17s 198ms/step - loss: 0.0363 - accuracy: 0.9894 - val_loss: 0.0233 - val_accuracy: 0.9934\n",
            "Epoch 5/200\n",
            "87/87 [==============================] - 17s 194ms/step - loss: 0.0300 - accuracy: 0.9906 - val_loss: 0.0219 - val_accuracy: 0.9932\n",
            "Epoch 6/200\n",
            "87/87 [==============================] - 17s 192ms/step - loss: 0.0250 - accuracy: 0.9918 - val_loss: 0.0179 - val_accuracy: 0.9946\n",
            "Epoch 7/200\n",
            "87/87 [==============================] - 17s 196ms/step - loss: 0.0236 - accuracy: 0.9929 - val_loss: 0.0169 - val_accuracy: 0.9952\n",
            "Epoch 8/200\n",
            "87/87 [==============================] - 17s 196ms/step - loss: 0.0201 - accuracy: 0.9937 - val_loss: 0.0164 - val_accuracy: 0.9953\n",
            "Epoch 9/200\n",
            "87/87 [==============================] - 17s 198ms/step - loss: 0.0235 - accuracy: 0.9936 - val_loss: 0.0154 - val_accuracy: 0.9956\n",
            "Epoch 10/200\n",
            "87/87 [==============================] - 17s 198ms/step - loss: 0.0172 - accuracy: 0.9945 - val_loss: 0.0138 - val_accuracy: 0.9961\n",
            "Epoch 11/200\n",
            "87/87 [==============================] - 17s 192ms/step - loss: 0.0174 - accuracy: 0.9941 - val_loss: 0.0126 - val_accuracy: 0.9963\n",
            "Epoch 12/200\n",
            "87/87 [==============================] - 17s 197ms/step - loss: 0.0151 - accuracy: 0.9948 - val_loss: 0.0123 - val_accuracy: 0.9968\n",
            "Epoch 13/200\n",
            "87/87 [==============================] - 17s 194ms/step - loss: 0.0155 - accuracy: 0.9947 - val_loss: 0.0112 - val_accuracy: 0.9966\n",
            "Epoch 14/200\n",
            "87/87 [==============================] - 17s 194ms/step - loss: 0.0131 - accuracy: 0.9957 - val_loss: 0.0112 - val_accuracy: 0.9961\n",
            "Epoch 15/200\n",
            "87/87 [==============================] - 17s 195ms/step - loss: 0.0134 - accuracy: 0.9954 - val_loss: 0.0114 - val_accuracy: 0.9965\n",
            "Epoch 16/200\n",
            "87/87 [==============================] - 17s 195ms/step - loss: 0.0119 - accuracy: 0.9960 - val_loss: 0.0116 - val_accuracy: 0.9970\n",
            "Epoch 17/200\n",
            "87/87 [==============================] - 17s 196ms/step - loss: 0.0112 - accuracy: 0.9957 - val_loss: 0.0113 - val_accuracy: 0.9969\n",
            "Epoch 18/200\n",
            "87/87 [==============================] - 17s 194ms/step - loss: 0.0115 - accuracy: 0.9962 - val_loss: 0.0111 - val_accuracy: 0.9974\n",
            "Epoch 19/200\n",
            "87/87 [==============================] - 17s 196ms/step - loss: 0.0115 - accuracy: 0.9963 - val_loss: 0.0114 - val_accuracy: 0.9973\n",
            "Epoch 20/200\n",
            "87/87 [==============================] - 17s 193ms/step - loss: 0.0103 - accuracy: 0.9968 - val_loss: 0.0114 - val_accuracy: 0.9975\n",
            "Epoch 21/200\n",
            "87/87 [==============================] - 17s 196ms/step - loss: 0.0105 - accuracy: 0.9965 - val_loss: 0.0156 - val_accuracy: 0.9960\n",
            "Epoch 22/200\n",
            "87/87 [==============================] - 17s 197ms/step - loss: 0.0136 - accuracy: 0.9960 - val_loss: 0.0118 - val_accuracy: 0.9975\n",
            "Epoch 23/200\n",
            "87/87 [==============================] - 17s 194ms/step - loss: 0.0099 - accuracy: 0.9965 - val_loss: 0.0121 - val_accuracy: 0.9972\n",
            "Epoch 24/200\n",
            "87/87 [==============================] - 17s 196ms/step - loss: 0.0115 - accuracy: 0.9963 - val_loss: 0.0120 - val_accuracy: 0.9974\n",
            "Epoch 25/200\n",
            "87/87 [==============================] - 17s 196ms/step - loss: 0.0087 - accuracy: 0.9969 - val_loss: 0.0116 - val_accuracy: 0.9973\n",
            "Epoch 26/200\n",
            "87/87 [==============================] - 17s 198ms/step - loss: 0.0091 - accuracy: 0.9967 - val_loss: 0.0120 - val_accuracy: 0.9971\n",
            "Epoch 27/200\n",
            "87/87 [==============================] - 17s 196ms/step - loss: 0.0088 - accuracy: 0.9972 - val_loss: 0.0120 - val_accuracy: 0.9971\n",
            "Epoch 28/200\n",
            "87/87 [==============================] - 17s 197ms/step - loss: 0.0103 - accuracy: 0.9965 - val_loss: 0.0114 - val_accuracy: 0.9973\n",
            "Epoch 29/200\n",
            "87/87 [==============================] - 17s 196ms/step - loss: 0.0092 - accuracy: 0.9970 - val_loss: 0.0106 - val_accuracy: 0.9977\n",
            "Epoch 30/200\n",
            "87/87 [==============================] - 17s 196ms/step - loss: 0.0075 - accuracy: 0.9977 - val_loss: 0.0104 - val_accuracy: 0.9976\n",
            "Epoch 31/200\n",
            "87/87 [==============================] - 17s 196ms/step - loss: 0.0081 - accuracy: 0.9974 - val_loss: 0.0106 - val_accuracy: 0.9975\n",
            "Epoch 32/200\n",
            "87/87 [==============================] - 17s 196ms/step - loss: 0.0071 - accuracy: 0.9975 - val_loss: 0.0107 - val_accuracy: 0.9974\n",
            "Epoch 33/200\n",
            "87/87 [==============================] - 17s 193ms/step - loss: 0.0072 - accuracy: 0.9977 - val_loss: 0.0103 - val_accuracy: 0.9976\n",
            "Epoch 34/200\n",
            "87/87 [==============================] - 17s 192ms/step - loss: 0.0076 - accuracy: 0.9971 - val_loss: 0.0113 - val_accuracy: 0.9975\n",
            "Epoch 35/200\n",
            "87/87 [==============================] - 17s 195ms/step - loss: 0.0071 - accuracy: 0.9977 - val_loss: 0.0112 - val_accuracy: 0.9976\n",
            "Epoch 36/200\n",
            "87/87 [==============================] - 17s 197ms/step - loss: 0.0074 - accuracy: 0.9975 - val_loss: 0.0118 - val_accuracy: 0.9972\n",
            "Epoch 37/200\n",
            "87/87 [==============================] - 17s 198ms/step - loss: 0.0080 - accuracy: 0.9976 - val_loss: 0.0118 - val_accuracy: 0.9972\n",
            "Epoch 38/200\n",
            "87/87 [==============================] - 17s 195ms/step - loss: 0.0076 - accuracy: 0.9975 - val_loss: 0.0121 - val_accuracy: 0.9971\n",
            "Epoch 39/200\n",
            "87/87 [==============================] - 17s 192ms/step - loss: 0.0077 - accuracy: 0.9975 - val_loss: 0.0111 - val_accuracy: 0.9971\n",
            "Epoch 40/200\n",
            "87/87 [==============================] - 17s 195ms/step - loss: 0.0081 - accuracy: 0.9973 - val_loss: 0.0106 - val_accuracy: 0.9972\n",
            "Epoch 41/200\n",
            "87/87 [==============================] - 17s 197ms/step - loss: 0.0079 - accuracy: 0.9971 - val_loss: 0.0102 - val_accuracy: 0.9975\n",
            "Epoch 42/200\n",
            "87/87 [==============================] - 17s 195ms/step - loss: 0.0068 - accuracy: 0.9980 - val_loss: 0.0099 - val_accuracy: 0.9977\n",
            "Epoch 43/200\n",
            "87/87 [==============================] - 17s 197ms/step - loss: 0.0072 - accuracy: 0.9978 - val_loss: 0.0108 - val_accuracy: 0.9971\n",
            "Epoch 44/200\n",
            "87/87 [==============================] - 17s 196ms/step - loss: 0.0057 - accuracy: 0.9981 - val_loss: 0.0111 - val_accuracy: 0.9973\n",
            "Epoch 45/200\n",
            "87/87 [==============================] - 17s 194ms/step - loss: 0.0072 - accuracy: 0.9976 - val_loss: 0.0103 - val_accuracy: 0.9978\n",
            "Epoch 46/200\n",
            "87/87 [==============================] - 17s 196ms/step - loss: 0.0058 - accuracy: 0.9979 - val_loss: 0.0111 - val_accuracy: 0.9978\n",
            "Epoch 47/200\n",
            "87/87 [==============================] - 17s 191ms/step - loss: 0.0072 - accuracy: 0.9978 - val_loss: 0.0120 - val_accuracy: 0.9977\n",
            "Epoch 48/200\n",
            "87/87 [==============================] - 17s 196ms/step - loss: 0.0068 - accuracy: 0.9977 - val_loss: 0.0112 - val_accuracy: 0.9979\n",
            "Epoch 49/200\n",
            "87/87 [==============================] - 17s 194ms/step - loss: 0.0055 - accuracy: 0.9983 - val_loss: 0.0106 - val_accuracy: 0.9978\n",
            "Epoch 50/200\n",
            "87/87 [==============================] - 17s 194ms/step - loss: 0.0063 - accuracy: 0.9980 - val_loss: 0.0107 - val_accuracy: 0.9978\n",
            "Epoch 51/200\n",
            "87/87 [==============================] - 17s 197ms/step - loss: 0.0067 - accuracy: 0.9979 - val_loss: 0.0096 - val_accuracy: 0.9978\n",
            "Epoch 52/200\n",
            "87/87 [==============================] - 17s 193ms/step - loss: 0.0064 - accuracy: 0.9978 - val_loss: 0.0096 - val_accuracy: 0.9978\n",
            "Epoch 53/200\n",
            "87/87 [==============================] - 17s 196ms/step - loss: 0.0068 - accuracy: 0.9980 - val_loss: 0.0103 - val_accuracy: 0.9979\n",
            "Epoch 54/200\n",
            "87/87 [==============================] - 16s 186ms/step - loss: 0.0070 - accuracy: 0.9977 - val_loss: 0.0094 - val_accuracy: 0.9979\n",
            "Epoch 55/200\n",
            "87/87 [==============================] - 17s 196ms/step - loss: 0.0060 - accuracy: 0.9980 - val_loss: 0.0093 - val_accuracy: 0.9979\n",
            "Epoch 56/200\n",
            "87/87 [==============================] - 17s 193ms/step - loss: 0.0057 - accuracy: 0.9982 - val_loss: 0.0095 - val_accuracy: 0.9978\n",
            "Epoch 57/200\n",
            "87/87 [==============================] - 17s 198ms/step - loss: 0.0060 - accuracy: 0.9982 - val_loss: 0.0094 - val_accuracy: 0.9979\n",
            "Epoch 58/200\n",
            "87/87 [==============================] - 17s 197ms/step - loss: 0.0058 - accuracy: 0.9980 - val_loss: 0.0120 - val_accuracy: 0.9971\n",
            "Epoch 59/200\n",
            "87/87 [==============================] - 17s 193ms/step - loss: 0.0056 - accuracy: 0.9981 - val_loss: 0.0251 - val_accuracy: 0.9940\n",
            "Epoch 60/200\n",
            "87/87 [==============================] - 17s 198ms/step - loss: 0.0088 - accuracy: 0.9971 - val_loss: 0.0104 - val_accuracy: 0.9978\n",
            "Epoch 61/200\n",
            "87/87 [==============================] - 17s 192ms/step - loss: 0.0064 - accuracy: 0.9980 - val_loss: 0.0100 - val_accuracy: 0.9980\n",
            "Epoch 62/200\n",
            "87/87 [==============================] - 17s 199ms/step - loss: 0.0058 - accuracy: 0.9983 - val_loss: 0.0103 - val_accuracy: 0.9978\n",
            "Epoch 63/200\n",
            "87/87 [==============================] - 17s 194ms/step - loss: 0.0056 - accuracy: 0.9983 - val_loss: 0.0097 - val_accuracy: 0.9980\n",
            "Epoch 64/200\n",
            "87/87 [==============================] - 17s 197ms/step - loss: 0.0057 - accuracy: 0.9983 - val_loss: 0.0102 - val_accuracy: 0.9979\n",
            "Epoch 65/200\n",
            "87/87 [==============================] - 17s 193ms/step - loss: 0.0052 - accuracy: 0.9983 - val_loss: 0.0101 - val_accuracy: 0.9980\n",
            "Epoch 66/200\n",
            "87/87 [==============================] - 17s 195ms/step - loss: 0.0051 - accuracy: 0.9983 - val_loss: 0.0104 - val_accuracy: 0.9980\n",
            "Epoch 67/200\n",
            "87/87 [==============================] - 17s 194ms/step - loss: 0.0048 - accuracy: 0.9984 - val_loss: 0.0107 - val_accuracy: 0.9976\n",
            "Epoch 68/200\n",
            "87/87 [==============================] - 17s 196ms/step - loss: 0.0047 - accuracy: 0.9985 - val_loss: 0.0103 - val_accuracy: 0.9981\n",
            "Epoch 69/200\n",
            "87/87 [==============================] - 17s 199ms/step - loss: 0.0049 - accuracy: 0.9983 - val_loss: 0.0107 - val_accuracy: 0.9980\n",
            "Epoch 70/200\n",
            "87/87 [==============================] - 17s 197ms/step - loss: 0.0050 - accuracy: 0.9984 - val_loss: 0.0105 - val_accuracy: 0.9981\n",
            "Epoch 71/200\n",
            "87/87 [==============================] - 17s 194ms/step - loss: 0.0047 - accuracy: 0.9984 - val_loss: 0.0106 - val_accuracy: 0.9978\n",
            "Epoch 72/200\n",
            "87/87 [==============================] - 17s 197ms/step - loss: 0.0056 - accuracy: 0.9983 - val_loss: 0.0104 - val_accuracy: 0.9979\n",
            "Epoch 73/200\n",
            "87/87 [==============================] - 17s 195ms/step - loss: 0.0046 - accuracy: 0.9984 - val_loss: 0.0107 - val_accuracy: 0.9980\n",
            "Epoch 74/200\n",
            "87/87 [==============================] - 17s 196ms/step - loss: 0.0044 - accuracy: 0.9986 - val_loss: 0.0100 - val_accuracy: 0.9982\n",
            "Epoch 75/200\n",
            "87/87 [==============================] - 17s 197ms/step - loss: 0.0053 - accuracy: 0.9983 - val_loss: 0.0111 - val_accuracy: 0.9981\n",
            "Epoch 76/200\n",
            "87/87 [==============================] - 17s 191ms/step - loss: 0.0051 - accuracy: 0.9982 - val_loss: 0.0113 - val_accuracy: 0.9981\n",
            "Epoch 77/200\n",
            "87/87 [==============================] - 17s 193ms/step - loss: 0.0049 - accuracy: 0.9985 - val_loss: 0.0119 - val_accuracy: 0.9980\n",
            "Epoch 78/200\n",
            "87/87 [==============================] - 17s 192ms/step - loss: 0.0051 - accuracy: 0.9984 - val_loss: 0.0105 - val_accuracy: 0.9981\n",
            "Epoch 79/200\n",
            "87/87 [==============================] - 17s 199ms/step - loss: 0.0043 - accuracy: 0.9986 - val_loss: 0.0106 - val_accuracy: 0.9980\n",
            "Epoch 80/200\n",
            "87/87 [==============================] - 17s 195ms/step - loss: 0.0047 - accuracy: 0.9984 - val_loss: 0.0106 - val_accuracy: 0.9982\n",
            "Epoch 81/200\n",
            "87/87 [==============================] - 17s 194ms/step - loss: 0.0050 - accuracy: 0.9983 - val_loss: 0.0108 - val_accuracy: 0.9980\n",
            "Epoch 82/200\n",
            "87/87 [==============================] - 17s 194ms/step - loss: 0.0044 - accuracy: 0.9985 - val_loss: 0.0112 - val_accuracy: 0.9981\n",
            "Epoch 83/200\n",
            "87/87 [==============================] - 17s 197ms/step - loss: 0.0044 - accuracy: 0.9985 - val_loss: 0.0139 - val_accuracy: 0.9967\n",
            "Epoch 84/200\n",
            "87/87 [==============================] - 17s 198ms/step - loss: 0.0056 - accuracy: 0.9982 - val_loss: 0.0130 - val_accuracy: 0.9978\n",
            "Epoch 85/200\n",
            "87/87 [==============================] - 17s 193ms/step - loss: 0.0060 - accuracy: 0.9981 - val_loss: 0.0109 - val_accuracy: 0.9982\n",
            "Epoch 86/200\n",
            "87/87 [==============================] - 17s 194ms/step - loss: 0.0053 - accuracy: 0.9985 - val_loss: 0.0098 - val_accuracy: 0.9983\n",
            "Epoch 87/200\n",
            "87/87 [==============================] - 17s 197ms/step - loss: 0.0055 - accuracy: 0.9983 - val_loss: 0.0104 - val_accuracy: 0.9983\n",
            "Epoch 88/200\n",
            "87/87 [==============================] - 17s 197ms/step - loss: 0.0051 - accuracy: 0.9982 - val_loss: 0.0109 - val_accuracy: 0.9980\n",
            "Epoch 89/200\n",
            "87/87 [==============================] - 17s 197ms/step - loss: 0.0049 - accuracy: 0.9983 - val_loss: 0.0103 - val_accuracy: 0.9982\n",
            "Epoch 90/200\n",
            "87/87 [==============================] - 17s 195ms/step - loss: 0.0045 - accuracy: 0.9984 - val_loss: 0.0103 - val_accuracy: 0.9983\n",
            "Epoch 91/200\n",
            "87/87 [==============================] - 17s 195ms/step - loss: 0.0047 - accuracy: 0.9984 - val_loss: 0.0104 - val_accuracy: 0.9981\n",
            "Epoch 92/200\n",
            "87/87 [==============================] - 17s 198ms/step - loss: 0.0042 - accuracy: 0.9986 - val_loss: 0.0101 - val_accuracy: 0.9982\n",
            "Epoch 93/200\n",
            "87/87 [==============================] - 17s 199ms/step - loss: 0.0047 - accuracy: 0.9985 - val_loss: 0.0099 - val_accuracy: 0.9983\n",
            "Epoch 94/200\n",
            "87/87 [==============================] - 17s 196ms/step - loss: 0.0043 - accuracy: 0.9986 - val_loss: 0.0099 - val_accuracy: 0.9983\n",
            "Epoch 95/200\n",
            "87/87 [==============================] - 17s 198ms/step - loss: 0.0051 - accuracy: 0.9984 - val_loss: 0.0103 - val_accuracy: 0.9983\n",
            "Epoch 96/200\n",
            "87/87 [==============================] - 17s 197ms/step - loss: 0.0041 - accuracy: 0.9987 - val_loss: 0.0103 - val_accuracy: 0.9983\n",
            "Epoch 97/200\n",
            "87/87 [==============================] - 17s 198ms/step - loss: 0.0038 - accuracy: 0.9988 - val_loss: 0.0108 - val_accuracy: 0.9981\n",
            "Epoch 98/200\n",
            "87/87 [==============================] - 17s 196ms/step - loss: 0.0041 - accuracy: 0.9986 - val_loss: 0.0099 - val_accuracy: 0.9985\n",
            "Epoch 99/200\n",
            "87/87 [==============================] - 17s 201ms/step - loss: 0.0041 - accuracy: 0.9986 - val_loss: 0.0102 - val_accuracy: 0.9984\n",
            "Epoch 100/200\n",
            "87/87 [==============================] - 17s 197ms/step - loss: 0.0042 - accuracy: 0.9985 - val_loss: 0.0115 - val_accuracy: 0.9978\n",
            "Epoch 101/200\n",
            "87/87 [==============================] - 17s 198ms/step - loss: 0.0035 - accuracy: 0.9990 - val_loss: 0.0104 - val_accuracy: 0.9983\n",
            "Epoch 102/200\n",
            "87/87 [==============================] - 17s 200ms/step - loss: 0.0040 - accuracy: 0.9985 - val_loss: 0.0106 - val_accuracy: 0.9984\n",
            "Epoch 103/200\n",
            "87/87 [==============================] - 17s 198ms/step - loss: 0.0047 - accuracy: 0.9985 - val_loss: 0.0102 - val_accuracy: 0.9984\n",
            "Epoch 104/200\n",
            "87/87 [==============================] - 17s 197ms/step - loss: 0.0038 - accuracy: 0.9988 - val_loss: 0.0106 - val_accuracy: 0.9983\n",
            "Epoch 105/200\n",
            "87/87 [==============================] - 17s 195ms/step - loss: 0.0041 - accuracy: 0.9986 - val_loss: 0.0107 - val_accuracy: 0.9983\n",
            "Epoch 106/200\n",
            "87/87 [==============================] - 17s 199ms/step - loss: 0.0041 - accuracy: 0.9986 - val_loss: 0.0104 - val_accuracy: 0.9982\n",
            "Epoch 107/200\n",
            "87/87 [==============================] - 17s 196ms/step - loss: 0.0035 - accuracy: 0.9988 - val_loss: 0.0115 - val_accuracy: 0.9978\n",
            "Epoch 108/200\n",
            "87/87 [==============================] - 17s 199ms/step - loss: 0.0035 - accuracy: 0.9987 - val_loss: 0.0106 - val_accuracy: 0.9984\n",
            "Epoch 109/200\n",
            "87/87 [==============================] - 17s 196ms/step - loss: 0.0037 - accuracy: 0.9987 - val_loss: 0.0108 - val_accuracy: 0.9982\n",
            "Epoch 110/200\n",
            "87/87 [==============================] - 17s 197ms/step - loss: 0.0032 - accuracy: 0.9990 - val_loss: 0.0126 - val_accuracy: 0.9979\n",
            "Epoch 111/200\n",
            "87/87 [==============================] - 17s 193ms/step - loss: 0.0040 - accuracy: 0.9984 - val_loss: 0.0105 - val_accuracy: 0.9984\n",
            "Epoch 112/200\n",
            "87/87 [==============================] - 17s 195ms/step - loss: 0.0039 - accuracy: 0.9986 - val_loss: 0.0106 - val_accuracy: 0.9984\n",
            "Epoch 113/200\n",
            "87/87 [==============================] - 17s 197ms/step - loss: 0.0040 - accuracy: 0.9986 - val_loss: 0.0101 - val_accuracy: 0.9982\n",
            "Epoch 114/200\n",
            "87/87 [==============================] - 17s 195ms/step - loss: 0.0033 - accuracy: 0.9988 - val_loss: 0.0105 - val_accuracy: 0.9983\n",
            "Epoch 115/200\n",
            "87/87 [==============================] - 17s 194ms/step - loss: 0.0034 - accuracy: 0.9987 - val_loss: 0.0112 - val_accuracy: 0.9983\n",
            "Epoch 116/200\n",
            "87/87 [==============================] - 17s 195ms/step - loss: 0.0034 - accuracy: 0.9988 - val_loss: 0.0128 - val_accuracy: 0.9974\n",
            "Epoch 117/200\n",
            "87/87 [==============================] - 17s 199ms/step - loss: 0.0047 - accuracy: 0.9983 - val_loss: 0.0121 - val_accuracy: 0.9980\n",
            "Epoch 118/200\n",
            "87/87 [==============================] - 17s 199ms/step - loss: 0.0043 - accuracy: 0.9985 - val_loss: 0.0104 - val_accuracy: 0.9982\n",
            "Epoch 119/200\n",
            "87/87 [==============================] - 17s 198ms/step - loss: 0.0038 - accuracy: 0.9987 - val_loss: 0.0125 - val_accuracy: 0.9982\n",
            "Epoch 120/200\n",
            "87/87 [==============================] - 17s 198ms/step - loss: 0.0041 - accuracy: 0.9986 - val_loss: 0.0112 - val_accuracy: 0.9982\n",
            "Epoch 121/200\n",
            "87/87 [==============================] - 17s 196ms/step - loss: 0.0040 - accuracy: 0.9986 - val_loss: 0.0103 - val_accuracy: 0.9984\n",
            "Epoch 122/200\n",
            "87/87 [==============================] - 17s 197ms/step - loss: 0.0035 - accuracy: 0.9987 - val_loss: 0.0098 - val_accuracy: 0.9984\n",
            "Epoch 123/200\n",
            "87/87 [==============================] - 17s 197ms/step - loss: 0.0031 - accuracy: 0.9989 - val_loss: 0.0141 - val_accuracy: 0.9984\n",
            "Epoch 124/200\n",
            "87/87 [==============================] - 17s 195ms/step - loss: 0.0039 - accuracy: 0.9987 - val_loss: 0.0122 - val_accuracy: 0.9984\n",
            "Epoch 125/200\n",
            "87/87 [==============================] - 17s 200ms/step - loss: 0.0034 - accuracy: 0.9990 - val_loss: 0.0125 - val_accuracy: 0.9981\n",
            "Epoch 126/200\n",
            "87/87 [==============================] - 17s 196ms/step - loss: 0.0036 - accuracy: 0.9987 - val_loss: 0.0131 - val_accuracy: 0.9985\n",
            "Epoch 127/200\n",
            "87/87 [==============================] - 17s 193ms/step - loss: 0.0028 - accuracy: 0.9991 - val_loss: 0.0128 - val_accuracy: 0.9985\n",
            "Epoch 128/200\n",
            "87/87 [==============================] - 17s 197ms/step - loss: 0.0034 - accuracy: 0.9988 - val_loss: 0.0134 - val_accuracy: 0.9984\n",
            "Epoch 129/200\n",
            "87/87 [==============================] - 17s 197ms/step - loss: 0.0039 - accuracy: 0.9987 - val_loss: 0.0128 - val_accuracy: 0.9984\n",
            "Epoch 130/200\n",
            "87/87 [==============================] - 17s 199ms/step - loss: 0.0034 - accuracy: 0.9987 - val_loss: 0.0131 - val_accuracy: 0.9985\n",
            "Epoch 131/200\n",
            "87/87 [==============================] - 17s 198ms/step - loss: 0.0036 - accuracy: 0.9987 - val_loss: 0.0132 - val_accuracy: 0.9985\n",
            "Epoch 132/200\n",
            "87/87 [==============================] - 17s 196ms/step - loss: 0.0028 - accuracy: 0.9990 - val_loss: 0.0130 - val_accuracy: 0.9985\n",
            "Epoch 133/200\n",
            "87/87 [==============================] - 17s 196ms/step - loss: 0.0041 - accuracy: 0.9985 - val_loss: 0.0133 - val_accuracy: 0.9982\n",
            "Epoch 134/200\n",
            "87/87 [==============================] - 17s 196ms/step - loss: 0.0030 - accuracy: 0.9989 - val_loss: 0.0134 - val_accuracy: 0.9985\n",
            "Epoch 135/200\n",
            "87/87 [==============================] - 17s 198ms/step - loss: 0.0035 - accuracy: 0.9989 - val_loss: 0.0128 - val_accuracy: 0.9985\n",
            "Epoch 136/200\n",
            "87/87 [==============================] - 17s 199ms/step - loss: 0.0030 - accuracy: 0.9990 - val_loss: 0.0132 - val_accuracy: 0.9985\n",
            "Epoch 137/200\n",
            "87/87 [==============================] - 18s 202ms/step - loss: 0.0036 - accuracy: 0.9987 - val_loss: 0.0099 - val_accuracy: 0.9984\n",
            "Epoch 138/200\n",
            "87/87 [==============================] - 17s 199ms/step - loss: 0.0032 - accuracy: 0.9989 - val_loss: 0.0106 - val_accuracy: 0.9985\n",
            "Epoch 139/200\n",
            "87/87 [==============================] - 17s 194ms/step - loss: 0.0031 - accuracy: 0.9990 - val_loss: 0.0105 - val_accuracy: 0.9984\n",
            "Epoch 140/200\n",
            "87/87 [==============================] - 17s 196ms/step - loss: 0.0028 - accuracy: 0.9990 - val_loss: 0.0103 - val_accuracy: 0.9984\n",
            "Epoch 141/200\n",
            "87/87 [==============================] - 17s 198ms/step - loss: 0.0033 - accuracy: 0.9990 - val_loss: 0.0114 - val_accuracy: 0.9984\n",
            "Epoch 142/200\n",
            "87/87 [==============================] - 17s 200ms/step - loss: 0.0031 - accuracy: 0.9989 - val_loss: 0.0104 - val_accuracy: 0.9983\n",
            "Epoch 143/200\n",
            "87/87 [==============================] - 17s 198ms/step - loss: 0.0037 - accuracy: 0.9988 - val_loss: 0.0106 - val_accuracy: 0.9985\n",
            "Epoch 144/200\n",
            "87/87 [==============================] - 17s 196ms/step - loss: 0.0034 - accuracy: 0.9989 - val_loss: 0.0111 - val_accuracy: 0.9985\n",
            "Epoch 145/200\n",
            "87/87 [==============================] - 17s 196ms/step - loss: 0.0029 - accuracy: 0.9989 - val_loss: 0.0105 - val_accuracy: 0.9987\n",
            "Epoch 146/200\n",
            "87/87 [==============================] - 17s 197ms/step - loss: 0.0027 - accuracy: 0.9991 - val_loss: 0.0108 - val_accuracy: 0.9983\n",
            "Epoch 147/200\n",
            "87/87 [==============================] - 17s 194ms/step - loss: 0.0029 - accuracy: 0.9990 - val_loss: 0.0104 - val_accuracy: 0.9985\n",
            "Epoch 148/200\n",
            "87/87 [==============================] - 17s 200ms/step - loss: 0.0030 - accuracy: 0.9989 - val_loss: 0.0107 - val_accuracy: 0.9984\n",
            "Epoch 149/200\n",
            "87/87 [==============================] - 17s 198ms/step - loss: 0.0030 - accuracy: 0.9991 - val_loss: 0.0106 - val_accuracy: 0.9985\n",
            "Epoch 150/200\n",
            "87/87 [==============================] - 17s 200ms/step - loss: 0.0031 - accuracy: 0.9990 - val_loss: 0.0108 - val_accuracy: 0.9982\n",
            "Epoch 151/200\n",
            "87/87 [==============================] - 17s 198ms/step - loss: 0.0035 - accuracy: 0.9990 - val_loss: 0.0103 - val_accuracy: 0.9984\n",
            "Epoch 152/200\n",
            "87/87 [==============================] - 17s 198ms/step - loss: 0.0031 - accuracy: 0.9989 - val_loss: 0.0111 - val_accuracy: 0.9984\n",
            "Epoch 153/200\n",
            "87/87 [==============================] - 17s 195ms/step - loss: 0.0036 - accuracy: 0.9989 - val_loss: 0.0110 - val_accuracy: 0.9985\n",
            "Epoch 154/200\n",
            "87/87 [==============================] - 17s 198ms/step - loss: 0.0030 - accuracy: 0.9991 - val_loss: 0.0108 - val_accuracy: 0.9983\n",
            "Epoch 155/200\n",
            "87/87 [==============================] - 17s 194ms/step - loss: 0.0029 - accuracy: 0.9992 - val_loss: 0.0106 - val_accuracy: 0.9985\n",
            "Epoch 156/200\n",
            "87/87 [==============================] - 17s 198ms/step - loss: 0.0029 - accuracy: 0.9989 - val_loss: 0.0106 - val_accuracy: 0.9984\n",
            "Epoch 157/200\n",
            "87/87 [==============================] - 17s 196ms/step - loss: 0.0029 - accuracy: 0.9989 - val_loss: 0.0111 - val_accuracy: 0.9984\n",
            "Epoch 158/200\n",
            "87/87 [==============================] - 17s 199ms/step - loss: 0.0033 - accuracy: 0.9989 - val_loss: 0.0102 - val_accuracy: 0.9986\n",
            "Epoch 159/200\n",
            "87/87 [==============================] - 18s 202ms/step - loss: 0.0030 - accuracy: 0.9989 - val_loss: 0.0109 - val_accuracy: 0.9984\n",
            "Epoch 160/200\n",
            "87/87 [==============================] - 17s 195ms/step - loss: 0.0038 - accuracy: 0.9987 - val_loss: 0.0108 - val_accuracy: 0.9984\n",
            "Epoch 161/200\n",
            "87/87 [==============================] - 17s 197ms/step - loss: 0.0033 - accuracy: 0.9989 - val_loss: 0.0109 - val_accuracy: 0.9984\n",
            "Epoch 162/200\n",
            "87/87 [==============================] - 17s 201ms/step - loss: 0.0033 - accuracy: 0.9988 - val_loss: 0.0105 - val_accuracy: 0.9985\n",
            "Epoch 163/200\n",
            "87/87 [==============================] - 17s 199ms/step - loss: 0.0031 - accuracy: 0.9990 - val_loss: 0.0103 - val_accuracy: 0.9985\n",
            "Epoch 164/200\n",
            "87/87 [==============================] - 17s 195ms/step - loss: 0.0026 - accuracy: 0.9992 - val_loss: 0.0105 - val_accuracy: 0.9986\n",
            "Epoch 165/200\n",
            "87/87 [==============================] - 17s 201ms/step - loss: 0.0030 - accuracy: 0.9990 - val_loss: 0.0109 - val_accuracy: 0.9983\n",
            "Epoch 166/200\n",
            "87/87 [==============================] - 17s 199ms/step - loss: 0.0033 - accuracy: 0.9989 - val_loss: 0.0182 - val_accuracy: 0.9975\n",
            "Epoch 167/200\n",
            "87/87 [==============================] - 17s 199ms/step - loss: 0.0046 - accuracy: 0.9985 - val_loss: 0.0110 - val_accuracy: 0.9985\n",
            "Epoch 168/200\n",
            "87/87 [==============================] - 17s 197ms/step - loss: 0.0026 - accuracy: 0.9992 - val_loss: 0.0111 - val_accuracy: 0.9986\n",
            "Epoch 169/200\n",
            "87/87 [==============================] - 17s 197ms/step - loss: 0.0025 - accuracy: 0.9992 - val_loss: 0.0104 - val_accuracy: 0.9985\n",
            "Epoch 170/200\n",
            "87/87 [==============================] - 17s 198ms/step - loss: 0.0030 - accuracy: 0.9990 - val_loss: 0.0114 - val_accuracy: 0.9985\n",
            "Epoch 171/200\n",
            "87/87 [==============================] - 17s 199ms/step - loss: 0.0027 - accuracy: 0.9991 - val_loss: 0.0111 - val_accuracy: 0.9986\n",
            "Epoch 172/200\n",
            "87/87 [==============================] - 17s 196ms/step - loss: 0.0032 - accuracy: 0.9990 - val_loss: 0.0114 - val_accuracy: 0.9987\n",
            "Epoch 173/200\n",
            "87/87 [==============================] - 17s 200ms/step - loss: 0.0025 - accuracy: 0.9991 - val_loss: 0.0114 - val_accuracy: 0.9987\n",
            "Epoch 174/200\n",
            "87/87 [==============================] - 17s 201ms/step - loss: 0.0033 - accuracy: 0.9988 - val_loss: 0.0116 - val_accuracy: 0.9986\n",
            "Epoch 175/200\n",
            "87/87 [==============================] - 17s 201ms/step - loss: 0.0025 - accuracy: 0.9991 - val_loss: 0.0110 - val_accuracy: 0.9985\n",
            "Epoch 176/200\n",
            "87/87 [==============================] - 17s 200ms/step - loss: 0.0025 - accuracy: 0.9992 - val_loss: 0.0109 - val_accuracy: 0.9986\n",
            "Epoch 177/200\n",
            "87/87 [==============================] - 17s 201ms/step - loss: 0.0026 - accuracy: 0.9991 - val_loss: 0.0113 - val_accuracy: 0.9986\n",
            "Epoch 178/200\n",
            "87/87 [==============================] - 17s 199ms/step - loss: 0.0029 - accuracy: 0.9991 - val_loss: 0.0105 - val_accuracy: 0.9985\n",
            "Epoch 179/200\n",
            "87/87 [==============================] - 17s 200ms/step - loss: 0.0030 - accuracy: 0.9990 - val_loss: 0.0107 - val_accuracy: 0.9985\n",
            "Epoch 180/200\n",
            "87/87 [==============================] - 17s 201ms/step - loss: 0.0026 - accuracy: 0.9991 - val_loss: 0.0104 - val_accuracy: 0.9987\n",
            "Epoch 181/200\n",
            "87/87 [==============================] - 17s 198ms/step - loss: 0.0032 - accuracy: 0.9989 - val_loss: 0.0109 - val_accuracy: 0.9986\n",
            "Epoch 182/200\n",
            "87/87 [==============================] - 17s 199ms/step - loss: 0.0032 - accuracy: 0.9988 - val_loss: 0.0110 - val_accuracy: 0.9985\n",
            "Epoch 183/200\n",
            "87/87 [==============================] - 17s 200ms/step - loss: 0.0029 - accuracy: 0.9991 - val_loss: 0.0111 - val_accuracy: 0.9986\n",
            "Epoch 184/200\n",
            "87/87 [==============================] - 18s 201ms/step - loss: 0.0028 - accuracy: 0.9990 - val_loss: 0.0127 - val_accuracy: 0.9983\n",
            "Epoch 185/200\n",
            "87/87 [==============================] - 17s 195ms/step - loss: 0.0033 - accuracy: 0.9986 - val_loss: 0.0108 - val_accuracy: 0.9985\n",
            "Epoch 186/200\n",
            "87/87 [==============================] - 17s 198ms/step - loss: 0.0025 - accuracy: 0.9991 - val_loss: 0.0107 - val_accuracy: 0.9987\n",
            "Epoch 187/200\n",
            "87/87 [==============================] - 17s 197ms/step - loss: 0.0033 - accuracy: 0.9988 - val_loss: 0.0115 - val_accuracy: 0.9986\n",
            "Epoch 188/200\n",
            "87/87 [==============================] - 17s 197ms/step - loss: 0.0028 - accuracy: 0.9990 - val_loss: 0.0112 - val_accuracy: 0.9986\n",
            "Epoch 189/200\n",
            "87/87 [==============================] - 17s 199ms/step - loss: 0.0024 - accuracy: 0.9991 - val_loss: 0.0107 - val_accuracy: 0.9985\n",
            "Epoch 190/200\n",
            "87/87 [==============================] - 17s 200ms/step - loss: 0.0024 - accuracy: 0.9992 - val_loss: 0.0112 - val_accuracy: 0.9986\n",
            "Epoch 191/200\n",
            "87/87 [==============================] - 17s 197ms/step - loss: 0.0026 - accuracy: 0.9991 - val_loss: 0.0117 - val_accuracy: 0.9985\n",
            "Epoch 192/200\n",
            "87/87 [==============================] - 17s 201ms/step - loss: 0.0026 - accuracy: 0.9992 - val_loss: 0.0118 - val_accuracy: 0.9985\n",
            "Epoch 193/200\n",
            "87/87 [==============================] - 17s 196ms/step - loss: 0.0021 - accuracy: 0.9994 - val_loss: 0.0113 - val_accuracy: 0.9984\n",
            "Epoch 194/200\n",
            "87/87 [==============================] - 17s 200ms/step - loss: 0.0024 - accuracy: 0.9992 - val_loss: 0.0113 - val_accuracy: 0.9985\n",
            "Epoch 195/200\n",
            "87/87 [==============================] - 17s 199ms/step - loss: 0.0029 - accuracy: 0.9991 - val_loss: 0.0118 - val_accuracy: 0.9984\n",
            "Epoch 196/200\n",
            "87/87 [==============================] - 17s 198ms/step - loss: 0.0029 - accuracy: 0.9991 - val_loss: 0.0122 - val_accuracy: 0.9985\n",
            "Epoch 197/200\n",
            "87/87 [==============================] - 17s 200ms/step - loss: 0.0026 - accuracy: 0.9991 - val_loss: 0.0116 - val_accuracy: 0.9986\n",
            "Epoch 198/200\n",
            "87/87 [==============================] - 17s 201ms/step - loss: 0.0028 - accuracy: 0.9991 - val_loss: 0.0125 - val_accuracy: 0.9985\n",
            "Epoch 199/200\n",
            "87/87 [==============================] - 17s 197ms/step - loss: 0.0022 - accuracy: 0.9992 - val_loss: 0.0116 - val_accuracy: 0.9984\n",
            "Epoch 200/200\n",
            "87/87 [==============================] - 17s 197ms/step - loss: 0.0024 - accuracy: 0.9992 - val_loss: 0.0113 - val_accuracy: 0.9987\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "PW_Mq1ZZS1Nc",
        "outputId": "b537aa38-36ac-4a34-bf88-66f69e04244b"
      },
      "source": [
        "from matplotlib import pyplot\r\n",
        "# plot learning curves\r\n",
        "pyplot.plot(history.history['accuracy'], label='train')\r\n",
        "pyplot.plot(history.history['val_accuracy'], label='test')\r\n",
        "pyplot.title('lrate='+str(lrate), pad=-50)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'lrate=0.1')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEFCAYAAADt1CyEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXhc9X3v8fd3RqstWbItWTaW9wUQuxFLEowpSVM7JDiYNoEkLbS5IUvp03t7uX2guZdyaShNQ5Mmt9w2pKUJIWUpSXpJAwVq9oRNgG0wxvsqb7K175o53/vHOZJHI8mWbdkSx5/X8+jRmd85c+Z7zow++s1vzjlj7o6IiMRXYrQLEBGRE0tBLyIScwp6EZGYU9CLiMScgl5EJOYU9CIiMaeglw88M9tmZh8b7TpExioFvZzSzMzNbP4JWO8kM/u5mbWZ2XYz+9xhlv0NM3vOzJrMbNtI1yKioJfYMrOcUXz4e4FuoAL4PPD3ZnbWEMu2AfcD/+Mk1SanGAW9xIaZ3WFmj5nZg2bWDNxoZheb2Stm1mhme8zs78wsL1r+xeiuq82s1cw+G7V/0sxWRff5tZmde5R1jAeuBf6Xu7e6+8vA48DvDra8u7/u7j8GthzjposcloJe4mY58BhQCvwESAP/DSgDPgR8FPgagLtfHt3nPHcvcvdHzOwCwt71l4HJwPeBx80sH8DM/j36BzDYz79H61sIpNx9Q0Zdq4GhevQiJ5SCXuLmFXf/N3cP3L3D3d9091fdPeXu2wiDe8lh7n8T8H13f83d0+7+I6ALuBTA3T/p7qVD/HwyWkcR0Jy13iageES3VGSYRnMMU+RE2Jl5w8wWAt8GqoFxhK/5Nw9z/1nADWb2RxltecBpR1FDKzAhq20C0HIU6xAZMerRS9xkX47174H3gQXuPgH4M8AOc/+dwF1ZPfVx7v4QgJk9GY3nD/bzZLSODUCOmS3IWO95wNoR2UKRo6Sgl7grJhxGaTWzM4CvZs3fB8zNuP0D4CtmdomFxpvZVWZWDODuy6Lx/MF+lkXLtAE/A+6M7v8Rws8OfjxYgWaWMLMCIDe8aQW9HxiLjAQFvcTdLcDnCIdNfgA8kjX/DuBH0Yepn3H3GuBLwN8BDcAm4MZjeNyvAYXAfuAh4KvuvhbAzBabWWvGspcDHcATwMxo+uljeEyRQZm+eEREJN7UoxcRibkxd9RNWVmZz549e7TLEBH5QHnzzTcPuHv5YPPGXNDPnj2bmpqa0S5DROQDxcy2DzVPQzciIjGnoBcRiTkFvYhIzB0x6M3sfjPbb2bvDjHfzOx7ZrbJzNaY2aKMeTeY2cbo54aRLFxERIZnOD36HwJLDzN/GbAg+rmJ8JRzzGwS8OfAJcDFwJ+b2cTjKVZERI7eEYPe3V8E6g+zyHLgAQ+9CpSa2TTgt4Bn3L3e3RuAZzj8PwwRETkBRmKMfjr9rxi4K2obqn0AM7vJzGrMrKaurm4EShIRkV5j4sNYd7/P3avdvbq8fNDj/UVETqi2rhSdPekRXcdbOxr44a+2srO+/XjLOy4jccJULTAj43Zl1FYLXJHV/vwIPJ7IqAgCpzsdUJCb7Gtzd1KBk5s81Gdq6exhfF4OiYT1LdPU0UPpuIEXpHR3NuxrJSdpzCsvOmINXak0B1u7OdjaTX17NzkJY+K4PGZMKmTDvlbe293Egopizp5eQlH+oT/vtq4Uv958kFQ6YNGsieysb2dHfTttXSnKi/OZOWk800oK2HKgjdrGDorykzR3pNjX3Mn+li4mF+Uxa9J42rpTJM0oKsihKD+HxvYe9jR1MKEwFxwOtnVT39ZFT9opys+hqCCH8fk5FOUnKcrPBaCupYv39jSxr7mLsqJ8yovyOK20kKrTJrCroYP39zRTkJekOD+HZCLB3qYOHBifn8Pepk6aO3pwoGJCAWVFeZgZq3Y2sqWuldauFIW5SZIJo6UzRXNnD+m0Uz4hHwivTz1j0jgOtnaz/WAb3emAaSWFlBfn8/aOBtzhjGnFTCkuoHRcLkX5OTR39NDZE1CQm+D8GaWUFxewYV8L7d0pDrR2s3F/C6WFefSkA2q2N5AOnHF5SYryc9jf0gXAHb94jwkFOSSj5yuRMLpTAUX5YVtXKk1XKuCMKeP4/g2XHO1L84hGIugfB242s4cJP3htcvc9ZvYU8JcZH8B+HLhtBB5PYsrdMTPcnV0NHbR3p5k6oYCCvASPvLGTzp40H6+aSn5uAndIB87z6/czaXw+V507rd+60oHTkxXKXak0STOSCWN/SxfPvLePlzceYE9zJ7MmjWPxgjKeX1/HzoZ2chLGuVPzKU128daBHNbva2Ffc/hHe870Ei6cNZHmzh5+tekAje09fKyqgnTaeae2idrGDmZOGsfV551GW3eKlev2s6O+nQ/NnUz17IkkzJg4LpctB9pYuW4/tY0dAFTPmsi88iLychK0daV4fVs9je09nDejhHF5ORxsamXrnjpagzx6sv5051ktU62eRi9ivc8gbTnMKC1kTmoLxT376exJEzhs9tPY7lPJp5u5todSa6WUVrrIZXUwjzJrYjydrPG5pMgBnMm5PdT35OAYs2wfk2lmorXw4cR7bPTpPJK+gjKaKLBudnsZ5+TW0p4sZlPXRIK+ayY606gnaWl2+RRm57ewqKieV7dPY2+7M5N9fCSxllqfzPPB+XRx6J9imTUzxRopoYXzcnbSkT+dlxMXsb+1i550wBWJVZxV2MD5pSU0TF6A93RQlDrItKJ65uetY2L3HprTRbQlJ9CUKGXj/jIK83OYObWT8UErO1MlvNU2m2vPKmR2y1uUHlzFxtZZrE3P5ECqgIW5DdTlTuU/Uhfx2qp3uCr5Kpcm1tFj+XTkFHNVYR47Oio4aKXcPK+VZG4B+3wie4ISplfO5rzZU1i3+nW2p0rZnT+HsgM1zOhcz8RkPfsoY1POGdQXz+Z3Gu6jtBXg4RH+yxrG1SvN7CHCnnkZ4bW7/5zwutm4+z+YmRFe0nUp0A78fnSpV8zsDwi/6AHCL3P45yMVVF1d7boEwuhzd55fX8fBtm5KC3M5a/oE8nOSuDuTi/L7lmnvTlPf1s2epk7e3tFAVypgakkBb25rYHdTB/k5Sdbtaaato5P5FSXMLmjDgZ3dxZjBuLwk+TlJDu7ZSknzBpIVZ7C7OUVzawvbvYJcC6gqbOS99hKm0MCixEZeDapoZhzzbTdrfRa5yQQr/+QKJgUH2Lryn8jd8RLd7c3sSJfxy6lfZXJ+wNz6l8hv2c72oJy1toCJQQPT7QBzC9uYUJCE1n1MDg4SJPJpLZ7NXpvCsqZ/ZbI10WQTeKTy6/ScdhGLt/0fptS9wtRgLwHG7rw5NBbMoLO5joZkGXWTqzmw4LME7/yUJY0/42XOZ8O05cybcRpnv/3nzOvZRCe5vBCcR1eikMuLdjE9v4vNky7nGw0f42BrN3k9TVyZeJv5ExMU5edwsKGBj3S9xBnpQ19Bm8oZRzp/Is0VF9NqRcze/CAWfedKOllIc245OT3NFKcb+z+vlmDH5MVMa15NXnf/eZnSucV4Tj7JrkYsSOF5RaTyJpDburtvmcBySHiKdOkcEk3bMQ9wS2KehkQufuGNBPvew/a/B+keEj3hlZl7SueR07QtXO4wgrwiPKeAZPuBgTNnLyb47R/Ruf4Zxv3iy0OvpGQGlC2EziboqIeWfdDTdmh+/gToyvjWx0QOTD0XDmyA7tb+66o4B69bhwUpgskLSODheoMUdDQcdlv6JPMhHXYYyB0HPdGQjiXCnw//EVx5OySOflTdzN509+pB5421yxSfMkGf7oFk7oisavvBNp55ZyfTJpUwoTB8K3jBhFaCPe+ybl8bW1KT6GprZkLjOl6rL2JzVzFzJjiXzSxkfgls37Of2v0HSHe2MH18QG7+eNa35LG23phuByiki3vTnyYggRn84SWTOefgE+Ts+DVvpOZRbO1cnHifnT6FgATzbReeyGVnwRn8OPe3+XrwD5zf9ivSJEgSkCbBc4W/xcFkGRU9OzmtZxcLg00DtquzYAr0dFCQbiFI5JIIegAILEmQyCMn3cHGy77Dp16YxpdO28Z/2X8XJd7CembD+DLmdKzFg4B8wj+szmQRBen+f7yeOw6zJMG4SbQXTGNcMkVi/3vQ045XXkxw5nKSax6C+i1haNRvgdOXQvmZ4R/4nlXQtAsKJ0HTTmiuhYqzYd9avHgq1rI3fJ6Lp0LzbjjjKryjCba/DEEaK1sIZlD3PvzBU7D5OXj5O4fCoFf5mXDmp6CgBHo6wmBp3QsbnwmD6sIb4dzPQste2PkatB2AnAKYfRmULwQM3OHdn8Kqn8CcxXDWChhfDoUTw3XUvglFFWG9W18ED8LtKigJt7GtLlzfpDmQUwjTL4Q1j8DbP4a5V8CE6dCwFaacBZtXhvMmzYW5vxGuc/J8SHfDpv8Mw3Tmh+DA+vBxiipg9mKo3ww7XgNPhyHa3Rpue+mMsI6y02H9L+HJW2Hhb8H+deF2/t7/g87GcD/mjYeiqeE6x0/uvx/dw+2wBBSUQjIHWvcfWk/ZAhg3CdIpaN0X7ucJp4X77fX7wm350Ndg4uz+6207AO0HoXRmuI0t+8Lnp3V/GOZlp4e17V0TrmPO4nB72g7C5mehtgYu+F2YevaxxoCCfsS110NuIXS1hC/mMz8VPvHr/wOKpsD0RRCko//SWd9al+6BZ78Bv/4ePm4yQXkVXlFF58QzaG08QO6258kvn0uyeAoNu95nx8xPs7bgQn616QALKoq4cOZEarY3UDGhgDMrxtH4ygPM2PwvzGcXX+i+jTf9dD6UWMsPcr9NkXWM2Ca/sfBPeHfW77FhXwufXvUlLkm8T0NOORNTdQSWpGXS2RR17SPhTsfE0ynIcRLbXg7/wNM9cMmXwx5M8VQ4sBHe/GEYlCWVYRjMvgxmXhrOMwMMtr4AueOhshoatkFhKcy4FDY+Bd3tsOFJKJnJAxNu4vNrbmAzlTRc9QMuqr40HB+v3wovfgtKZ8GFN4SP3VQb/lFPmBY+dkHJwI3t6QwDq/yMsJbW/fBPvwmtdXDdgzDvysF3kjusfhh++d9h9kfgMw+EofLCN2HTs/Dpew/dt6MR8DBku9vg7y4Ow7arGc66JuzZTagMl7UEjC8b+FqCcD+01cHEWcf/JI+09vpw+war+3i9/B34zzvC6ev+Bc64auQf4wNGQT+SVj8C//bVQ2+10l1hT+qjt8N3zgY87K0c3ATpHrykEjv9EzDzUlKdrSRe+S6JuvfZPeOTrNrTyfTuLZxuOymwsLe6KTiNCmtgPJ20UEgxHTyUvpLOwikc6IAuz6HL8pnkjSxLvkFVYju1eXOpKEhBdxsH53yK8vX/wsG86Tw991YWzZ7MrEQd4woLYdr5Ya+z7QDkF7OjLcG+jiRnzJxGccnEsCeUOy7qMdaHvZniafD4H4W9jq+8DJPmEnxjKi1n/y4lK74d9iCTeWEvKNvWl2DlnWEP6Kxr+s/rbArvl1t47M/FS9+Glf+bdPlZdNbvYuNnX+T8hbOPfX2H09EQBnJJ5ZGX7WoN9+XRBNy6X8AjX4ALfx+u+vYxvXU/pQRpePDasLNwwy9OzD+TDxgF/XD1dIZvK8vPgLL5h9q3vAA/uyn8463fEvY+py8Kl2+uDUPw0q/BS/fApV/Da9/Cpp3HG7u7aN2xmsXJd8jxFAAbg+l8K/UZng4uYl75eFYsqsQ8zdTUbkqKx1MybT7v7DhIe0cnH543kbmv38GEjT8bdDyzrXgOfsWfUbTod8Ke8D9+LBx/PGsFLPvm4OF7LFr2wv+phrOvgcX/Hb57Hnzqe2EveTS17IVvV4Vv85f+FVya/XWwHzCNO8LhIYXW8LiHwz6J5JGXPQUo6IejvR4e/hzseCW8PXtx+NZ764vw8y+Hb/8nzw+HZpbeDbmF7Gpo5x8f/Al3HLwFtyRNky/gK3nfoGZbA+dUlvD2jkbOmV7Czj17mZdzgE+cWUrX1EUUFeRRObGQxQvK+x2Wd1hBOhz76+kIx/wKJ0HeuP7LNO6ARG44LDHSfnR1OKxw5f+CB1fAjb8M/+GNtkdvgL3vwNdegZz80a5GZNQcLujH3BePnHRdrfCr70LNP4XTn/peOKzw7Dfgb8+F7hY47QLaP/MIW9sLeG93M+88sZmcRIJ/X7Obju5Kvkg5M6jjr/acz8aCVn77wkpe2FDHZ6tn8JcrzqGhvZvcZIKSwuP48DWRhERhNNQxRE+9dOaxr/9IKs6Cmn8Oh6QAJs07cY91NK75PgQ9CnmRwzi1g75hOzx0Pex/DxYuhSV/Gg7JAF5ZTecvb8PO+R1+nrOMO+55i65UAEBRfg7uTnlxPg/+l0uYvOaL9Lx+L9d89g/5n3Nn9jtRBaCsKAYhNKUKUh3hMFXu+PCDzbEgtwAoGO0qRMa0Uy/ou9vDw9y2Pg//eWd4qtwXfgrzP9q3yP6WTm57LpeVO/4U2wnu77N4QRnXXzyThRVFzC0rIpGwvhN8+OifwmVf5ZLC0lHbrBNuSlX4e/Oz4aFiGkcW+cA4tYK+fgvcvyw8vhVgzuXwqe+Gh/dF3tzewFcefJPmjh5u+fhCutNOWVEen79kFslE/3Cz3rBLJMND/+JsyhmAhZ8TTJ57xMVFZOw4dYK+7QD8eEV4OOTye8Px7NmL+3qm+5s7+b/Pb+Ynr23ntNJCHvziJZw+tXiUix5D8saH5wo0bB074/MiMiynTtC//J3wGPLf/w+YcVG/WW9sq+emB2po7kxx7aLp/Nknzhz0AlSnvIqzwqCfrKAX+SA5NYI+1R2esXj6JwaE/JPv7OGPH17F9ImF/OtXPsz8KUe+guApa8qZ8P6/q0cv8gFzagT9xqeg/UB4LQng3dom7n95KwV5SR5+fQfnzyjl/hsvUi/+SBYuhQ1PhT17EfnAiHfQr34EXvzr6KJJU2Helbxb28TnfvAq6eja4pcvLOfezy1ifH68d8WIqKyGr7w02lWIyFGKb7qle+DZvwjPIu1qgcv/B41dATf+8+sUF+TyyJcvZVpJ4YAjaURE4ia+Qf/uT8MPX69/JDxGPpHDX/50DQ3tPfzi5kuonDjuyOsQEYmBeF4izx1e/tvwJJ8FH4dkLq9trefRml18afFcqk6bMNoVioicNPEM+o4GqFsH513fd7nX767cyJTifP74owtGuTgRkZMrnkHfEp35Gl07fM2uRn69+SBfvGwOhXm6pKmInFpiGvR7wt/F4eV6v//iForzc7j+khN4dUcRkTFqWEFvZkvNbL2ZbTKzWweZP8vMVprZGjN73swqM+Z908zejX4+O5LFD6m3R19cwfaDbTz5zh4+f+ksJhSMzHe0ioh8kBwx6M0sCdwLLAOqgOvNrCprsXuAB9z9XOBO4O7ovlcBi4DzgUuAW8zsxH8S2nvRsqKp/ONLW8lJJPj9j8w+4Q8rIjIWDadHfzGwyd23uHs38DCwPGuZKuDZaPq5jPlVwIvunnL3NmANsPT4yz6Clr1QUMLB7iSP1uzkmgumUzFB1ywXkVPTcIJ+OrAz4/auqC3TamBFNH0NUGxmk6P2pWY2zszKgN8AZmQ/gJndZGY1ZlZTV1d3tNswUMseKJ7GY2/uoisV8KXLdVldETl1jdSHsbcAS8zsbWAJUAuk3f1p4Ang18BDwCvAgG+5dvf73L3a3avLy8uPv5qWvVBUwZpdTcycNE4XKhORU9pwgr6W/r3wyqitj7vvdvcV7n4B8PWorTH6fZe7n+/uv0n4fU4bRqTyw2nZC8XTWLenmTOn6ZryInJqG07QvwEsMLM5ZpYHXAc8nrmAmZWZWe+6bgPuj9qT0RAOZnYucC7w9EgVPyh3aNlLz/gpbD3YxpnTdBasiJzajnitG3dPmdnNwFNAErjf3dea2Z1Ajbs/DlwB3G1mDrwI/GF091zgpegr95qBL7h7auQ3I0N7PQQ97Asm4o6CXkROecO6qJm7P0E41p7ZdnvG9GPAY4Pcr5PwyJuTJzpZamtXGPBVCnoROcXF78zY6GSp91vHUZyfQ+XEwlEuSERkdMUv6KOTpd5uLOCMacVEw0YiIqes+AV9NHTzal2uxudFRIhj0O9ZTVB8GvVdCaaXathGRCReQd/VChufoWv+MgDG6ZLEIiIxC/oN/wGpTlrmXgVAQa6CXkQkXkH/3r9BUQVNZRcC6EtGRESIU9BHwzZULacjuppOoXr0IiIxCvqedrjgC3DOZ+joDpNeQS8iMswzYz8QiqbAVX8DQMf6/QAUaOhGRCRGPfoMnT3q0YuI9Ipl0Hco6EVE+sQz6LsDQEfdiIhAXIM+6tHrOHoRkZgGvcboRUQOiWXQd3SnSSaM3KSuXCkiEs+g70lTmJvUJYpFRIhx0Gt8XkQkFMug7+xOU5Aby00TETlqw0pDM1tqZuvNbJOZ3TrI/FlmttLM1pjZ82ZWmTHvr81srZmtM7Pv2UkYT+kduhERkWEEvZklgXuBZYRf9H29mWV/4fc9wAPufi5wJ3B3dN8PAx8BzgXOBi4CloxY9UPo6EnrGHoRkchwevQXA5vcfYu7dwMPA8uzlqkCno2mn8uY70ABkAfkA7nAvuMt+kg6ujVGLyLSazhBPx3YmXF7V9SWaTWwIpq+Big2s8nu/gph8O+Jfp5y93XZD2BmN5lZjZnV1NXVHe02DNCpoRsRkT4j9YnlLcASM3ubcGimFkib2XzgTKCS8J/DlWa2OPvO7n6fu1e7e3V5eflxF6MxehGRQ4ZzmeJaYEbG7cqorY+77ybq0ZtZEXCtuzea2ZeAV929NZr3JPAh4KURqH1IGqMXETlkOD36N4AFZjbHzPKA64DHMxcwszIz613XbcD90fQOwp5+jpnlEvb2BwzdjLSO7kBj9CIikSMGvbungJuBpwhD+lF3X2tmd5rZ1dFiVwDrzWwDUAHcFbU/BmwG3iEcx1/t7r8Y2U0YSGP0IiKHDOsbptz9CeCJrLbbM6YfIwz17PulgS8fZ41Hxd2joRudMCUiAjE8M7Yn7aQDV49eRCQSu6DXtehFRPqLXdD3XYteR92IiAAxDPqObn3piIhIpvgFvb5dSkSkn9gGfYGGbkREgBgGfaeGbkRE+old0GvoRkSkv/gGvYZuRESAOAa9hm5ERPqJXdB36oQpEZF+Yhf0GroREekvfkHfHQBQkBO7TRMROSaxS8POVJq8ZIKcZOw2TUTkmMQuDbt6AvLUmxcR6RO7RAzcSdhoVyEiMnbELuhTQaBhGxGRDLFLxHQACVOXXkSkV+yCPggcdehFRA4ZViSa2VIzW29mm8zs1kHmzzKzlWa2xsyeN7PKqP03zGxVxk+nmX16pDciU9qdpHr0IiJ9jhj0ZpYE7gWWAVXA9WZWlbXYPcAD7n4ucCdwN4C7P+fu57v7+cCVQDvw9AjWP0AQOAl9Gisi0mc4PfqLgU3uvsXdu4GHgeVZy1QBz0bTzw0yH+C3gSfdvf1Yix2OtDtJBb2ISJ/hBP10YGfG7V1RW6bVwIpo+hqg2MwmZy1zHfDQYA9gZjeZWY2Z1dTV1Q2jpKGlAwW9iEimkfrY8hZgiZm9DSwBaoF070wzmwacAzw12J3d/T53r3b36vLy8uMqJB1ojF5EJFPOMJapBWZk3K6M2vq4+26iHr2ZFQHXuntjxiKfAX7u7j3HV+6RqUcvItLfcHr0bwALzGyOmeURDsE8nrmAmZWZWe+6bgPuz1rH9QwxbDPSwjNjFfQiIr2OGPTungJuJhx2WQc86u5rzexOM7s6WuwKYL2ZbQAqgLt6729mswnfEbwwopUPQT16EZH+hjN0g7s/ATyR1XZ7xvRjwGND3HcbAz+8PWHSjg6vFBHJELtzSIPAyVHQi4j0iV3Qp4JAR92IiGSIXdAHASRit1UiIscudpGoM2NFRPqLX9AHOrxSRCRT7II+UI9eRKSf2AW9LoEgItJfPINePXoRkT4KehGRmItf0Lu+eEREJFPsgj7QGL2ISD+xC3odRy8i0l/sgj4I0HH0IiIZYhf0qSDQRc1ERDLELujTgS5TLCKSKXZBH54ZO9pViIiMHbGLRJ0ZKyLSX+yCPgh0HL2ISKbYBX3a1aMXEck0rKA3s6Vmtt7MNpnZrYPMn2VmK81sjZk9b2aVGfNmmtnTZrbOzN6Lviz8hEkFTjKpoBcR6XXEoDezJHAvsAyoAq43s6qsxe4BHnD3c4E7gbsz5j0AfMvdzwQuBvaPROFD0ZmxIiL9DadHfzGwyd23uHs38DCwPGuZKuDZaPq53vnRP4Qcd38GwN1b3b19RCofgs6MFRHpbzhBPx3YmXF7V9SWaTWwIpq+Big2s8nAQqDRzH5mZm+b2beidwj9mNlNZlZjZjV1dXVHvxURd8ddZ8aKiGQaqQ9jbwGWmNnbwBKgFkgDOcDiaP5FwFzgxuw7u/t97l7t7tXl5eXHXEQ6cAD16EVEMgwn6GuBGRm3K6O2Pu6+291XuPsFwNejtkbC3v+qaNgnBfwbsGhEKh9E2hX0IiLZhhP0bwALzGyOmeUB1wGPZy5gZmVm1ruu24D7M+5bama93fQrgfeOv+zBqUcvIjLQEYM+6onfDDwFrAMedfe1ZnanmV0dLXYFsN7MNgAVwF3RfdOEwzYrzewdwIAfjPhWRPqCXmP0IiJ9coazkLs/ATyR1XZ7xvRjwGND3PcZ4NzjqHHYgiD8rTNjRUQOidWZsX1j9Mp5EZE+8Qp6jdGLiAwQq6APoh69hm5ERA6JVdCnoh69vmFKROSQWAV9EAW9zowVETkkVkGvMXoRkYHiFfQ6M1ZEZIBYBb2GbkREBopV0KtHLyIyUKyCPpVW0IuIZItV0Aeua92IiGSLVdDrqBsRkYFiFfQ6M1ZEZKBYBX06unqlhm5ERA6JWdD39uhHuRARkTEkVpHYO3STo6QXEekTq0RM9X0YO8qFiIiMIbGKRJ0ZKyIyUKyCXodXiogMNKygN7OlZrbezDaZ2a2DzJ9lZivNbI2ZPW9mlRnz0ma2Kvp5fCSLz9Z7CQT16EVEDjnil4ObWRK4F/hNYBfwhpk97u7vZSx2D/CAu//IzK4E7gZ+N5rX4bR5HUoAAAs4SURBVO7nj3DdgwrUoxcRGWA4PfqLgU3uvsXdu4GHgeVZy1QBz0bTzw0y/6RIu75hSkQk23CCfjqwM+P2rqgt02pgRTR9DVBsZpOj2wVmVmNmr5rZpwd7ADO7KVqmpq6u7ijK7+/QcfQKehGRXiP1YewtwBIzextYAtQC6WjeLHevBj4H/K2Zzcu+s7vf5+7V7l5dXl5+zEX0fRirMXoRkT5HHKMnDO0ZGbcro7Y+7r6bqEdvZkXAte7eGM2rjX5vMbPngQuAzcdd+SB01I2IyEDD6dG/ASwwszlmlgdcB/Q7esbMysysd123AfdH7RPNLL93GeAjQOaHuCNKFzUTERnoiEHv7ingZuApYB3wqLuvNbM7zezqaLErgPVmtgGoAO6K2s8EasxsNeGHtH+VdbTOiNJFzUREBhrO0A3u/gTwRFbb7RnTjwGPDXK/XwPnHGeNw6avEhQRGSheZ8ZGXXoFvYjIIfEK+rBDr6EbEZEMsQr6QNejFxEZIFaRqDF6EZGB4hX0ukyxiMgAsQr63qEbXetGROSQWAV9SmfGiogMEKugD9wxA9PQjYhIn1gFfTpwHVopIpIlXkHvruvciIhkiVXQB+rRi4gMEKugTwf6IFZEJFvMgj5Q0IuIZIlX0Lsr6EVEssQr6AOdFSsiki1WQR8ETjJWWyQicvxiFYtp11E3IiLZYhX0QaDj6EVEssUq6FOB64JmIiJZhhX0ZrbUzNab2SYzu3WQ+bPMbKWZrTGz582sMmv+BDPbZWZ/N1KFD0ZnxoqIDHTEoDezJHAvsAyoAq43s6qsxe4BHnD3c4E7gbuz5v8F8OLxl3t4OjNWRGSg4fToLwY2ufsWd+8GHgaWZy1TBTwbTT+XOd/MLgQqgKePv9zDSwc6jl5EJNtwgn46sDPj9q6oLdNqYEU0fQ1QbGaTzSwB/A1wy+EewMxuMrMaM6upq6sbXuWDCNx1HL2ISJaR+jD2FmCJmb0NLAFqgTTwNeAJd991uDu7+33uXu3u1eXl5cdchHr0IiID5QxjmVpgRsbtyqitj7vvJurRm1kRcK27N5rZh4DFZvY1oAjIM7NWdx/wge5ISCnoRUQGGE7QvwEsMLM5hAF/HfC5zAXMrAyod/cAuA24H8DdP5+xzI1A9YkKeQiHbhT0IiL9HXHoxt1TwM3AU8A64FF3X2tmd5rZ1dFiVwDrzWwD4Qevd52geg9L3zAlIjLQcHr0uPsTwBNZbbdnTD8GPHaEdfwQ+OFRV3gUggASsToFTETk+MUqFnWZYhGRgeIV9IEOrxQRyRa7oNe1bkRE+otd0GvoRkSkv1gFvc6MFREZKFZBrx69iMhA8Qp6XaZYRGSAWAW9LlMsIjJQrII+7TrqRkQkW7yCPq2hGxGRbPEKetfQjYhItngFfYB69CIiWWIV9OFlike7ChGRsSVWsajLFIuIDBSroA8CJ6nrFIuI9BOrVAy/SnC0qxARGVtiFYs6M1ZEZKBYBb3OjBURGShWQa9vmBIRGWhYQW9mS81svZltMrNbB5k/y8xWmtkaM3vezCoz2t8ys1VmttbMvjLSG9DL3XFHlykWEclyxKA3syRwL7AMqAKuN7OqrMXuAR5w93OBO4G7o/Y9wIfc/XzgEuBWMzttpIrPlA4cQNe6ERHJMpwe/cXAJnff4u7dwMPA8qxlqoBno+nneue7e7e7d0Xt+cN8vGOSioJeH8aKiPQ3nOCdDuzMuL0rasu0GlgRTV8DFJvZZAAzm2Fma6J1fNPdd2c/gJndZGY1ZlZTV1d3tNsAhGfFAhqjFxHJMlI97FuAJWb2NrAEqAXSAO6+MxrSmQ/cYGYV2Xd29/vcvdrdq8vLy4+pgN6hGx11IyLS33CCvhaYkXG7Mmrr4+673X2Fu18AfD1qa8xeBngXWHxcFQ8hCMLfGroREelvOEH/BrDAzOaYWR5wHfB45gJmVmZmveu6Dbg/aq80s8JoeiJwGbB+pIrPlO4dulHOi4j0c8Sgd/cUcDPwFLAOeNTd15rZnWZ2dbTYFcB6M9sAVAB3Re1nAq+Z2WrgBeAed39nhLcBgJykcdU505hdNv5ErF5E5APLPOoJjxXV1dVeU1Mz2mWIiHygmNmb7l492LxYnRkrIiIDKehFRGJOQS8iEnMKehGRmFPQi4jEnIJeRCTmFPQiIjGnoBcRibkxd8KUmdUB249jFWXAgREqZySprqMzVuuCsVub6jo6Y7UuOLbaZrn7oFeFHHNBf7zMrGaos8NGk+o6OmO1Lhi7tamuozNW64KRr01DNyIiMaegFxGJuTgG/X2jXcAQVNfRGat1wditTXUdnbFaF4xwbbEboxcRkf7i2KMXEZEMCnoRkZiLTdCb2VIzW29mm8zs1lGsY4aZPWdm75nZWjP746j9DjOrNbNV0c8nRqm+bWb2TlRDTdQ2ycyeMbON0e+JJ7mm0zP2yyozazaz/zoa+8zM7jez/Wb2bkbboPvHQt+LXnNrzGzRSa7rW2b2fvTYPzez0qh9tpl1ZOy3fzhRdR2mtiGfOzO7Ldpn683st05yXY9k1LTNzFZF7Sdtnx0mI07c68zdP/A/QBLYDMwF8oDVQNUo1TINWBRNFwMbgCrgDuCWMbCvtgFlWW1/DdwaTd8KfHOUn8u9wKzR2GfA5cAi4N0j7R/gE8CTgAGXAq+d5Lo+DuRE09/MqGt25nKjtM8Gfe6iv4XVQD4wJ/q7TZ6surLm/w1w+8neZ4fJiBP2OotLj/5iYJO7b3H3buBhYPloFOLue9z9rWi6hfB7dqePRi1HYTnwo2j6R8CnR7GWjwKb3f14zo4+Zu7+IlCf1TzU/lkOPOChV4FSM5t2supy96c9/E5ngFeByhPx2EcyxD4bynLgYXfvcvetwCbCv9+TWpeZGfAZ4KET8diHc5iMOGGvs7gE/XRgZ8btXYyBcDWz2cAFwGtR083RW6/7T/bwSAYHnjazN83spqitwt33RNN7Cb/gfbRcR/8/vrGwz4baP2PpdfcHhL2+XnPM7G0ze8HMFo9STYM9d2Nlny0G9rn7xoy2k77PsjLihL3O4hL0Y46ZFQE/Bf6ruzcDfw/MA84H9hC+bRwNl7n7ImAZ8IdmdnnmTA/fK47KMbdmlgdcDfxr1DRW9lmf0dw/QzGzrwMp4CdR0x5gprtfAPwJ8C9mNuEklzXmnrss19O/Q3HS99kgGdFnpF9ncQn6WmBGxu3KqG1UmFku4RP4E3f/GYC773P3tLsHwA84QW9Xj8Tda6Pf+4GfR3Xs630rGP3ePxq1Ef7zecvd90U1jol9xtD7Z9Rfd2Z2I/BJ4PNROBANixyMpt8kHAdfeDLrOsxzNxb2WQ6wAnikt+1k77PBMoIT+DqLS9C/ASwwszlRr/A64PHRKCQa+/snYJ27fzujPXNM7Rrg3ez7noTaxptZce804Yd57xLuqxuixW4A/t/Jri3Sr5c1FvZZZKj98zjwe9FREZcCTRlvvU84M1sK/Clwtbu3Z7SXm1kymp4LLAC2nKy6oscd6rl7HLjOzPLNbE5U2+snszbgY8D77r6rt+Fk7rOhMoIT+To7GZ8yn4wfwk+mNxD+J/76KNZxGeFbrjXAqujnE8CPgXei9seBaaNQ21zCIx5WA2t79xMwGVgJbAT+E5g0CrWNBw4CJRltJ32fEf6j2QP0EI6FfnGo/UN4FMS90WvuHaD6JNe1iXDstvd19g/RstdGz+8q4C3gU6Owz4Z87oCvR/tsPbDsZNYVtf8Q+ErWsidtnx0mI07Y60yXQBARibm4DN2IiMgQFPQiIjGnoBcRiTkFvYhIzCnoRURiTkEvIhJzCnoRkZj7/wuT07+FQ7ozAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}