{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NSLKDD_LSTM_Model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "16_X2LMa1V7ENkqAtbATRAiCkL9sf9fXy",
      "authorship_tag": "ABX9TyM6mlVV16AZyMAFXyffybYu",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aminecloud9/PFE/blob/main/NSLKDD_LSTM_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zubbW2t0qkI4"
      },
      "source": [
        "%matplotlib inline\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "from __future__ import print_function\r\n",
        "import numpy as np\r\n",
        "np.random.seed(1337)  # for reproducibility\r\n",
        "from keras.preprocessing import sequence\r\n",
        "from keras.utils import np_utils\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.optimizers import SGD\r\n",
        "from keras.layers import Dense, Dropout, Activation, Embedding\r\n",
        "from keras.layers import LSTM, SimpleRNN, GRU\r\n",
        "from keras.datasets import imdb\r\n",
        "from keras.utils.np_utils import to_categorical\r\n",
        "from sklearn.metrics import (precision_score, recall_score,\r\n",
        "                             f1_score, accuracy_score,mean_squared_error,mean_absolute_error)\r\n",
        "from sklearn import metrics\r\n",
        "from sklearn.preprocessing import Normalizer\r\n",
        "import h5py\r\n",
        "from keras import callbacks\r\n",
        "from keras import callbacks\r\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGP9ArVErwIu"
      },
      "source": [
        "#data=pd.read_csv('/content/drive/MyDrive/CICIDS2017_multi_class_StandardScaler_NormalsationResults.csv')\r\n",
        "\r\n",
        "train_data=pd.read_csv('/content/drive/MyDrive/Train_42col_normalisation.csv')\r\n",
        "test_data=pd.read_csv('/content/drive/MyDrive/Test_42col_normalisation.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vbi5xXhgsu1K"
      },
      "source": [
        "X=train_data.values[:,1:-1]\r\n",
        "y=train_data.values[:,-1]\r\n",
        "train_X = X\r\n",
        "train_y = pd.factorize(y)\r\n",
        "test_X = test_data.values[:,1:-1]\r\n",
        "test_y = pd.factorize(test_data.values[:,-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfaE4iDWZJB9"
      },
      "source": [
        "test_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-4aCMAUrn3D"
      },
      "source": [
        "#Import Module\r\n",
        "#from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "\r\n",
        "# train_X, test_X, train_y, test_y = train_test_split(X, y, \r\n",
        "#                                                     train_size=0.7,\r\n",
        "#                                                     test_size=0.3,\r\n",
        "#                                                     random_state=122)\r\n",
        "# print(\"Labels for training and testing data\")\r\n",
        "# print(train_y)\r\n",
        "# print(test_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qa6201kxssMy",
        "outputId": "a79cd950-2280-4960-b63c-e8f6870f4d99"
      },
      "source": [
        "train_X.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(125973, 41)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgIXyYp5sgq-",
        "outputId": "20939bb1-a461-48e9-869a-a91b93924cf1"
      },
      "source": [
        "test_X.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(22543, 41)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7FOxrdJ2tye"
      },
      "source": [
        "# reshape input to be [samples, time steps, features]\r\n",
        "import keras\r\n",
        "\r\n",
        "#X_train = np.reshape(data.values[:,:-1].astype('float32'), (data.values[:,:-1].shape[0], 1, data.values[:,:-1].shape[1]))\r\n",
        "X_train = np.reshape(train_X.astype('float32'), (train_X.shape[0], 1, train_X.shape[1]))\r\n",
        "X_test = np.reshape(test_X.astype('float32'), (test_X.shape[0], 1, test_X.shape[1]))\r\n",
        "#y_train = keras.utils.to_categorical(pd.factorize(data.values[:,77])[0],15)\r\n",
        "y_train = keras.utils.to_categorical(train_y[0],5)\r\n",
        "y_train = y_train.astype('int')\r\n",
        "y_train = np.reshape(y_train,(y_train.shape[0],1,y_train.shape[1]))\r\n",
        "y_test = keras.utils.to_categorical(test_y[0],5)\r\n",
        "y_test = y_test.astype('int')\r\n",
        "y_test = np.reshape(y_test,(y_test.shape[0],1,y_test.shape[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhuIuz4p00I8"
      },
      "source": [
        "# 1. define the network\r\n",
        "import time\r\n",
        "data_dim = X_train.shape[1]\r\n",
        "timesteps = 1\r\n",
        "num_classes = y_train.shape[1]\r\n",
        "batch_size = 512\r\n",
        "\r\n",
        "\r\n",
        "model = Sequential()\r\n",
        "model.add(SimpleRNN(4, input_dim=X_train.shape[1]))  # try using a GRU instead, for fun\r\n",
        "model.add(Dropout(0.2))\r\n",
        "model.add(Dense(5))\r\n",
        "model.add(Activation('softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bmi9FeVpM7z8"
      },
      "source": [
        "import time\r\n",
        "data_dim = X_train.shape[1]\r\n",
        "timesteps = 1\r\n",
        "num_classes = y_train.shape[1]\r\n",
        "batch_size = 1\r\n",
        "\r\n",
        "model = Sequential()\r\n",
        "model.add(LSTM(50, return_sequences=True, stateful=False, batch_input_shape=(batch_size, timesteps, data_dim)))\r\n",
        "model.add(Dropout(0.1))\r\n",
        "model.add(LSTM(50, return_sequences=True, stateful=False))\r\n",
        "model.add(Dropout(0.1))\r\n",
        "model.add(LSTM(50, stateful=True))\r\n",
        "model.add(Dropout(0.1))\r\n",
        "model.add(Dense(15, activation='softmax'))\r\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "\r\n",
        "\r\n",
        "start = time.time()\r\n",
        "\r\n",
        "start_time = time.time()\r\n",
        "model.fit(X_train, y_train,batch_size=batch_size, epochs=2, shuffle=False)\r\n",
        "finish_time = time.time() - start_time\r\n",
        "model.save(\"/content/drive/My Drive/final_model_NSL-KDD_brute_Multiclass.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2X1EbkTFiow",
        "outputId": "a2303c51-b7bf-4890-8a25-063bea879b2e"
      },
      "source": [
        "import time\r\n",
        "#train_object_num=len(train_data)\r\n",
        "#print(test[0])\r\n",
        "#model training\r\n",
        "batch_size = 1024\r\n",
        "model=Sequential()\r\n",
        "model.add(LSTM(X_train.shape[1],input_dim=X_train.shape[2],return_sequences=True,kernel_initializer='uniform',activation='relu'))\r\n",
        "model.add(LSTM(32,kernel_initializer='uniform',return_sequences=True,activation='relu'))\r\n",
        "#model.add(LSTM(128,kernel_initializer='uniform',activation='relu'))\r\n",
        "model.add(Dense(256,activation='relu',r))\r\n",
        "model.add(Dense(128,activation='relu'))\r\n",
        "model.add(Dense(64,activation='relu'))\r\n",
        "model.add(Dense(16,activation='relu'))\r\n",
        "model.add(Dense(y_train.shape[2],activation='sigmoid'))\r\n",
        "model.compile(loss='binary_crossentropy',optimizer='NADAM',metrics=['accuracy'])\r\n",
        "model.summary()\r\n",
        "start_time = time.time()\r\n",
        "history = model.fit(X_train,y_train,validation_split=.3,epochs=100,batch_size=batch_size,verbose=1)\r\n",
        "finish_time = time.time() - start_time\r\n",
        "model.save(\"/content/drive/My Drive/model_NSL-KDD_42col_Multiclass_valsplit.3_bs1024_100ep.h5\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_12 (LSTM)               (None, None, 1)           72        \n",
            "_________________________________________________________________\n",
            "lstm_13 (LSTM)               (None, None, 32)          4352      \n",
            "_________________________________________________________________\n",
            "dense_30 (Dense)             (None, None, 256)         8448      \n",
            "_________________________________________________________________\n",
            "dense_31 (Dense)             (None, None, 128)         32896     \n",
            "_________________________________________________________________\n",
            "dense_32 (Dense)             (None, None, 64)          8256      \n",
            "_________________________________________________________________\n",
            "dense_33 (Dense)             (None, None, 16)          1040      \n",
            "_________________________________________________________________\n",
            "dense_34 (Dense)             (None, None, 5)           85        \n",
            "=================================================================\n",
            "Total params: 55,149\n",
            "Trainable params: 55,149\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "87/87 [==============================] - 6s 32ms/step - loss: 0.5757 - accuracy: 0.3883 - val_loss: 0.3503 - val_accuracy: 0.5351\n",
            "Epoch 2/100\n",
            "87/87 [==============================] - 2s 19ms/step - loss: 0.3216 - accuracy: 0.6109 - val_loss: 0.1609 - val_accuracy: 0.8643\n",
            "Epoch 3/100\n",
            "87/87 [==============================] - 2s 23ms/step - loss: 0.1597 - accuracy: 0.8662 - val_loss: 0.1546 - val_accuracy: 0.8655\n",
            "Epoch 4/100\n",
            "87/87 [==============================] - 2s 20ms/step - loss: 0.1527 - accuracy: 0.8666 - val_loss: 0.1331 - val_accuracy: 0.8632\n",
            "Epoch 5/100\n",
            "87/87 [==============================] - 2s 19ms/step - loss: 0.1209 - accuracy: 0.8653 - val_loss: 0.1028 - val_accuracy: 0.8612\n",
            "Epoch 6/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0956 - accuracy: 0.8794 - val_loss: 0.0994 - val_accuracy: 0.8822\n",
            "Epoch 7/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0902 - accuracy: 0.8976 - val_loss: 0.0880 - val_accuracy: 0.9125\n",
            "Epoch 8/100\n",
            "87/87 [==============================] - 2s 22ms/step - loss: 0.0842 - accuracy: 0.9092 - val_loss: 0.0824 - val_accuracy: 0.9141\n",
            "Epoch 9/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0824 - accuracy: 0.9132 - val_loss: 0.0819 - val_accuracy: 0.9140\n",
            "Epoch 10/100\n",
            "87/87 [==============================] - 2s 19ms/step - loss: 0.0813 - accuracy: 0.9129 - val_loss: 0.0802 - val_accuracy: 0.9150\n",
            "Epoch 11/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0789 - accuracy: 0.9151 - val_loss: 0.0860 - val_accuracy: 0.8664\n",
            "Epoch 12/100\n",
            "87/87 [==============================] - 2s 19ms/step - loss: 0.0803 - accuracy: 0.9103 - val_loss: 0.0797 - val_accuracy: 0.9162\n",
            "Epoch 13/100\n",
            "87/87 [==============================] - 2s 20ms/step - loss: 0.0782 - accuracy: 0.9163 - val_loss: 0.0811 - val_accuracy: 0.9117\n",
            "Epoch 14/100\n",
            "87/87 [==============================] - 2s 23ms/step - loss: 0.0777 - accuracy: 0.9151 - val_loss: 0.0785 - val_accuracy: 0.9162\n",
            "Epoch 15/100\n",
            "87/87 [==============================] - 2s 23ms/step - loss: 0.0771 - accuracy: 0.9158 - val_loss: 0.0851 - val_accuracy: 0.9148\n",
            "Epoch 16/100\n",
            "87/87 [==============================] - 2s 20ms/step - loss: 0.0778 - accuracy: 0.9153 - val_loss: 0.0777 - val_accuracy: 0.9154\n",
            "Epoch 17/100\n",
            "87/87 [==============================] - 2s 20ms/step - loss: 0.0766 - accuracy: 0.9161 - val_loss: 0.0768 - val_accuracy: 0.9160\n",
            "Epoch 18/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0773 - accuracy: 0.9157 - val_loss: 0.0760 - val_accuracy: 0.9167\n",
            "Epoch 19/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0768 - accuracy: 0.9146 - val_loss: 0.0756 - val_accuracy: 0.9171\n",
            "Epoch 20/100\n",
            "87/87 [==============================] - 2s 23ms/step - loss: 0.0762 - accuracy: 0.9157 - val_loss: 0.0782 - val_accuracy: 0.9150\n",
            "Epoch 21/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0768 - accuracy: 0.9148 - val_loss: 0.0755 - val_accuracy: 0.9163\n",
            "Epoch 22/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0755 - accuracy: 0.9157 - val_loss: 0.0750 - val_accuracy: 0.9181\n",
            "Epoch 23/100\n",
            "87/87 [==============================] - 2s 22ms/step - loss: 0.0755 - accuracy: 0.9168 - val_loss: 0.0756 - val_accuracy: 0.9155\n",
            "Epoch 24/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0756 - accuracy: 0.9158 - val_loss: 0.0749 - val_accuracy: 0.9163\n",
            "Epoch 25/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0750 - accuracy: 0.9164 - val_loss: 0.0744 - val_accuracy: 0.9166\n",
            "Epoch 26/100\n",
            "87/87 [==============================] - 2s 23ms/step - loss: 0.0749 - accuracy: 0.9165 - val_loss: 0.0752 - val_accuracy: 0.9189\n",
            "Epoch 27/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0735 - accuracy: 0.9188 - val_loss: 0.0751 - val_accuracy: 0.9167\n",
            "Epoch 28/100\n",
            "87/87 [==============================] - 2s 23ms/step - loss: 0.0750 - accuracy: 0.9163 - val_loss: 0.0770 - val_accuracy: 0.9170\n",
            "Epoch 29/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0741 - accuracy: 0.9178 - val_loss: 0.0754 - val_accuracy: 0.9174\n",
            "Epoch 30/100\n",
            "87/87 [==============================] - 2s 20ms/step - loss: 0.0746 - accuracy: 0.9167 - val_loss: 0.0750 - val_accuracy: 0.9172\n",
            "Epoch 31/100\n",
            "87/87 [==============================] - 2s 22ms/step - loss: 0.0738 - accuracy: 0.9164 - val_loss: 0.0749 - val_accuracy: 0.9198\n",
            "Epoch 32/100\n",
            "87/87 [==============================] - 2s 20ms/step - loss: 0.0749 - accuracy: 0.9179 - val_loss: 0.0738 - val_accuracy: 0.9173\n",
            "Epoch 33/100\n",
            "87/87 [==============================] - 2s 20ms/step - loss: 0.0733 - accuracy: 0.9189 - val_loss: 0.0736 - val_accuracy: 0.9174\n",
            "Epoch 34/100\n",
            "87/87 [==============================] - 2s 23ms/step - loss: 0.0758 - accuracy: 0.9154 - val_loss: 0.0771 - val_accuracy: 0.9182\n",
            "Epoch 35/100\n",
            "87/87 [==============================] - 2s 23ms/step - loss: 0.0731 - accuracy: 0.9201 - val_loss: 0.0733 - val_accuracy: 0.9182\n",
            "Epoch 36/100\n",
            "87/87 [==============================] - 2s 22ms/step - loss: 0.0732 - accuracy: 0.9176 - val_loss: 0.0736 - val_accuracy: 0.9169\n",
            "Epoch 37/100\n",
            "87/87 [==============================] - 2s 22ms/step - loss: 0.0734 - accuracy: 0.9180 - val_loss: 0.0751 - val_accuracy: 0.9181\n",
            "Epoch 38/100\n",
            "87/87 [==============================] - 2s 23ms/step - loss: 0.0738 - accuracy: 0.9187 - val_loss: 0.0768 - val_accuracy: 0.9151\n",
            "Epoch 39/100\n",
            "87/87 [==============================] - 2s 23ms/step - loss: 0.0731 - accuracy: 0.9181 - val_loss: 0.0751 - val_accuracy: 0.9178\n",
            "Epoch 40/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0726 - accuracy: 0.9187 - val_loss: 0.0735 - val_accuracy: 0.9182\n",
            "Epoch 41/100\n",
            "87/87 [==============================] - 2s 19ms/step - loss: 0.0725 - accuracy: 0.9194 - val_loss: 0.0735 - val_accuracy: 0.9185\n",
            "Epoch 42/100\n",
            "87/87 [==============================] - 2s 20ms/step - loss: 0.0736 - accuracy: 0.9172 - val_loss: 0.0760 - val_accuracy: 0.9184\n",
            "Epoch 43/100\n",
            "87/87 [==============================] - 2s 19ms/step - loss: 0.0726 - accuracy: 0.9190 - val_loss: 0.0732 - val_accuracy: 0.9187\n",
            "Epoch 44/100\n",
            "87/87 [==============================] - 2s 22ms/step - loss: 0.0722 - accuracy: 0.9197 - val_loss: 0.0736 - val_accuracy: 0.9182\n",
            "Epoch 45/100\n",
            "87/87 [==============================] - 2s 20ms/step - loss: 0.0726 - accuracy: 0.9195 - val_loss: 0.0761 - val_accuracy: 0.9189\n",
            "Epoch 46/100\n",
            "87/87 [==============================] - 2s 20ms/step - loss: 0.0730 - accuracy: 0.9181 - val_loss: 0.0763 - val_accuracy: 0.9197\n",
            "Epoch 47/100\n",
            "87/87 [==============================] - 2s 20ms/step - loss: 0.0725 - accuracy: 0.9203 - val_loss: 0.0761 - val_accuracy: 0.9164\n",
            "Epoch 48/100\n",
            "87/87 [==============================] - 2s 22ms/step - loss: 0.0732 - accuracy: 0.9185 - val_loss: 0.0727 - val_accuracy: 0.9188\n",
            "Epoch 49/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0733 - accuracy: 0.9187 - val_loss: 0.0733 - val_accuracy: 0.9182\n",
            "Epoch 50/100\n",
            "87/87 [==============================] - 2s 20ms/step - loss: 0.0727 - accuracy: 0.9197 - val_loss: 0.0750 - val_accuracy: 0.9189\n",
            "Epoch 51/100\n",
            "87/87 [==============================] - 2s 20ms/step - loss: 0.0723 - accuracy: 0.9194 - val_loss: 0.0734 - val_accuracy: 0.9187\n",
            "Epoch 52/100\n",
            "87/87 [==============================] - 2s 20ms/step - loss: 0.0732 - accuracy: 0.9187 - val_loss: 0.0735 - val_accuracy: 0.9195\n",
            "Epoch 53/100\n",
            "87/87 [==============================] - 2s 20ms/step - loss: 0.0733 - accuracy: 0.9179 - val_loss: 0.0757 - val_accuracy: 0.9160\n",
            "Epoch 54/100\n",
            "87/87 [==============================] - 2s 19ms/step - loss: 0.0728 - accuracy: 0.9183 - val_loss: 0.0731 - val_accuracy: 0.9194\n",
            "Epoch 55/100\n",
            "87/87 [==============================] - 2s 20ms/step - loss: 0.0721 - accuracy: 0.9192 - val_loss: 0.0725 - val_accuracy: 0.9191\n",
            "Epoch 56/100\n",
            "87/87 [==============================] - 2s 20ms/step - loss: 0.0720 - accuracy: 0.9193 - val_loss: 0.0723 - val_accuracy: 0.9193\n",
            "Epoch 57/100\n",
            "87/87 [==============================] - 2s 24ms/step - loss: 0.0738 - accuracy: 0.9183 - val_loss: 0.0731 - val_accuracy: 0.9191\n",
            "Epoch 58/100\n",
            "87/87 [==============================] - 2s 23ms/step - loss: 0.0727 - accuracy: 0.9199 - val_loss: 0.0727 - val_accuracy: 0.9198\n",
            "Epoch 59/100\n",
            "87/87 [==============================] - 2s 19ms/step - loss: 0.0732 - accuracy: 0.9183 - val_loss: 0.0727 - val_accuracy: 0.9191\n",
            "Epoch 60/100\n",
            "87/87 [==============================] - 2s 19ms/step - loss: 0.0726 - accuracy: 0.9188 - val_loss: 0.0723 - val_accuracy: 0.9191\n",
            "Epoch 61/100\n",
            "87/87 [==============================] - 2s 20ms/step - loss: 0.0723 - accuracy: 0.9193 - val_loss: 0.0727 - val_accuracy: 0.9189\n",
            "Epoch 62/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0730 - accuracy: 0.9179 - val_loss: 0.0723 - val_accuracy: 0.9198\n",
            "Epoch 63/100\n",
            "87/87 [==============================] - 2s 23ms/step - loss: 0.0725 - accuracy: 0.9191 - val_loss: 0.0723 - val_accuracy: 0.9188\n",
            "Epoch 64/100\n",
            "87/87 [==============================] - 2s 23ms/step - loss: 0.0719 - accuracy: 0.9190 - val_loss: 0.0719 - val_accuracy: 0.9202\n",
            "Epoch 65/100\n",
            "87/87 [==============================] - 2s 22ms/step - loss: 0.0720 - accuracy: 0.9195 - val_loss: 0.0722 - val_accuracy: 0.9203\n",
            "Epoch 66/100\n",
            "87/87 [==============================] - 2s 20ms/step - loss: 0.0725 - accuracy: 0.9197 - val_loss: 0.0720 - val_accuracy: 0.9203\n",
            "Epoch 67/100\n",
            "87/87 [==============================] - 2s 20ms/step - loss: 0.0710 - accuracy: 0.9210 - val_loss: 0.0718 - val_accuracy: 0.9202\n",
            "Epoch 68/100\n",
            "87/87 [==============================] - 2s 23ms/step - loss: 0.0724 - accuracy: 0.9190 - val_loss: 0.0723 - val_accuracy: 0.9197\n",
            "Epoch 69/100\n",
            "87/87 [==============================] - 2s 23ms/step - loss: 0.0721 - accuracy: 0.9189 - val_loss: 0.0718 - val_accuracy: 0.9205\n",
            "Epoch 70/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0716 - accuracy: 0.9207 - val_loss: 0.0720 - val_accuracy: 0.9206\n",
            "Epoch 71/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0715 - accuracy: 0.9204 - val_loss: 0.0724 - val_accuracy: 0.9204\n",
            "Epoch 72/100\n",
            "87/87 [==============================] - 2s 22ms/step - loss: 0.0717 - accuracy: 0.9195 - val_loss: 0.0723 - val_accuracy: 0.9205\n",
            "Epoch 73/100\n",
            "87/87 [==============================] - 2s 22ms/step - loss: 0.0723 - accuracy: 0.9194 - val_loss: 0.0725 - val_accuracy: 0.9204\n",
            "Epoch 74/100\n",
            "87/87 [==============================] - 2s 23ms/step - loss: 0.0720 - accuracy: 0.9207 - val_loss: 0.0768 - val_accuracy: 0.8694\n",
            "Epoch 75/100\n",
            "87/87 [==============================] - 2s 24ms/step - loss: 0.0733 - accuracy: 0.9147 - val_loss: 0.0719 - val_accuracy: 0.9207\n",
            "Epoch 76/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0713 - accuracy: 0.9201 - val_loss: 0.0717 - val_accuracy: 0.9202\n",
            "Epoch 77/100\n",
            "87/87 [==============================] - 2s 19ms/step - loss: 0.0710 - accuracy: 0.9214 - val_loss: 0.0719 - val_accuracy: 0.9203\n",
            "Epoch 78/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0712 - accuracy: 0.9200 - val_loss: 0.0728 - val_accuracy: 0.9204\n",
            "Epoch 79/100\n",
            "87/87 [==============================] - 2s 22ms/step - loss: 0.0714 - accuracy: 0.9205 - val_loss: 0.0730 - val_accuracy: 0.9207\n",
            "Epoch 80/100\n",
            "87/87 [==============================] - 2s 20ms/step - loss: 0.0720 - accuracy: 0.9197 - val_loss: 0.0716 - val_accuracy: 0.9205\n",
            "Epoch 81/100\n",
            "87/87 [==============================] - 2s 19ms/step - loss: 0.0720 - accuracy: 0.9193 - val_loss: 0.0729 - val_accuracy: 0.9199\n",
            "Epoch 82/100\n",
            "87/87 [==============================] - 2s 20ms/step - loss: 0.0712 - accuracy: 0.9211 - val_loss: 0.0720 - val_accuracy: 0.9200\n",
            "Epoch 83/100\n",
            "87/87 [==============================] - 2s 20ms/step - loss: 0.0716 - accuracy: 0.9204 - val_loss: 0.0717 - val_accuracy: 0.9204\n",
            "Epoch 84/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0711 - accuracy: 0.9206 - val_loss: 0.0718 - val_accuracy: 0.9208\n",
            "Epoch 85/100\n",
            "87/87 [==============================] - 2s 20ms/step - loss: 0.0705 - accuracy: 0.9209 - val_loss: 0.0788 - val_accuracy: 0.9210\n",
            "Epoch 86/100\n",
            "87/87 [==============================] - 2s 20ms/step - loss: 0.0724 - accuracy: 0.9209 - val_loss: 0.0723 - val_accuracy: 0.9208\n",
            "Epoch 87/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0712 - accuracy: 0.9196 - val_loss: 0.0718 - val_accuracy: 0.9210\n",
            "Epoch 88/100\n",
            "87/87 [==============================] - 2s 22ms/step - loss: 0.0713 - accuracy: 0.9209 - val_loss: 0.0716 - val_accuracy: 0.9204\n",
            "Epoch 89/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0719 - accuracy: 0.9197 - val_loss: 0.0712 - val_accuracy: 0.9213\n",
            "Epoch 90/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0727 - accuracy: 0.9181 - val_loss: 0.0714 - val_accuracy: 0.9209\n",
            "Epoch 91/100\n",
            "87/87 [==============================] - 2s 22ms/step - loss: 0.0702 - accuracy: 0.9213 - val_loss: 0.0714 - val_accuracy: 0.9211\n",
            "Epoch 92/100\n",
            "87/87 [==============================] - 2s 22ms/step - loss: 0.0707 - accuracy: 0.9202 - val_loss: 0.0738 - val_accuracy: 0.9174\n",
            "Epoch 93/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0708 - accuracy: 0.9198 - val_loss: 0.0722 - val_accuracy: 0.9213\n",
            "Epoch 94/100\n",
            "87/87 [==============================] - 2s 20ms/step - loss: 0.0711 - accuracy: 0.9203 - val_loss: 0.0711 - val_accuracy: 0.9208\n",
            "Epoch 95/100\n",
            "87/87 [==============================] - 2s 20ms/step - loss: 0.0716 - accuracy: 0.9195 - val_loss: 0.0718 - val_accuracy: 0.9213\n",
            "Epoch 96/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0711 - accuracy: 0.9214 - val_loss: 0.0714 - val_accuracy: 0.9209\n",
            "Epoch 97/100\n",
            "87/87 [==============================] - 2s 19ms/step - loss: 0.0724 - accuracy: 0.9200 - val_loss: 0.0713 - val_accuracy: 0.9207\n",
            "Epoch 98/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0709 - accuracy: 0.9221 - val_loss: 0.0712 - val_accuracy: 0.9213\n",
            "Epoch 99/100\n",
            "87/87 [==============================] - 2s 22ms/step - loss: 0.0703 - accuracy: 0.9218 - val_loss: 0.0715 - val_accuracy: 0.9209\n",
            "Epoch 100/100\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.0713 - accuracy: 0.9203 - val_loss: 0.0713 - val_accuracy: 0.9214\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgFh6eY-RogJ",
        "outputId": "6b00474b-d40d-4194-dc70-e71eb8a0075a"
      },
      "source": [
        "from sklearn.metrics import classification_report\r\n",
        "from sklearn.metrics import precision_recall_fscore_support\r\n",
        "loss, accuracy = model.evaluate(X_test,y_test)\r\n",
        "print(\"\\nLoss: %.2f, Accuracy: %.2f%%\" % (loss, accuracy*100))\r\n",
        "y_pred = model.predict_classes(X_test)\r\n",
        "target_names = ['Normal', 'Dos', 'Probe','R2L', 'U2R']\r\n",
        "print(classification_report(y_true = np.transpose(test_y[0]), y_pred = y_pred))\r\n",
        "#print(accuracy_score(y_true = np.transpose(test_y[0]), y_pred = y_pred))\r\n",
        "print(\"execution time :  \",finish_time)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "705/705 [==============================] - 1s 1ms/step - loss: 0.8132 - accuracy: 0.4852\n",
            "\n",
            "Loss: 0.81, Accuracy: 48.52%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.46      0.21      0.29      7458\n",
            "           1       0.55      0.87      0.68      9710\n",
            "           2       0.00      0.00      0.00      2421\n",
            "           3       0.24      0.34      0.29      2754\n",
            "           4       0.00      0.00      0.00       200\n",
            "\n",
            "    accuracy                           0.49     22543\n",
            "   macro avg       0.25      0.28      0.25     22543\n",
            "weighted avg       0.42      0.49      0.42     22543\n",
            "\n",
            "execution time :   187.6243758201599\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBLLxMkV25MY",
        "outputId": "1cd20db3-6f7d-45b6-e3d5-521895213215"
      },
      "source": [
        "model.metrics_names"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['loss', 'accuracy']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqrg8ftGdlZd",
        "outputId": "aace09a7-f2e0-47bb-adba-465e6711a6c6"
      },
      "source": [
        "test_y.sh"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 1, ..., 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPW7sxx-yiwT",
        "outputId": "72de27e0-0eaa-46a9-ccfa-5e71a66065a6"
      },
      "source": [
        "s= np.transpose(test_y[0])\r\n",
        "s.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(22543,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c98fMqo4yf5b",
        "outputId": "82a77c16-52b0-4d62-b7c4-3bc1116832bb"
      },
      "source": [
        "y_pred.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(22543, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vn7ng4MLMOb"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix,  roc_auc_score, roc_curve\r\n",
        "import sklearn.metrics as metrics\r\n",
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\r\n",
        "score = model.evaluate(X_test, y_test, batch_size)\r\n",
        "y_pred = model.predict(X_test, batch_size= batch_size)\r\n",
        "y_pred = y_pred.astype('int')\r\n",
        "#y_pred = [np.round(x) for x in y_pred]\r\n",
        "#y_pred = np.array(y_pred)\r\n",
        "y2 = np.array(test_y[0])\r\n",
        "\r\n",
        "#print(y2)\r\n",
        "print(classification_report(y_test,y_pred,\"multiclass\"))\r\n",
        "cm = confusion_matrix(y2.argmax(axis=1), y_pred.argmax(axis=1))\r\n",
        "print(cm)\r\n",
        "\r\n",
        "\r\n",
        "probs = model.predict_proba(X_test,batch_size=batch_size)\r\n",
        "#y_preds = np.transpose([pred[:, 1] for pred in probs])\r\n",
        "#y_preds=y_preds.reshape(1,-1)\r\n",
        "fpr, tpr, threshold = metrics.roc_auc_score(y_score==np.transpose(y_test),y_true= np.transpose(probs),multi_class='ovo')\r\n",
        "roc_auc = metrics.auc(fpr, tpr)\r\n",
        "\r\n",
        "# method I: plt\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "plt.title('Receiver Operating Characteristic')\r\n",
        "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\r\n",
        "plt.legend(loc = 'lower right')\r\n",
        "plt.plot([0, 1], [0, 1],'r--')\r\n",
        "plt.xlim([0, 1])\r\n",
        "plt.ylim([0, 1])\r\n",
        "plt.ylabel('True Positive Rate')\r\n",
        "plt.xlabel('False Positive Rate')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5ojJxnFa2aF",
        "outputId": "28466071-306a-4e2b-cba4-bbd039f3f64f"
      },
      "source": [
        "y2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 1, ..., 0, 1, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bq6QuCZu2BgD"
      },
      "source": [
        "# try using different optimizers and different optimizer configs\r\n",
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\r\n",
        "#checkpointer = callbacks.ModelCheckpoint(filepath=\"kddresults/lstm1layer/checkpoint-{epoch:02d}.hdf5\", verbose=1, save_best_only=True, monitor='val_acc',mode='max')\r\n",
        "#csv_logger = CSVLogger('training_set.csv',separator=',', append=False)\r\n",
        "start_time = time.time()\r\n",
        "model.fit(X_train, y_train, batch_size=batch_size, epochs=2, validation_split=.2)#,callbacks=[checkpointer,csv_logger]\r\n",
        "finish_time = time.time() - start_time\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieGpV2zYXZwV"
      },
      "source": [
        "#saving model\r\n",
        "model.save(\"/content/drive/MyDrive/NSL-KDD_results_3LSTM_1Dense_20epoch_1batch_model.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xLvXBh0SyWh",
        "outputId": "5f8c4d80-d3a3-4c10-c1d7-aaf4d59dddb8"
      },
      "source": [
        "from keras.models import load_model\r\n",
        "#model = load_model('/content/drive/MyDrive/NSL-KDD_results_SimpleRNN_model.hdf5')\r\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\r\n",
        "print(\"\\nLoss: %.2f, Accuracy: %.2f%%\" % (loss, accuracy*100))\r\n",
        "\r\n",
        "#print(\"--- %s seconds ---\",finish_time)\r\n",
        "y_pred = model.predict_classes(X_train)\r\n",
        "#np.savetxt('/content/drive/MyDrive/NSL-KDD_results_SimpleRNN_predicted.txt', np.transpose([y_test,y_pred]), fmt='%s')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "705/705 [==============================] - 4s 5ms/step - loss: 4.9965 - accuracy: 0.0768\n",
            "\n",
            "Loss: 5.00, Accuracy: 7.68%\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 41) for input KerasTensor(type_spec=TensorSpec(shape=(None, 41), dtype=tf.float32, name='dense_35_input'), name='dense_35_input', description=\"created by layer 'dense_35_input'\"), but it was called on an input with incompatible shape (None, 1, 41).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRdRVMFrQSrR",
        "outputId": "d2a8581b-03d8-4ceb-f4ef-b734c10842eb"
      },
      "source": [
        "print(y_train)\r\n",
        "y_pred"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[1 0 0 0 0]]\n",
            "\n",
            " [[1 0 0 0 0]]\n",
            "\n",
            " [[0 1 0 0 0]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[1 0 0 0 0]]\n",
            "\n",
            " [[0 1 0 0 0]]\n",
            "\n",
            " [[1 0 0 0 0]]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0],\n",
              "       [0],\n",
              "       [1],\n",
              "       ...,\n",
              "       [0],\n",
              "       [1],\n",
              "       [0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqAFEhU4_CMH",
        "outputId": "dc7ca7e5-50c4-4595-ba53-6ba7104f0767"
      },
      "source": [
        "42 col pca 80% : b_s=256 epochs= 300 Lstm(32,32)->Dense(256,128,64,ytrain.shape(2))   ->   accuracy = 62.8%\r\n",
        "\r\n",
        "\r\n",
        "24 col normalisé : b_s=1024 epochs= 100 Lstm(32,32)->Dense(256,128,64,16,ytrain.shape(2))   ->   accuracy = 83.73%\r\n",
        "24 col pca 80% : b_s=1024 epochs= 100 Lstm(32,32)->Dense(256,128,64,16,ytrain.shape(2))   ->   accuracy = 40.28%\r\n",
        "42 col normalisé : b_s=1024 epochs= 100 Lstm(32,32)->Dense(256,128,64,16,ytrain.shape(2))   ->   accuracy = 48.52%\r\n",
        "42 col pca 80% : b_s=1024 epochs= 100 Lstm(32,32)->Dense(256,128,64,16,ytrain.shape(2))   ->   accuracy = 45.22%\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<tensorflow.python.keras.engine.sequential.Sequential object at 0x7f324f860ba8>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZpW4B3GT5_ac",
        "outputId": "fb3d66bb-ec6d-45b3-d3f4-e6c73d38cc97"
      },
      "source": [
        "batch_size= 1024\r\n",
        "lrate = 0.1\r\n",
        "model = Sequential()\r\n",
        "model.add(Dense(X_train.shape[1], input_dim=X_train.shape[2], activation='relu', kernel_initializer='he_uniform'))\r\n",
        "model.add(Dense(1024, input_dim=X_train.shape[2], activation='relu', kernel_initializer='he_uniform'))\r\n",
        "model.add(Dropout(0.1))\r\n",
        "model.add(Dense(768, input_dim=X_train.shape[2], activation='relu', kernel_initializer='he_uniform'))\r\n",
        "model.add(Dropout(0.1))\r\n",
        "model.add(Dense(512, input_dim=X_train.shape[2], activation='relu', kernel_initializer='he_uniform'))\r\n",
        "model.add(Dropout(0.1))\r\n",
        "model.add(Dense(256, input_dim=X_train.shape[2], activation='relu', kernel_initializer='he_uniform'))\r\n",
        "model.add(Dropout(0.1))\r\n",
        "model.add(Dense(128, input_dim=X_train.shape[2], activation='relu', kernel_initializer='he_uniform'))\r\n",
        "model.add(Dropout(0.1))\r\n",
        "model.add(Dense(y_train.shape[2], activation='softmax'))\r\n",
        "\t# compile model\r\n",
        "opt = SGD(lr=lrate)\r\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\r\n",
        "# fit model\r\n",
        "history = model.fit(X_train, y_train, validation_split=.3,batch_size=batch_size, epochs=200, verbose=1)\r\n",
        "# plot learning curves\r\n",
        "pyplot.plot(history.history['accuracy'], label='train')\r\n",
        "pyplot.plot(history.history['val_accuracy'], label='test')\r\n",
        "pyplot.title('lrate='+str(lrate), pad=-50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 41) for input KerasTensor(type_spec=TensorSpec(shape=(None, 41), dtype=tf.float32, name='dense_35_input'), name='dense_35_input', description=\"created by layer 'dense_35_input'\"), but it was called on an input with incompatible shape (None, 1, 41).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 41) for input KerasTensor(type_spec=TensorSpec(shape=(None, 41), dtype=tf.float32, name='dense_35_input'), name='dense_35_input', description=\"created by layer 'dense_35_input'\"), but it was called on an input with incompatible shape (None, 1, 41).\n",
            "86/87 [============================>.] - ETA: 0s - loss: 0.7423 - accuracy: 0.8581WARNING:tensorflow:Model was constructed with shape (None, 41) for input KerasTensor(type_spec=TensorSpec(shape=(None, 41), dtype=tf.float32, name='dense_35_input'), name='dense_35_input', description=\"created by layer 'dense_35_input'\"), but it was called on an input with incompatible shape (None, 1, 41).\n",
            "87/87 [==============================] - 19s 213ms/step - loss: 0.7370 - accuracy: 0.8585 - val_loss: 0.3572 - val_accuracy: 0.8837\n",
            "Epoch 2/200\n",
            "87/87 [==============================] - 18s 209ms/step - loss: 0.3681 - accuracy: 0.8877 - val_loss: 0.3436 - val_accuracy: 0.8844\n",
            "Epoch 3/200\n",
            "87/87 [==============================] - 18s 209ms/step - loss: 0.3022 - accuracy: 0.9119 - val_loss: 0.2939 - val_accuracy: 0.9272\n",
            "Epoch 4/200\n",
            "87/87 [==============================] - 18s 209ms/step - loss: 0.2836 - accuracy: 0.9293 - val_loss: 0.3055 - val_accuracy: 0.9267\n",
            "Epoch 5/200\n",
            "87/87 [==============================] - 18s 209ms/step - loss: 0.2827 - accuracy: 0.9313 - val_loss: 0.2542 - val_accuracy: 0.9473\n",
            "Epoch 6/200\n",
            "87/87 [==============================] - 18s 210ms/step - loss: 0.2582 - accuracy: 0.9420 - val_loss: 0.2374 - val_accuracy: 0.9488\n",
            "Epoch 7/200\n",
            "87/87 [==============================] - 18s 208ms/step - loss: 0.2588 - accuracy: 0.9400 - val_loss: 0.2476 - val_accuracy: 0.9464\n",
            "Epoch 8/200\n",
            "87/87 [==============================] - 18s 209ms/step - loss: 0.2555 - accuracy: 0.9400 - val_loss: 0.3310 - val_accuracy: 0.8963\n",
            "Epoch 9/200\n",
            "87/87 [==============================] - 18s 209ms/step - loss: 0.2528 - accuracy: 0.9415 - val_loss: 0.2247 - val_accuracy: 0.9501\n",
            "Epoch 10/200\n",
            "87/87 [==============================] - 18s 210ms/step - loss: 0.2695 - accuracy: 0.9371 - val_loss: 0.2207 - val_accuracy: 0.9505\n",
            "Epoch 11/200\n",
            "87/87 [==============================] - 18s 208ms/step - loss: 0.2311 - accuracy: 0.9490 - val_loss: 0.4212 - val_accuracy: 0.8803\n",
            "Epoch 12/200\n",
            "87/87 [==============================] - 18s 209ms/step - loss: 0.2685 - accuracy: 0.9375 - val_loss: 0.2170 - val_accuracy: 0.9515\n",
            "Epoch 13/200\n",
            "87/87 [==============================] - 18s 209ms/step - loss: 0.2264 - accuracy: 0.9506 - val_loss: 0.2196 - val_accuracy: 0.9513\n",
            "Epoch 14/200\n",
            "87/87 [==============================] - 18s 209ms/step - loss: 0.2262 - accuracy: 0.9500 - val_loss: 0.2161 - val_accuracy: 0.9503\n",
            "Epoch 15/200\n",
            "87/87 [==============================] - 21s 238ms/step - loss: 0.2222 - accuracy: 0.9508 - val_loss: 0.2177 - val_accuracy: 0.9522\n",
            "Epoch 16/200\n",
            "87/87 [==============================] - 18s 209ms/step - loss: 0.2222 - accuracy: 0.9512 - val_loss: 0.3294 - val_accuracy: 0.8923\n",
            "Epoch 17/200\n",
            "87/87 [==============================] - 18s 208ms/step - loss: 0.2303 - accuracy: 0.9480 - val_loss: 0.2198 - val_accuracy: 0.9513\n",
            "Epoch 18/200\n",
            "87/87 [==============================] - 18s 209ms/step - loss: 0.2245 - accuracy: 0.9500 - val_loss: 0.2200 - val_accuracy: 0.9510\n",
            "Epoch 19/200\n",
            "87/87 [==============================] - 18s 209ms/step - loss: 0.2257 - accuracy: 0.9496 - val_loss: 0.2113 - val_accuracy: 0.9516\n",
            "Epoch 20/200\n",
            "87/87 [==============================] - 18s 209ms/step - loss: 0.2236 - accuracy: 0.9504 - val_loss: 0.2819 - val_accuracy: 0.9320\n",
            "Epoch 21/200\n",
            "87/87 [==============================] - 18s 209ms/step - loss: 0.2268 - accuracy: 0.9479 - val_loss: 0.6602 - val_accuracy: 0.8775\n",
            "Epoch 22/200\n",
            "87/87 [==============================] - 18s 209ms/step - loss: 0.3859 - accuracy: 0.9184 - val_loss: 0.2561 - val_accuracy: 0.9397\n",
            "Epoch 23/200\n",
            "87/87 [==============================] - 18s 210ms/step - loss: 0.2288 - accuracy: 0.9487 - val_loss: 0.2637 - val_accuracy: 0.9312\n",
            "Epoch 24/200\n",
            "87/87 [==============================] - 18s 209ms/step - loss: 0.2267 - accuracy: 0.9499 - val_loss: 0.2103 - val_accuracy: 0.9529\n",
            "Epoch 25/200\n",
            "87/87 [==============================] - 18s 209ms/step - loss: 0.2170 - accuracy: 0.9519 - val_loss: 0.2149 - val_accuracy: 0.9503\n",
            "Epoch 26/200\n",
            "87/87 [==============================] - 18s 210ms/step - loss: 0.2183 - accuracy: 0.9511 - val_loss: 0.2336 - val_accuracy: 0.9466\n",
            "Epoch 27/200\n",
            "87/87 [==============================] - 18s 211ms/step - loss: 0.2152 - accuracy: 0.9524 - val_loss: 0.2129 - val_accuracy: 0.9519\n",
            "Epoch 28/200\n",
            "87/87 [==============================] - 18s 209ms/step - loss: 0.2204 - accuracy: 0.9504 - val_loss: 0.2444 - val_accuracy: 0.9437\n",
            "Epoch 29/200\n",
            "87/87 [==============================] - 18s 209ms/step - loss: 0.2182 - accuracy: 0.9514 - val_loss: 0.2323 - val_accuracy: 0.9459\n",
            "Epoch 30/200\n",
            "87/87 [==============================] - 18s 209ms/step - loss: 0.2193 - accuracy: 0.9515 - val_loss: 0.2125 - val_accuracy: 0.9514\n",
            "Epoch 31/200\n",
            "87/87 [==============================] - 18s 209ms/step - loss: 0.2197 - accuracy: 0.9505 - val_loss: 0.2088 - val_accuracy: 0.9525\n",
            "Epoch 32/200\n",
            "87/87 [==============================] - 18s 210ms/step - loss: 0.2147 - accuracy: 0.9529 - val_loss: 0.2184 - val_accuracy: 0.9495\n",
            "Epoch 33/200\n",
            "87/87 [==============================] - 18s 210ms/step - loss: 0.2167 - accuracy: 0.9521 - val_loss: 0.2315 - val_accuracy: 0.9450\n",
            "Epoch 34/200\n",
            "87/87 [==============================] - 18s 209ms/step - loss: 0.2184 - accuracy: 0.9519 - val_loss: 0.2045 - val_accuracy: 0.9537\n",
            "Epoch 35/200\n",
            "87/87 [==============================] - 18s 209ms/step - loss: 0.2151 - accuracy: 0.9528 - val_loss: 0.2127 - val_accuracy: 0.9508\n",
            "Epoch 36/200\n",
            "87/87 [==============================] - 18s 208ms/step - loss: 0.2145 - accuracy: 0.9523 - val_loss: 0.2033 - val_accuracy: 0.9531\n",
            "Epoch 37/200\n",
            "87/87 [==============================] - 18s 209ms/step - loss: 0.2177 - accuracy: 0.9511 - val_loss: 0.2156 - val_accuracy: 0.9503\n",
            "Epoch 38/200\n",
            "87/87 [==============================] - 18s 209ms/step - loss: 0.2194 - accuracy: 0.9516 - val_loss: 0.2691 - val_accuracy: 0.9330\n",
            "Epoch 39/200\n",
            "87/87 [==============================] - 18s 210ms/step - loss: 0.2170 - accuracy: 0.9500 - val_loss: 0.2707 - val_accuracy: 0.9179\n",
            "Epoch 40/200\n",
            "87/87 [==============================] - 18s 209ms/step - loss: 0.2179 - accuracy: 0.9502 - val_loss: 0.2056 - val_accuracy: 0.9536\n",
            "Epoch 41/200\n",
            "87/87 [==============================] - 18s 209ms/step - loss: 0.2216 - accuracy: 0.9496 - val_loss: 0.2063 - val_accuracy: 0.9535\n",
            "Epoch 42/200\n",
            "87/87 [==============================] - 18s 209ms/step - loss: 0.2139 - accuracy: 0.9530 - val_loss: 0.2396 - val_accuracy: 0.9479\n",
            "Epoch 43/200\n",
            "87/87 [==============================] - 18s 209ms/step - loss: 0.2148 - accuracy: 0.9524 - val_loss: 0.2068 - val_accuracy: 0.9520\n",
            "Epoch 44/200\n",
            "87/87 [==============================] - 18s 212ms/step - loss: 0.2131 - accuracy: 0.9526 - val_loss: 0.2103 - val_accuracy: 0.9519\n",
            "Epoch 45/200\n",
            "87/87 [==============================] - 18s 210ms/step - loss: 0.2170 - accuracy: 0.9521 - val_loss: 0.2079 - val_accuracy: 0.9535\n",
            "Epoch 46/200\n",
            "87/87 [==============================] - 18s 209ms/step - loss: 0.2112 - accuracy: 0.9533 - val_loss: 0.2274 - val_accuracy: 0.9474\n",
            "Epoch 47/200\n",
            "87/87 [==============================] - 18s 209ms/step - loss: 0.2189 - accuracy: 0.9514 - val_loss: 0.2134 - val_accuracy: 0.9530\n",
            "Epoch 48/200\n",
            "87/87 [==============================] - 18s 209ms/step - loss: 0.2115 - accuracy: 0.9534 - val_loss: 0.2257 - val_accuracy: 0.9488\n",
            "Epoch 49/200\n",
            "87/87 [==============================] - 18s 210ms/step - loss: 0.2080 - accuracy: 0.9541 - val_loss: 0.2116 - val_accuracy: 0.9507\n",
            "Epoch 50/200\n",
            "87/87 [==============================] - 18s 210ms/step - loss: 0.2144 - accuracy: 0.9528 - val_loss: 0.2536 - val_accuracy: 0.9390\n",
            "Epoch 51/200\n",
            "87/87 [==============================] - 18s 210ms/step - loss: 0.2271 - accuracy: 0.9484 - val_loss: 0.2034 - val_accuracy: 0.9533\n",
            "Epoch 52/200\n",
            "87/87 [==============================] - 18s 209ms/step - loss: 0.2135 - accuracy: 0.9523 - val_loss: 0.3280 - val_accuracy: 0.8927\n",
            "Epoch 53/200\n",
            "87/87 [==============================] - 18s 210ms/step - loss: 0.2145 - accuracy: 0.9518 - val_loss: 0.2248 - val_accuracy: 0.9458\n",
            "Epoch 54/200\n",
            "87/87 [==============================] - 18s 209ms/step - loss: 0.2132 - accuracy: 0.9527 - val_loss: 0.2101 - val_accuracy: 0.9514\n",
            "Epoch 55/200\n",
            "87/87 [==============================] - 18s 210ms/step - loss: 0.2075 - accuracy: 0.9539 - val_loss: 0.2315 - val_accuracy: 0.9448\n",
            "Epoch 56/200\n",
            "87/87 [==============================] - 18s 210ms/step - loss: 0.2110 - accuracy: 0.9530 - val_loss: 0.2042 - val_accuracy: 0.9522\n",
            "Epoch 57/200\n",
            "87/87 [==============================] - 18s 209ms/step - loss: 0.2135 - accuracy: 0.9529 - val_loss: 0.2145 - val_accuracy: 0.9517\n",
            "Epoch 58/200\n",
            "87/87 [==============================] - 18s 210ms/step - loss: 0.2114 - accuracy: 0.9529 - val_loss: 0.2112 - val_accuracy: 0.9523\n",
            "Epoch 59/200\n",
            "87/87 [==============================] - 18s 209ms/step - loss: 0.2173 - accuracy: 0.9512 - val_loss: 0.2029 - val_accuracy: 0.9536\n",
            "Epoch 60/200\n",
            "87/87 [==============================] - 18s 210ms/step - loss: 0.2088 - accuracy: 0.9535 - val_loss: 0.2103 - val_accuracy: 0.9535\n",
            "Epoch 61/200\n",
            "87/87 [==============================] - 18s 211ms/step - loss: 0.2111 - accuracy: 0.9530 - val_loss: 0.2558 - val_accuracy: 0.9346\n",
            "Epoch 62/200\n",
            "87/87 [==============================] - 18s 212ms/step - loss: 0.2135 - accuracy: 0.9522 - val_loss: 0.2354 - val_accuracy: 0.9474\n",
            "Epoch 63/200\n",
            "87/87 [==============================] - 18s 209ms/step - loss: 0.2099 - accuracy: 0.9539 - val_loss: 0.2840 - val_accuracy: 0.9311\n",
            "Epoch 64/200\n",
            "87/87 [==============================] - 18s 209ms/step - loss: 0.2200 - accuracy: 0.9497 - val_loss: 0.2304 - val_accuracy: 0.9484\n",
            "Epoch 65/200\n",
            "87/87 [==============================] - 18s 209ms/step - loss: 0.2138 - accuracy: 0.9522 - val_loss: 0.2145 - val_accuracy: 0.9545\n",
            "Epoch 66/200\n",
            "87/87 [==============================] - 18s 210ms/step - loss: 0.2119 - accuracy: 0.9533 - val_loss: 0.2446 - val_accuracy: 0.9462\n",
            "Epoch 67/200\n",
            "87/87 [==============================] - 18s 209ms/step - loss: 0.2123 - accuracy: 0.9527 - val_loss: 0.2055 - val_accuracy: 0.9532\n",
            "Epoch 68/200\n",
            "87/87 [==============================] - 18s 210ms/step - loss: 0.2059 - accuracy: 0.9543 - val_loss: 0.2362 - val_accuracy: 0.9468\n",
            "Epoch 69/200\n",
            "87/87 [==============================] - 18s 210ms/step - loss: 0.2131 - accuracy: 0.9523 - val_loss: 0.3732 - val_accuracy: 0.8719\n",
            "Epoch 70/200\n",
            "87/87 [==============================] - 18s 209ms/step - loss: 0.2122 - accuracy: 0.9524 - val_loss: 0.2145 - val_accuracy: 0.9536\n",
            "Epoch 71/200\n",
            "87/87 [==============================] - 18s 210ms/step - loss: 0.2085 - accuracy: 0.9536 - val_loss: 0.2051 - val_accuracy: 0.9537\n",
            "Epoch 72/200\n",
            "87/87 [==============================] - 18s 210ms/step - loss: 0.2147 - accuracy: 0.9520 - val_loss: 0.2122 - val_accuracy: 0.9517\n",
            "Epoch 73/200\n",
            "87/87 [==============================] - 18s 211ms/step - loss: 0.2080 - accuracy: 0.9533 - val_loss: 0.2212 - val_accuracy: 0.9508\n",
            "Epoch 74/200\n",
            "87/87 [==============================] - 18s 210ms/step - loss: 0.2100 - accuracy: 0.9530 - val_loss: 0.2153 - val_accuracy: 0.9536\n",
            "Epoch 75/200\n",
            "87/87 [==============================] - 18s 210ms/step - loss: 0.2117 - accuracy: 0.9519 - val_loss: 0.2085 - val_accuracy: 0.9533\n",
            "Epoch 76/200\n",
            "87/87 [==============================] - 20s 226ms/step - loss: 0.2137 - accuracy: 0.9520 - val_loss: 0.2887 - val_accuracy: 0.9309\n",
            "Epoch 77/200\n",
            "87/87 [==============================] - 19s 213ms/step - loss: 0.2140 - accuracy: 0.9521 - val_loss: 0.2175 - val_accuracy: 0.9527\n",
            "Epoch 78/200\n",
            "87/87 [==============================] - 18s 210ms/step - loss: 0.2066 - accuracy: 0.9537 - val_loss: 0.2195 - val_accuracy: 0.9532\n",
            "Epoch 79/200\n",
            "87/87 [==============================] - 18s 212ms/step - loss: 0.2130 - accuracy: 0.9524 - val_loss: 0.2319 - val_accuracy: 0.9419\n",
            "Epoch 80/200\n",
            "87/87 [==============================] - 18s 212ms/step - loss: 0.2131 - accuracy: 0.9525 - val_loss: 0.2834 - val_accuracy: 0.9304\n",
            "Epoch 81/200\n",
            "87/87 [==============================] - 18s 210ms/step - loss: 0.2175 - accuracy: 0.9509 - val_loss: 0.2247 - val_accuracy: 0.9448\n",
            "Epoch 82/200\n",
            "87/87 [==============================] - 18s 210ms/step - loss: 0.2106 - accuracy: 0.9527 - val_loss: 0.2067 - val_accuracy: 0.9517\n",
            "Epoch 83/200\n",
            "87/87 [==============================] - 18s 212ms/step - loss: 0.2091 - accuracy: 0.9531 - val_loss: 0.2048 - val_accuracy: 0.9532\n",
            "Epoch 84/200\n",
            "87/87 [==============================] - 18s 210ms/step - loss: 0.2090 - accuracy: 0.9537 - val_loss: 0.2097 - val_accuracy: 0.9522\n",
            "Epoch 85/200\n",
            "87/87 [==============================] - 18s 210ms/step - loss: 0.2147 - accuracy: 0.9517 - val_loss: 0.2038 - val_accuracy: 0.9536\n",
            "Epoch 86/200\n",
            "87/87 [==============================] - 18s 210ms/step - loss: 0.2134 - accuracy: 0.9523 - val_loss: 0.2113 - val_accuracy: 0.9530\n",
            "Epoch 87/200\n",
            "87/87 [==============================] - 18s 211ms/step - loss: 0.2057 - accuracy: 0.9541 - val_loss: 0.2389 - val_accuracy: 0.9441\n",
            "Epoch 88/200\n",
            "87/87 [==============================] - 18s 210ms/step - loss: 0.2105 - accuracy: 0.9536 - val_loss: 0.2062 - val_accuracy: 0.9524\n",
            "Epoch 89/200\n",
            "87/87 [==============================] - 18s 210ms/step - loss: 0.2073 - accuracy: 0.9532 - val_loss: 0.2084 - val_accuracy: 0.9531\n",
            "Epoch 90/200\n",
            "87/87 [==============================] - 18s 210ms/step - loss: 0.2119 - accuracy: 0.9521 - val_loss: 0.2074 - val_accuracy: 0.9529\n",
            "Epoch 91/200\n",
            "87/87 [==============================] - 18s 210ms/step - loss: 0.2102 - accuracy: 0.9527 - val_loss: 0.2287 - val_accuracy: 0.9504\n",
            "Epoch 92/200\n",
            "87/87 [==============================] - 18s 210ms/step - loss: 0.2084 - accuracy: 0.9531 - val_loss: 0.2305 - val_accuracy: 0.9492\n",
            "Epoch 93/200\n",
            "87/87 [==============================] - 18s 211ms/step - loss: 0.2114 - accuracy: 0.9531 - val_loss: 0.2352 - val_accuracy: 0.9480\n",
            "Epoch 94/200\n",
            "87/87 [==============================] - 18s 211ms/step - loss: 0.2073 - accuracy: 0.9535 - val_loss: 0.2172 - val_accuracy: 0.9529\n",
            "Epoch 95/200\n",
            "87/87 [==============================] - 18s 209ms/step - loss: 0.2040 - accuracy: 0.9541 - val_loss: 0.2361 - val_accuracy: 0.9503\n",
            "Epoch 96/200\n",
            "87/87 [==============================] - 18s 211ms/step - loss: 0.2096 - accuracy: 0.9530 - val_loss: 0.2256 - val_accuracy: 0.9498\n",
            "Epoch 97/200\n",
            "87/87 [==============================] - 18s 210ms/step - loss: 0.2109 - accuracy: 0.9527 - val_loss: 0.2098 - val_accuracy: 0.9539\n",
            "Epoch 98/200\n",
            "87/87 [==============================] - 18s 211ms/step - loss: 0.2016 - accuracy: 0.9551 - val_loss: 0.2088 - val_accuracy: 0.9526\n",
            "Epoch 99/200\n",
            "87/87 [==============================] - 18s 211ms/step - loss: 0.2111 - accuracy: 0.9526 - val_loss: 0.3587 - val_accuracy: 0.9057\n",
            "Epoch 100/200\n",
            "87/87 [==============================] - 19s 214ms/step - loss: 0.2284 - accuracy: 0.9464 - val_loss: 0.2460 - val_accuracy: 0.9422\n",
            "Epoch 101/200\n",
            "87/87 [==============================] - 18s 211ms/step - loss: 0.2075 - accuracy: 0.9533 - val_loss: 0.2199 - val_accuracy: 0.9519\n",
            "Epoch 102/200\n",
            "87/87 [==============================] - 18s 211ms/step - loss: 0.2090 - accuracy: 0.9524 - val_loss: 0.2055 - val_accuracy: 0.9533\n",
            "Epoch 103/200\n",
            "87/87 [==============================] - 18s 211ms/step - loss: 0.2056 - accuracy: 0.9535 - val_loss: 0.2217 - val_accuracy: 0.9533\n",
            "Epoch 104/200\n",
            "87/87 [==============================] - 18s 211ms/step - loss: 0.2088 - accuracy: 0.9530 - val_loss: 0.2026 - val_accuracy: 0.9541\n",
            "Epoch 105/200\n",
            "87/87 [==============================] - 18s 211ms/step - loss: 0.2121 - accuracy: 0.9519 - val_loss: 0.2214 - val_accuracy: 0.9504\n",
            "Epoch 106/200\n",
            "87/87 [==============================] - 18s 210ms/step - loss: 0.2053 - accuracy: 0.9541 - val_loss: 0.2447 - val_accuracy: 0.9452\n",
            "Epoch 107/200\n",
            "87/87 [==============================] - 18s 211ms/step - loss: 0.2156 - accuracy: 0.9512 - val_loss: 0.2995 - val_accuracy: 0.9002\n",
            "Epoch 108/200\n",
            "87/87 [==============================] - 18s 211ms/step - loss: 0.2038 - accuracy: 0.9549 - val_loss: 0.2501 - val_accuracy: 0.9410\n",
            "Epoch 109/200\n",
            "87/87 [==============================] - 18s 210ms/step - loss: 0.2145 - accuracy: 0.9515 - val_loss: 0.2108 - val_accuracy: 0.9530\n",
            "Epoch 110/200\n",
            "87/87 [==============================] - 18s 211ms/step - loss: 0.2037 - accuracy: 0.9540 - val_loss: 0.2161 - val_accuracy: 0.9505\n",
            "Epoch 111/200\n",
            "87/87 [==============================] - 18s 211ms/step - loss: 0.2069 - accuracy: 0.9530 - val_loss: 0.2007 - val_accuracy: 0.9544\n",
            "Epoch 112/200\n",
            "87/87 [==============================] - 18s 211ms/step - loss: 0.2116 - accuracy: 0.9527 - val_loss: 0.2084 - val_accuracy: 0.9519\n",
            "Epoch 113/200\n",
            "87/87 [==============================] - 19s 214ms/step - loss: 0.2140 - accuracy: 0.9518 - val_loss: 0.2049 - val_accuracy: 0.9522\n",
            "Epoch 114/200\n",
            "87/87 [==============================] - 19s 213ms/step - loss: 0.2122 - accuracy: 0.9526 - val_loss: 0.2122 - val_accuracy: 0.9549\n",
            "Epoch 115/200\n",
            "87/87 [==============================] - 18s 211ms/step - loss: 0.2085 - accuracy: 0.9534 - val_loss: 0.2134 - val_accuracy: 0.9542\n",
            "Epoch 116/200\n",
            "87/87 [==============================] - 18s 212ms/step - loss: 0.2054 - accuracy: 0.9543 - val_loss: 0.2201 - val_accuracy: 0.9513\n",
            "Epoch 117/200\n",
            "87/87 [==============================] - 19s 214ms/step - loss: 0.2110 - accuracy: 0.9528 - val_loss: 0.2106 - val_accuracy: 0.9532\n",
            "Epoch 118/200\n",
            "87/87 [==============================] - 18s 211ms/step - loss: 0.2110 - accuracy: 0.9527 - val_loss: 0.2045 - val_accuracy: 0.9535\n",
            "Epoch 119/200\n",
            "87/87 [==============================] - 18s 211ms/step - loss: 0.2049 - accuracy: 0.9539 - val_loss: 0.2059 - val_accuracy: 0.9532\n",
            "Epoch 120/200\n",
            "87/87 [==============================] - 18s 212ms/step - loss: 0.2019 - accuracy: 0.9545 - val_loss: 0.2698 - val_accuracy: 0.9382\n",
            "Epoch 121/200\n",
            "87/87 [==============================] - 18s 212ms/step - loss: 0.2110 - accuracy: 0.9522 - val_loss: 0.2096 - val_accuracy: 0.9528\n",
            "Epoch 122/200\n",
            "87/87 [==============================] - 18s 211ms/step - loss: 0.2052 - accuracy: 0.9539 - val_loss: 0.2139 - val_accuracy: 0.9503\n",
            "Epoch 123/200\n",
            "87/87 [==============================] - 18s 212ms/step - loss: 0.2044 - accuracy: 0.9542 - val_loss: 0.2398 - val_accuracy: 0.9445\n",
            "Epoch 124/200\n",
            "87/87 [==============================] - 18s 212ms/step - loss: 0.2105 - accuracy: 0.9525 - val_loss: 0.2033 - val_accuracy: 0.9547\n",
            "Epoch 125/200\n",
            "87/87 [==============================] - 18s 212ms/step - loss: 0.2051 - accuracy: 0.9544 - val_loss: 0.2134 - val_accuracy: 0.9539\n",
            "Epoch 126/200\n",
            "87/87 [==============================] - 18s 211ms/step - loss: 0.2035 - accuracy: 0.9547 - val_loss: 0.2154 - val_accuracy: 0.9540\n",
            "Epoch 127/200\n",
            "87/87 [==============================] - 18s 211ms/step - loss: 0.2058 - accuracy: 0.9542 - val_loss: 0.2015 - val_accuracy: 0.9537\n",
            "Epoch 128/200\n",
            "87/87 [==============================] - 18s 211ms/step - loss: 0.2119 - accuracy: 0.9525 - val_loss: 0.2065 - val_accuracy: 0.9520\n",
            "Epoch 129/200\n",
            "87/87 [==============================] - 18s 211ms/step - loss: 0.2048 - accuracy: 0.9535 - val_loss: 0.5194 - val_accuracy: 0.8791\n",
            "Epoch 130/200\n",
            "87/87 [==============================] - 18s 212ms/step - loss: 0.2687 - accuracy: 0.9412 - val_loss: 0.2311 - val_accuracy: 0.9472\n",
            "Epoch 131/200\n",
            "87/87 [==============================] - 19s 216ms/step - loss: 0.2078 - accuracy: 0.9534 - val_loss: 0.2108 - val_accuracy: 0.9537\n",
            "Epoch 132/200\n",
            "87/87 [==============================] - 18s 212ms/step - loss: 0.2100 - accuracy: 0.9526 - val_loss: 0.2287 - val_accuracy: 0.9495\n",
            "Epoch 133/200\n",
            "87/87 [==============================] - 19s 213ms/step - loss: 0.2062 - accuracy: 0.9532 - val_loss: 0.2263 - val_accuracy: 0.9450\n",
            "Epoch 134/200\n",
            "87/87 [==============================] - 18s 211ms/step - loss: 0.2084 - accuracy: 0.9527 - val_loss: 0.2052 - val_accuracy: 0.9538\n",
            "Epoch 135/200\n",
            "87/87 [==============================] - 18s 212ms/step - loss: 0.2088 - accuracy: 0.9529 - val_loss: 0.2537 - val_accuracy: 0.9400\n",
            "Epoch 136/200\n",
            "87/87 [==============================] - 18s 211ms/step - loss: 0.2203 - accuracy: 0.9499 - val_loss: 0.3295 - val_accuracy: 0.9018\n",
            "Epoch 137/200\n",
            "87/87 [==============================] - 18s 212ms/step - loss: 0.2245 - accuracy: 0.9486 - val_loss: 0.2392 - val_accuracy: 0.9455\n",
            "Epoch 138/200\n",
            "87/87 [==============================] - 18s 211ms/step - loss: 0.2065 - accuracy: 0.9539 - val_loss: 0.2059 - val_accuracy: 0.9540\n",
            "Epoch 139/200\n",
            "87/87 [==============================] - 18s 212ms/step - loss: 0.2070 - accuracy: 0.9534 - val_loss: 0.2187 - val_accuracy: 0.9528\n",
            "Epoch 140/200\n",
            "87/87 [==============================] - 18s 212ms/step - loss: 0.2024 - accuracy: 0.9550 - val_loss: 0.2005 - val_accuracy: 0.9541\n",
            "Epoch 141/200\n",
            "87/87 [==============================] - 18s 212ms/step - loss: 0.2051 - accuracy: 0.9541 - val_loss: 0.2094 - val_accuracy: 0.9537\n",
            "Epoch 142/200\n",
            "87/87 [==============================] - 18s 212ms/step - loss: 0.2112 - accuracy: 0.9519 - val_loss: 0.2432 - val_accuracy: 0.9452\n",
            "Epoch 143/200\n",
            "87/87 [==============================] - 18s 212ms/step - loss: 0.2082 - accuracy: 0.9533 - val_loss: 0.2562 - val_accuracy: 0.9424\n",
            "Epoch 144/200\n",
            "87/87 [==============================] - 18s 211ms/step - loss: 0.2050 - accuracy: 0.9536 - val_loss: 0.2058 - val_accuracy: 0.9527\n",
            "Epoch 145/200\n",
            "87/87 [==============================] - 18s 212ms/step - loss: 0.2031 - accuracy: 0.9546 - val_loss: 0.2300 - val_accuracy: 0.9496\n",
            "Epoch 146/200\n",
            "87/87 [==============================] - 18s 212ms/step - loss: 0.2095 - accuracy: 0.9528 - val_loss: 0.2155 - val_accuracy: 0.9509\n",
            "Epoch 147/200\n",
            "87/87 [==============================] - 18s 212ms/step - loss: 0.2058 - accuracy: 0.9537 - val_loss: 0.2043 - val_accuracy: 0.9524\n",
            "Epoch 148/200\n",
            "87/87 [==============================] - 20s 234ms/step - loss: 0.2094 - accuracy: 0.9531 - val_loss: 0.2138 - val_accuracy: 0.9479\n",
            "Epoch 149/200\n",
            "87/87 [==============================] - 18s 212ms/step - loss: 0.2105 - accuracy: 0.9522 - val_loss: 0.2037 - val_accuracy: 0.9542\n",
            "Epoch 150/200\n",
            "87/87 [==============================] - 19s 214ms/step - loss: 0.2085 - accuracy: 0.9531 - val_loss: 0.2128 - val_accuracy: 0.9545\n",
            "Epoch 151/200\n",
            "87/87 [==============================] - 18s 212ms/step - loss: 0.2044 - accuracy: 0.9545 - val_loss: 0.2973 - val_accuracy: 0.8990\n",
            "Epoch 152/200\n",
            "87/87 [==============================] - 18s 213ms/step - loss: 0.2096 - accuracy: 0.9530 - val_loss: 0.2760 - val_accuracy: 0.9338\n",
            "Epoch 153/200\n",
            "87/87 [==============================] - 18s 212ms/step - loss: 0.2079 - accuracy: 0.9532 - val_loss: 0.2217 - val_accuracy: 0.9537\n",
            "Epoch 154/200\n",
            "87/87 [==============================] - 18s 212ms/step - loss: 0.2111 - accuracy: 0.9522 - val_loss: 0.2109 - val_accuracy: 0.9517\n",
            "Epoch 155/200\n",
            "87/87 [==============================] - 18s 212ms/step - loss: 0.2119 - accuracy: 0.9524 - val_loss: 0.2077 - val_accuracy: 0.9533\n",
            "Epoch 156/200\n",
            "87/87 [==============================] - 18s 212ms/step - loss: 0.2047 - accuracy: 0.9544 - val_loss: 0.2132 - val_accuracy: 0.9548\n",
            "Epoch 157/200\n",
            "87/87 [==============================] - 18s 212ms/step - loss: 0.2066 - accuracy: 0.9534 - val_loss: 0.2131 - val_accuracy: 0.9537\n",
            "Epoch 158/200\n",
            "87/87 [==============================] - 18s 212ms/step - loss: 0.2081 - accuracy: 0.9530 - val_loss: 0.2023 - val_accuracy: 0.9543\n",
            "Epoch 159/200\n",
            "87/87 [==============================] - 19s 213ms/step - loss: 0.2017 - accuracy: 0.9549 - val_loss: 0.2053 - val_accuracy: 0.9544\n",
            "Epoch 160/200\n",
            "87/87 [==============================] - 18s 211ms/step - loss: 0.2027 - accuracy: 0.9542 - val_loss: 0.2073 - val_accuracy: 0.9544\n",
            "Epoch 161/200\n",
            "87/87 [==============================] - 18s 211ms/step - loss: 0.2088 - accuracy: 0.9527 - val_loss: 0.2025 - val_accuracy: 0.9543\n",
            "Epoch 162/200\n",
            "87/87 [==============================] - 18s 211ms/step - loss: 0.2050 - accuracy: 0.9538 - val_loss: 0.2187 - val_accuracy: 0.9519\n",
            "Epoch 163/200\n",
            "87/87 [==============================] - 18s 212ms/step - loss: 0.2018 - accuracy: 0.9546 - val_loss: 0.2034 - val_accuracy: 0.9541\n",
            "Epoch 164/200\n",
            "87/87 [==============================] - 18s 212ms/step - loss: 0.2081 - accuracy: 0.9528 - val_loss: 0.2335 - val_accuracy: 0.9478\n",
            "Epoch 165/200\n",
            "87/87 [==============================] - 19s 215ms/step - loss: 0.2044 - accuracy: 0.9542 - val_loss: 0.2273 - val_accuracy: 0.9482\n",
            "Epoch 166/200\n",
            "87/87 [==============================] - 18s 213ms/step - loss: 0.2090 - accuracy: 0.9528 - val_loss: 0.2102 - val_accuracy: 0.9541\n",
            "Epoch 167/200\n",
            "87/87 [==============================] - 19s 213ms/step - loss: 0.2080 - accuracy: 0.9532 - val_loss: 0.2209 - val_accuracy: 0.9534\n",
            "Epoch 168/200\n",
            "87/87 [==============================] - 18s 212ms/step - loss: 0.2077 - accuracy: 0.9534 - val_loss: 0.2077 - val_accuracy: 0.9535\n",
            "Epoch 169/200\n",
            "87/87 [==============================] - 18s 212ms/step - loss: 0.2090 - accuracy: 0.9525 - val_loss: 0.2133 - val_accuracy: 0.9526\n",
            "Epoch 170/200\n",
            "87/87 [==============================] - 18s 212ms/step - loss: 0.2073 - accuracy: 0.9528 - val_loss: 0.2099 - val_accuracy: 0.9541\n",
            "Epoch 171/200\n",
            "87/87 [==============================] - 18s 212ms/step - loss: 0.2079 - accuracy: 0.9532 - val_loss: 0.2008 - val_accuracy: 0.9539\n",
            "Epoch 172/200\n",
            "87/87 [==============================] - 18s 213ms/step - loss: 0.2047 - accuracy: 0.9538 - val_loss: 0.2259 - val_accuracy: 0.9527\n",
            "Epoch 173/200\n",
            "87/87 [==============================] - 18s 212ms/step - loss: 0.2030 - accuracy: 0.9546 - val_loss: 0.2015 - val_accuracy: 0.9546\n",
            "Epoch 174/200\n",
            "87/87 [==============================] - 18s 212ms/step - loss: 0.2059 - accuracy: 0.9534 - val_loss: 0.2132 - val_accuracy: 0.9536\n",
            "Epoch 175/200\n",
            "87/87 [==============================] - 18s 212ms/step - loss: 0.2074 - accuracy: 0.9531 - val_loss: 0.2204 - val_accuracy: 0.9530\n",
            "Epoch 176/200\n",
            "87/87 [==============================] - 18s 212ms/step - loss: 0.2056 - accuracy: 0.9540 - val_loss: 0.2044 - val_accuracy: 0.9525\n",
            "Epoch 177/200\n",
            "87/87 [==============================] - 18s 212ms/step - loss: 0.2054 - accuracy: 0.9537 - val_loss: 0.2101 - val_accuracy: 0.9537\n",
            "Epoch 178/200\n",
            "87/87 [==============================] - 18s 211ms/step - loss: 0.2122 - accuracy: 0.9516 - val_loss: 0.2055 - val_accuracy: 0.9546\n",
            "Epoch 179/200\n",
            "87/87 [==============================] - 18s 212ms/step - loss: 0.2076 - accuracy: 0.9528 - val_loss: 0.2061 - val_accuracy: 0.9542\n",
            "Epoch 180/200\n",
            "87/87 [==============================] - 18s 211ms/step - loss: 0.2086 - accuracy: 0.9528 - val_loss: 0.3122 - val_accuracy: 0.8902\n",
            "Epoch 181/200\n",
            "87/87 [==============================] - 18s 211ms/step - loss: 0.2107 - accuracy: 0.9519 - val_loss: 0.2247 - val_accuracy: 0.9527\n",
            "Epoch 182/200\n",
            "87/87 [==============================] - 19s 215ms/step - loss: 0.2067 - accuracy: 0.9538 - val_loss: 0.2544 - val_accuracy: 0.9396\n",
            "Epoch 183/200\n",
            "87/87 [==============================] - 19s 213ms/step - loss: 0.2039 - accuracy: 0.9541 - val_loss: 0.2091 - val_accuracy: 0.9525\n",
            "Epoch 184/200\n",
            "87/87 [==============================] - 18s 211ms/step - loss: 0.2088 - accuracy: 0.9529 - val_loss: 0.2415 - val_accuracy: 0.9446\n",
            "Epoch 185/200\n",
            "87/87 [==============================] - 18s 212ms/step - loss: 0.2052 - accuracy: 0.9539 - val_loss: 0.2597 - val_accuracy: 0.9377\n",
            "Epoch 186/200\n",
            "87/87 [==============================] - 18s 211ms/step - loss: 0.2044 - accuracy: 0.9540 - val_loss: 0.2051 - val_accuracy: 0.9528\n",
            "Epoch 187/200\n",
            "87/87 [==============================] - 18s 212ms/step - loss: 0.2040 - accuracy: 0.9538 - val_loss: 0.2635 - val_accuracy: 0.9321\n",
            "Epoch 188/200\n",
            "87/87 [==============================] - 18s 212ms/step - loss: 0.2073 - accuracy: 0.9534 - val_loss: 0.2095 - val_accuracy: 0.9544\n",
            "Epoch 189/200\n",
            "87/87 [==============================] - 18s 212ms/step - loss: 0.2079 - accuracy: 0.9527 - val_loss: 0.2991 - val_accuracy: 0.9201\n",
            "Epoch 190/200\n",
            "87/87 [==============================] - 18s 212ms/step - loss: 0.2113 - accuracy: 0.9518 - val_loss: 0.2139 - val_accuracy: 0.9540\n",
            "Epoch 191/200\n",
            "87/87 [==============================] - 18s 212ms/step - loss: 0.2067 - accuracy: 0.9530 - val_loss: 0.2012 - val_accuracy: 0.9540\n",
            "Epoch 192/200\n",
            "87/87 [==============================] - 18s 212ms/step - loss: 0.2084 - accuracy: 0.9532 - val_loss: 0.2065 - val_accuracy: 0.9536\n",
            "Epoch 193/200\n",
            "87/87 [==============================] - 18s 212ms/step - loss: 0.2037 - accuracy: 0.9543 - val_loss: 0.2015 - val_accuracy: 0.9543\n",
            "Epoch 194/200\n",
            "87/87 [==============================] - 18s 212ms/step - loss: 0.2051 - accuracy: 0.9537 - val_loss: 0.3437 - val_accuracy: 0.8935\n",
            "Epoch 195/200\n",
            "87/87 [==============================] - 18s 212ms/step - loss: 0.2250 - accuracy: 0.9468 - val_loss: 0.2080 - val_accuracy: 0.9532\n",
            "Epoch 196/200\n",
            "87/87 [==============================] - 18s 212ms/step - loss: 0.2064 - accuracy: 0.9536 - val_loss: 0.2326 - val_accuracy: 0.9478\n",
            "Epoch 197/200\n",
            "87/87 [==============================] - 18s 212ms/step - loss: 0.2090 - accuracy: 0.9524 - val_loss: 0.2071 - val_accuracy: 0.9539\n",
            "Epoch 198/200\n",
            "87/87 [==============================] - 18s 212ms/step - loss: 0.2046 - accuracy: 0.9535 - val_loss: 0.2632 - val_accuracy: 0.9335\n",
            "Epoch 199/200\n",
            "87/87 [==============================] - 18s 213ms/step - loss: 0.2083 - accuracy: 0.9527 - val_loss: 0.2264 - val_accuracy: 0.9512\n",
            "Epoch 200/200\n",
            "87/87 [==============================] - 19s 217ms/step - loss: 0.2093 - accuracy: 0.9530 - val_loss: 0.2114 - val_accuracy: 0.9542\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-5902c3a2c184>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# plot learning curves\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mpyplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mpyplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mpyplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lrate='\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlrate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pyplot' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "PW_Mq1ZZS1Nc",
        "outputId": "dff02375-b0cf-409c-ca66-084b69586c01"
      },
      "source": [
        "from matplotlib import pyplot\r\n",
        "# plot learning curves\r\n",
        "pyplot.plot(history.history['accuracy'], label='train')\r\n",
        "pyplot.plot(history.history['val_accuracy'], label='test')\r\n",
        "pyplot.title('lrate='+str(lrate), pad=-50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'lrate=0.1')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}